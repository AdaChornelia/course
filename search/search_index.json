{
    "docs": [
        {
            "location": "/",
            "text": "SGBC Bioinformatics Course\n\n\nThis website is a collection of lectures and tutorials given during the annual Bioinformatics course given at \nSLU\n by the \nSGBC\n.\n\n\nWelcome to the course!\n\n\nTable of Content\n\n\n\n\n\n\nWeek 1\n\n\n\n\nIntroduction to proteins analysis\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\nIntroduction to the command-line\n\n\nCloud Computing\n\n\nInstalling Software\n\n\nVersion control with Git\n\n\nCommand-line Blast\n\n\nProject organisation\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\nSequencing Technologies\n\n\nQuality Control\n\n\nDe-novo Genome Assembly\n\n\nAssembly Challenge\n\n\nGenome Annotation\n\n\nPan-genome Analysis\n\n\nNanopore Sequencing\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\nRNA Sequencing\n\n\nMetabarcoding\n\n\nMetagenome assembly\n\n\nComparative metagenomics\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\n\nGenome Annotation (NBIS)\n\n\n\n\n\n\n\n\nLicense\n\n\nUnless stated otherwise, all the lessons are licensed under the Creative Commons Attribution 4.0 International License.\nTo view a copy of this license, visit \nhttp://creativecommons.org/licenses/by/4.0/\n or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n\nContributors\n\n\nThe following people have contributed to these course material, in no particular order:\n\n\n\n\nHadrien Gourl\u00e9\n\n\nJuliette Hayer\n\n\nOskar Karlsson\n\n\nJacques Dainat",
            "title": "Home"
        },
        {
            "location": "/#sgbc-bioinformatics-course",
            "text": "This website is a collection of lectures and tutorials given during the annual Bioinformatics course given at  SLU  by the  SGBC .  Welcome to the course!",
            "title": "SGBC Bioinformatics Course"
        },
        {
            "location": "/#table-of-content",
            "text": "Week 1   Introduction to proteins analysis     Week 2   Introduction to the command-line  Cloud Computing  Installing Software  Version control with Git  Command-line Blast  Project organisation     Week 3   Sequencing Technologies  Quality Control  De-novo Genome Assembly  Assembly Challenge  Genome Annotation  Pan-genome Analysis  Nanopore Sequencing     Week 4   RNA Sequencing  Metabarcoding  Metagenome assembly  Comparative metagenomics     Week 5   Genome Annotation (NBIS)",
            "title": "Table of Content"
        },
        {
            "location": "/#license",
            "text": "Unless stated otherwise, all the lessons are licensed under the Creative Commons Attribution 4.0 International License.\nTo view a copy of this license, visit  http://creativecommons.org/licenses/by/4.0/  or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.",
            "title": "License"
        },
        {
            "location": "/#contributors",
            "text": "The following people have contributed to these course material, in no particular order:   Hadrien Gourl\u00e9  Juliette Hayer  Oskar Karlsson  Jacques Dainat",
            "title": "Contributors"
        },
        {
            "location": "/proteins/",
            "text": "Introduction to protein sequences and structures analysis\n\n\n\n\n\nToolBox that could be useful for protein sequences analysis:\n\n\nhttp://expasy.org/\n\n\nhttp://www.uniprot.org/\n\n\nhttp://www.ebiokit.eu/\n\n\nhttp://npsa-pbil.ibcp.fr\n\n\nhttp://blast.ncbi.nlm.nih.gov/Blast.cgi\n\n\nhttps://www.ebi.ac.uk/interpro\n\n\nhttp://www.rcsb.org/pdb\n\n\n\n\n\nAfter cloning and sequencing of coding DNA, the sequence of the X\nprotein had been determined. The sequence of X is given here:\n\n\nLAAVSVDCSEYPKPACTLEYRPLCGSDNKTYGNKCNFCNAVVESNGTLTLSHFGKC\n\n\nIn normal conditions, this X protein is expressed but we have no idea\nabout it function. The goal of this practical work is to collect the\nmaximum of information about structure and function of the X protein.\n\n\nI - Search Patterns, Profiles\n\n\nA way to identify the function of X is to look if it contains signatures\n(pattern) of a function or a protein family.\n\n\n2 options:\n\n\nhttp://prosite.expasy.org/scanprosite/\n\n\nNPS@\n and follow the link \"ProScan: scan a\nsequence for sites/signatures against PROSITE database\" (activate:\nInclude documentation in result file).\n\n\n\n\nQuestion\n\n\n\n\n\n\nWhich signature(s) could you identify? Which specific features in\n    this protein?\n\n\n\n\n\n\nTry to change the parameters and comment the results.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nInterPro gives a summary of several methods. You can find it at the\n\nEBI\n.\n\n\n\n\nKeep the signatures that could attest the function in your notepad.\n\n\n\n\nWhat do you think about the function of X?\n\n\n\n\nII - Search homolog proteins with BLAST\n\n\n\n\n\n\nGo to the \nNCBI BLAST\n page\n\n\n\n\n\n\nChoose the Protein Blast (blastp)\n\n\n\n\n\n\nPaste your sequence\n\n\n\n\n\n\nSelect the Swissprot database\n\n\n\n\n\n\n\n\nQuestion\n\n\nDid you identify homologs? What are their function(s)?\n\n\n\n\nIII - Multiple sequences alignment\n\n\n\n\n\n\nSelect several homolog sequences from the Blast results.\n\n\n\n\n\n\nPerform a multiple sequence alignment (MSA) of these sequence using\n    Clustal Omega for example\n\n\n\n\n\n\nTry other MSA tools (for example Tcoffee and Muscle)\n\n\n\n\n\n\n\n\nQuestion\n\n\nDo you observe differences between the results obtained from\ndifferent algorithms?\n\n\nWhat can you observe in these MSAs?\n\n\n\n\nInfo\n: You could also retrieve the selected sequences in Fasta format\nand perform MSAs elsewhere\n\n\nClustal Omega and Muscle: available in Seaview alignment viewer\n\n\nTcoffee: \nhttp://tcoffee.vital-it.ch/apps/tcoffee/index.html\n\n\nOther tools: \nhttp://expasy.org/genomics/sequence_alignment\n\n\nIV - The Y protein\n\n\nAnother experiment had shown that the X protein was interacting\nspecifically with another protein: Y.\n\n\nAfter purification of the active Y protein, from the complex, a partial\nsequence of Y was obtained (by protein extremity sequencing).\n\n\nThe corresponding peptide could be:\n\n\nISGGD\n or \nISGGN\n\n\n1. Identification of the Y sequence using PROSITE patterns\n\n\n\n\n\n\nDesign the pattern (regular expression) corresponding to these\n    peptides.\n\n\n\n\n\n\nSearch the sequences containing this pattern in SwissProt using\n    \nPATTERN SEARCH\n at\n    SIB or\n    \nPATTINPROT\n\n    at NPS@.\n\n\n\n\n\n\nIf needed, use the help to design your pattern.\n\n\n\n\nQuestion\n\n\nHow many results do you get? How can you identify the right one?\n\n\n\n\nOnce the Y protein sequence identified, copy the FASTA sequence in your\nnotepad.\n\n\n2. Composition analysis\n\n\nAfter purification of the Y active protein, the amino-acid composition\nhas been determined (% of each aa in the protein) and is given in the\nfollowing table:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\n\n\n8.11\n\n\nF\n\n\n2.70\n\n\nL\n\n\n3.78\n\n\nR\n\n\n4.32\n\n\nX\n\n\n0\n\n\n\n\n\n\nB\n\n\n0\n\n\nG\n\n\n17.30\n\n\nM\n\n\n1.08\n\n\nS\n\n\n11.89\n\n\nY\n\n\n5.41\n\n\n\n\n\n\nC\n\n\n2.16\n\n\nH\n\n\n1.08\n\n\nN\n\n\n5.41\n\n\nT\n\n\n15.14\n\n\nZ\n\n\n0\n\n\n\n\n\n\nD\n\n\n3.78\n\n\nI\n\n\n3.78\n\n\nP\n\n\n2.70\n\n\nV\n\n\n7.57\n\n\n\n\n\n\n\n\n\n\nE\n\n\n1.08\n\n\nK\n\n\n0.54\n\n\nQ\n\n\n1.08\n\n\nW\n\n\n1.08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompute the composition of the sequence that you retrieve. Use\n    \nPROTPARAM\n or the tool\n    'Amino-acid composition' at \nNPS@\n\n\n\n\n\n\nCompare this computed composition with the composition of Y\n    experimentally determined.\n\n\n\n\n\n\n\n\nQuestion\n\n\nDo you observe differences? Explain.\n\n\n\n\n3. Search pattern in Y\n\n\nOnce the correct sequence of Y obtained, keep it in your notepad, you\nwill need it for the following analyses.\n\n\n\n\nQuestion\n\n\nIdentify the signatures (motifs, Pfam profiles) of Y using PROSCAN\nand/or Interpro.\n\n\n\n\n4. Identification of homologs to Y\n\n\n\n\n\n\nUse NCBI BLASTP or NPS@\n    \nBLASTP\n\n    against SwissProt database to search sequences similar to Y.\n\n\n\n\n\n\nUse PSI-BLAST (with SwissProt) to see if you can detect more distant\n    sequences.\n\n\n\n\n\n\nSelect sequences from BLAST and/or PSI-BLAST results to perform a\n    multiple sequence alignment.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nDid you observe difference in the results of BLAST and PSI-BLAST?\nComment.\n\n\nPropose a strategy to retrieve all the proteins having the same\ncatalytic activity as Y protein.\n\n\n\n\n\n\nV - Secondary structure prediction for X and Y\n\n\n\n\n\n\nGo to the \nconsensus secondary structure\n    prediction\n\n    page at NPS@.\n\n\n\n\n\n\nAnalyze the secondary structure of the protein Y. Include secondary\n    structure predictions by methods (DPM, GOR1, PREDATOR, SIMPA96).\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nConclude on the organization of secondary structures.\n\n\nPerform the same analysis for X protein.\n\n\n\n\n\n\nVI - Comparison with solved structures\n\n\n1. The Z protein\n\n\nThe structure of a protein Z has just been published. The sequence of\nprotein Z is shown below:\n\n\nIAGGEAITTGGSRCSLGFNVSVNGVAHALTAGHCTNISASWSIGTRTGTSFPNNDYGIIRHSNPAAANGRVYLYNGSYQD\n\n\nITTAGNAFVGQAVQRSGSTTGLRSGSVTGLNATVNYGSSGIVYGMIQTNVCAQPGDSGGSLFAGSTALGLTSGGSGNCRT\n\n\nGGTTFYQPVTEALSAYGATVL\n\n\n\n\nQuestion\n\n\nCould you use this information for the study of protein Y? Make your\nown analysis.\n\n\n\n\n2. Find the correct structures\n\n\n\n\n\n\nDownload and install \nDeep-View -\n    SwissPDBViewer\n. You can\n    find the tutorial and user guide of DeepView\n    \nhere\n.\n\n\n\n\n\n\nDownload to the archive \nPDB_files_part6.zip\n and unzip it.\n\n\n\n\n\n\nYou might find 8 PDB files in the directory.\n\n\n\n\n\n\nOpen them with DeepView.\n\n\n\n\n\n\nDisplay the secondary structure representation mode (see part\n    VII-A-5 and/or the user guide).\n\n\n\n\n\n\n\n\nQuestion\n\n\nTry to identify the structures corresponding to X and Y proteins.\n\n\n\n\nVII - Tridimensional protein structure: Play with 3D structures using SwissPDBViewer (DeepView)\n\n\n\n\n\n\nGo to the \nProtein Data Bank\n\n\n\n\n\n\nSearch and download the following PDB files: 1CRN, 1LDM.\n\n\n\n\n\n\nYou will visualize these protein structures using DeepView\n\n\nA - Analyze protein structures with DeepView\n\n\n1. Load a 3D structure\n\n\nFile => Open\n\n\nChoose the 1CRN.pdb file that you have downloaded from the PDB.\n\n\n2. Visualize the number of chains\n\n\nIs it only the protein or can we find ligands? Is it a monomer or a\npolymer?\n\n\n3. Visualize the general shape\n\n\nTry to get the actual space taken by the molecule. You need to use the\ncontrol panel and use the ':v' column to activate the space-filling\nspheres representation (+ menu Display > Render in solid 3D).\n\n\nTest also the Slab mode to visualize the space within the molecule:\nDisplay > Slab\n\n\n4. Display a distance between 2 atoms, angle between 3 atoms\n\n\nUse the graphical panel. You can now measure the real dimensions of your\nprotein\n\n\n5. Visualize secondary structure elements\n\n\nIn the control panel, activate \"ribbon\" (rbn). You can also color the\nmolecule by secondary structures.\n\n\n6. Visualize ligands (if there is any)\n\n\nSelect and color them. You could also remove the rest, or better, have a\nlook at the residues that are around those ligands (radius function in\nthe graphical panel).\n\n\n7. Analysis of other protein structures\n\n\nThe teacher will give PDB codes of other structures to analyze. Choose\nDeepView or Rasmol/Jmol to do so, that is up to you!\n\n\nB - Optional: if you want to use RasMol/Jmol\n\n\n1. Load a 3D structure\n\n\nFile => Open\n\n\nChoose the 1CRN.pdb file that you have downloaded from the PDB.\n\n\n\n\nHELP SECTION FOR RASMOL\n\n\nMolecule main moves with the mouse:\n\n\nLeft button: XY rotation\n\n\nLeft button + Shift: Zoom\n\n\nRight button: Translation\n\n\nRight button + Shift: Z rotation\n\n\nKeep the graphical window and the command (text) window on your screen\n(> \u200b\u200bis a command to type in the text window).\n\n\nFor each selection (SELECT command), the number of selected atoms\nappears in the text window. After you can apply an action to be able to\nvisualize the elements that you have selected (e.g. COLOR GREEN).\n\n\nCtrl+Z does not exist in Rasmol. You can type the command RESET.\n\n\nIf you want to come back in a standard representation of your molecule,\ntype:\n\n\nSELECT ALL\n\nCPK\n\n\n\n\n=> This will reset previous actions on representation modes (but keep\ncolors). CPK: space-filling spheres representation\n\n\nCOLOR CPK: colors \\'atom\\' objects by the atom (element) type\n\n\n\n\n\nHelp for Jmol:\n\n\nA lot of \"actions\" (color, selection...) are available by right clicking\non the main screen\n\n\nTo get the terminal window: menu File > Console\n\n\n\n\n\n2. Example: visualize the disulfide bonds\n\n\nType in the text window\n\n\nSELECT CYS\n\n\n\n\nThe text window \\\"answers\\\" 36 atoms selected (selected cysteine's\natoms)\n\n\nCOLOR GREEN\n\n\n\n\n\n\nObserve the graphics window.\n\n\n\n\nRESTRICT CYS\n\n\n\n\n\n\nCompare with the SELECT command\n\n\n\n\nHighlight the disulfide bonds:\n\n\nSSBONDS\n\nCOLOR YELLOW\n\nSSBONDS 75\n\nCOLOR CPK\n\n\n\n\n3. Visualize secondary structure elements\n\n\nSSBONDS OFF (remove SS bonds)\n\nSELECT ALL\n\nCARTOONS\n\nCOLOR STRUCTURE\n\n\n\n\n4. Display a distance between 2 atoms\n\n\nActivate the compute distance mode typing:\n\n\nSET PICKING DISTANCE\n\n\n\n\nThen, you can click the 2 atoms.\n\n\nYou can display angle values typing:\n\n\nSET PICKING ANGLE\n\n\n\n\nThen pick the 3 atoms\n\n\n5. Other useful commands\n\n\nSHOW SEQUENCE\n\nSHOW INFO\n\nSELECT ALL\n\nCPK ON\n\nRESTRICT NOT HOH (remove water molecules)\n\nCPK OFF\n\nHBONDS\n\nSELECT CYCLIC AND NOT PRO\n\nSTEREO ON\n\n\n\n\nTry them to better understand the Rasmol command language.\n\n\n6. Store a command script and reload it\n\n\nRepeat the actions described in paragraph 2\n\n\nWRITE SCRIPT MY_SCRIPT.SC\n\n\n\n\nExit the software (File => Quit)\n\n\nRestart the software\n\n\nSOURCE MY_SCRIPT.SC\n\n\n\n\n7. Select the atoms in a sphere\n\n\nFile => Close\n\n\nLoad the file 1LDM.pdb\n\n\nDiscover and analyze the molecule (number of channels, ligands, \netc\n.)\n\n\nTo select all the atoms in a 3\u00c5 radius sphere centered on a ligand\n(\ne.g.\n NAD)\n\n\nSELECT ALL\n\nCOLOR CHAIN\n\nSELECT WITHIN (3.0, NAD)\n\nCPK\n\n\n\n\nOption => Slab Mode (comment).",
            "title": "Proteins"
        },
        {
            "location": "/proteins/#introduction-to-protein-sequences-and-structures-analysis",
            "text": "ToolBox that could be useful for protein sequences analysis:  http://expasy.org/  http://www.uniprot.org/  http://www.ebiokit.eu/  http://npsa-pbil.ibcp.fr  http://blast.ncbi.nlm.nih.gov/Blast.cgi  https://www.ebi.ac.uk/interpro  http://www.rcsb.org/pdb   After cloning and sequencing of coding DNA, the sequence of the X\nprotein had been determined. The sequence of X is given here:  LAAVSVDCSEYPKPACTLEYRPLCGSDNKTYGNKCNFCNAVVESNGTLTLSHFGKC  In normal conditions, this X protein is expressed but we have no idea\nabout it function. The goal of this practical work is to collect the\nmaximum of information about structure and function of the X protein.",
            "title": "Introduction to protein sequences and structures analysis"
        },
        {
            "location": "/proteins/#i-search-patterns-profiles",
            "text": "A way to identify the function of X is to look if it contains signatures\n(pattern) of a function or a protein family.  2 options:  http://prosite.expasy.org/scanprosite/  NPS@  and follow the link \"ProScan: scan a\nsequence for sites/signatures against PROSITE database\" (activate:\nInclude documentation in result file).   Question    Which signature(s) could you identify? Which specific features in\n    this protein?    Try to change the parameters and comment the results.      Note  InterPro gives a summary of several methods. You can find it at the EBI .   Keep the signatures that could attest the function in your notepad.   What do you think about the function of X?",
            "title": "I - Search Patterns, Profiles"
        },
        {
            "location": "/proteins/#ii-search-homolog-proteins-with-blast",
            "text": "Go to the  NCBI BLAST  page    Choose the Protein Blast (blastp)    Paste your sequence    Select the Swissprot database     Question  Did you identify homologs? What are their function(s)?",
            "title": "II - Search homolog proteins with BLAST"
        },
        {
            "location": "/proteins/#iii-multiple-sequences-alignment",
            "text": "Select several homolog sequences from the Blast results.    Perform a multiple sequence alignment (MSA) of these sequence using\n    Clustal Omega for example    Try other MSA tools (for example Tcoffee and Muscle)     Question  Do you observe differences between the results obtained from\ndifferent algorithms?  What can you observe in these MSAs?   Info : You could also retrieve the selected sequences in Fasta format\nand perform MSAs elsewhere  Clustal Omega and Muscle: available in Seaview alignment viewer  Tcoffee:  http://tcoffee.vital-it.ch/apps/tcoffee/index.html  Other tools:  http://expasy.org/genomics/sequence_alignment",
            "title": "III - Multiple sequences alignment"
        },
        {
            "location": "/proteins/#iv-the-y-protein",
            "text": "Another experiment had shown that the X protein was interacting\nspecifically with another protein: Y.  After purification of the active Y protein, from the complex, a partial\nsequence of Y was obtained (by protein extremity sequencing).  The corresponding peptide could be:  ISGGD  or  ISGGN",
            "title": "IV - The Y protein"
        },
        {
            "location": "/proteins/#1-identification-of-the-y-sequence-using-prosite-patterns",
            "text": "Design the pattern (regular expression) corresponding to these\n    peptides.    Search the sequences containing this pattern in SwissProt using\n     PATTERN SEARCH  at\n    SIB or\n     PATTINPROT \n    at NPS@.    If needed, use the help to design your pattern.   Question  How many results do you get? How can you identify the right one?   Once the Y protein sequence identified, copy the FASTA sequence in your\nnotepad.",
            "title": "1. Identification of the Y sequence using PROSITE patterns"
        },
        {
            "location": "/proteins/#2-composition-analysis",
            "text": "After purification of the Y active protein, the amino-acid composition\nhas been determined (% of each aa in the protein) and is given in the\nfollowing table:                   A  8.11  F  2.70  L  3.78  R  4.32  X  0    B  0  G  17.30  M  1.08  S  11.89  Y  5.41    C  2.16  H  1.08  N  5.41  T  15.14  Z  0    D  3.78  I  3.78  P  2.70  V  7.57      E  1.08  K  0.54  Q  1.08  W  1.08         Compute the composition of the sequence that you retrieve. Use\n     PROTPARAM  or the tool\n    'Amino-acid composition' at  NPS@    Compare this computed composition with the composition of Y\n    experimentally determined.     Question  Do you observe differences? Explain.",
            "title": "2. Composition analysis"
        },
        {
            "location": "/proteins/#3-search-pattern-in-y",
            "text": "Once the correct sequence of Y obtained, keep it in your notepad, you\nwill need it for the following analyses.   Question  Identify the signatures (motifs, Pfam profiles) of Y using PROSCAN\nand/or Interpro.",
            "title": "3. Search pattern in Y"
        },
        {
            "location": "/proteins/#4-identification-of-homologs-to-y",
            "text": "Use NCBI BLASTP or NPS@\n     BLASTP \n    against SwissProt database to search sequences similar to Y.    Use PSI-BLAST (with SwissProt) to see if you can detect more distant\n    sequences.    Select sequences from BLAST and/or PSI-BLAST results to perform a\n    multiple sequence alignment.     Question   Did you observe difference in the results of BLAST and PSI-BLAST?\nComment.  Propose a strategy to retrieve all the proteins having the same\ncatalytic activity as Y protein.",
            "title": "4. Identification of homologs to Y"
        },
        {
            "location": "/proteins/#v-secondary-structure-prediction-for-x-and-y",
            "text": "Go to the  consensus secondary structure\n    prediction \n    page at NPS@.    Analyze the secondary structure of the protein Y. Include secondary\n    structure predictions by methods (DPM, GOR1, PREDATOR, SIMPA96).     Question   Conclude on the organization of secondary structures.  Perform the same analysis for X protein.",
            "title": "V - Secondary structure prediction for X and Y"
        },
        {
            "location": "/proteins/#vi-comparison-with-solved-structures",
            "text": "",
            "title": "VI - Comparison with solved structures"
        },
        {
            "location": "/proteins/#1-the-z-protein",
            "text": "The structure of a protein Z has just been published. The sequence of\nprotein Z is shown below:  IAGGEAITTGGSRCSLGFNVSVNGVAHALTAGHCTNISASWSIGTRTGTSFPNNDYGIIRHSNPAAANGRVYLYNGSYQD  ITTAGNAFVGQAVQRSGSTTGLRSGSVTGLNATVNYGSSGIVYGMIQTNVCAQPGDSGGSLFAGSTALGLTSGGSGNCRT  GGTTFYQPVTEALSAYGATVL   Question  Could you use this information for the study of protein Y? Make your\nown analysis.",
            "title": "1. The Z protein"
        },
        {
            "location": "/proteins/#2-find-the-correct-structures",
            "text": "Download and install  Deep-View -\n    SwissPDBViewer . You can\n    find the tutorial and user guide of DeepView\n     here .    Download to the archive  PDB_files_part6.zip  and unzip it.    You might find 8 PDB files in the directory.    Open them with DeepView.    Display the secondary structure representation mode (see part\n    VII-A-5 and/or the user guide).     Question  Try to identify the structures corresponding to X and Y proteins.",
            "title": "2. Find the correct structures"
        },
        {
            "location": "/proteins/#vii-tridimensional-protein-structure-play-with-3d-structures-using-swisspdbviewer-deepview",
            "text": "Go to the  Protein Data Bank    Search and download the following PDB files: 1CRN, 1LDM.    You will visualize these protein structures using DeepView",
            "title": "VII - Tridimensional protein structure: Play with 3D structures using SwissPDBViewer (DeepView)"
        },
        {
            "location": "/proteins/#a-analyze-protein-structures-with-deepview",
            "text": "",
            "title": "A - Analyze protein structures with DeepView"
        },
        {
            "location": "/proteins/#1-load-a-3d-structure",
            "text": "File => Open  Choose the 1CRN.pdb file that you have downloaded from the PDB.",
            "title": "1. Load a 3D structure"
        },
        {
            "location": "/proteins/#2-visualize-the-number-of-chains",
            "text": "Is it only the protein or can we find ligands? Is it a monomer or a\npolymer?",
            "title": "2. Visualize the number of chains"
        },
        {
            "location": "/proteins/#3-visualize-the-general-shape",
            "text": "Try to get the actual space taken by the molecule. You need to use the\ncontrol panel and use the ':v' column to activate the space-filling\nspheres representation (+ menu Display > Render in solid 3D).  Test also the Slab mode to visualize the space within the molecule:\nDisplay > Slab",
            "title": "3. Visualize the general shape"
        },
        {
            "location": "/proteins/#4-display-a-distance-between-2-atoms-angle-between-3-atoms",
            "text": "Use the graphical panel. You can now measure the real dimensions of your\nprotein",
            "title": "4. Display a distance between 2 atoms, angle between 3 atoms"
        },
        {
            "location": "/proteins/#5-visualize-secondary-structure-elements",
            "text": "In the control panel, activate \"ribbon\" (rbn). You can also color the\nmolecule by secondary structures.",
            "title": "5. Visualize secondary structure elements"
        },
        {
            "location": "/proteins/#6-visualize-ligands-if-there-is-any",
            "text": "Select and color them. You could also remove the rest, or better, have a\nlook at the residues that are around those ligands (radius function in\nthe graphical panel).",
            "title": "6. Visualize ligands (if there is any)"
        },
        {
            "location": "/proteins/#7-analysis-of-other-protein-structures",
            "text": "The teacher will give PDB codes of other structures to analyze. Choose\nDeepView or Rasmol/Jmol to do so, that is up to you!",
            "title": "7. Analysis of other protein structures"
        },
        {
            "location": "/proteins/#b-optional-if-you-want-to-use-rasmoljmol",
            "text": "",
            "title": "B - Optional: if you want to use RasMol/Jmol"
        },
        {
            "location": "/proteins/#1-load-a-3d-structure_1",
            "text": "File => Open  Choose the 1CRN.pdb file that you have downloaded from the PDB.   HELP SECTION FOR RASMOL  Molecule main moves with the mouse:  Left button: XY rotation  Left button + Shift: Zoom  Right button: Translation  Right button + Shift: Z rotation  Keep the graphical window and the command (text) window on your screen\n(> \u200b\u200bis a command to type in the text window).  For each selection (SELECT command), the number of selected atoms\nappears in the text window. After you can apply an action to be able to\nvisualize the elements that you have selected (e.g. COLOR GREEN).  Ctrl+Z does not exist in Rasmol. You can type the command RESET.  If you want to come back in a standard representation of your molecule,\ntype:  SELECT ALL\n\nCPK  => This will reset previous actions on representation modes (but keep\ncolors). CPK: space-filling spheres representation  COLOR CPK: colors \\'atom\\' objects by the atom (element) type   Help for Jmol:  A lot of \"actions\" (color, selection...) are available by right clicking\non the main screen  To get the terminal window: menu File > Console",
            "title": "1. Load a 3D structure"
        },
        {
            "location": "/proteins/#2-example-visualize-the-disulfide-bonds",
            "text": "Type in the text window  SELECT CYS  The text window \\\"answers\\\" 36 atoms selected (selected cysteine's\natoms)  COLOR GREEN   Observe the graphics window.   RESTRICT CYS   Compare with the SELECT command   Highlight the disulfide bonds:  SSBONDS\n\nCOLOR YELLOW\n\nSSBONDS 75\n\nCOLOR CPK",
            "title": "2. Example: visualize the disulfide bonds"
        },
        {
            "location": "/proteins/#3-visualize-secondary-structure-elements",
            "text": "SSBONDS OFF (remove SS bonds)\n\nSELECT ALL\n\nCARTOONS\n\nCOLOR STRUCTURE",
            "title": "3. Visualize secondary structure elements"
        },
        {
            "location": "/proteins/#4-display-a-distance-between-2-atoms",
            "text": "Activate the compute distance mode typing:  SET PICKING DISTANCE  Then, you can click the 2 atoms.  You can display angle values typing:  SET PICKING ANGLE  Then pick the 3 atoms",
            "title": "4. Display a distance between 2 atoms"
        },
        {
            "location": "/proteins/#5-other-useful-commands",
            "text": "SHOW SEQUENCE\n\nSHOW INFO\n\nSELECT ALL\n\nCPK ON\n\nRESTRICT NOT HOH (remove water molecules)\n\nCPK OFF\n\nHBONDS\n\nSELECT CYCLIC AND NOT PRO\n\nSTEREO ON  Try them to better understand the Rasmol command language.",
            "title": "5. Other useful commands"
        },
        {
            "location": "/proteins/#6-store-a-command-script-and-reload-it",
            "text": "Repeat the actions described in paragraph 2  WRITE SCRIPT MY_SCRIPT.SC  Exit the software (File => Quit)  Restart the software  SOURCE MY_SCRIPT.SC",
            "title": "6. Store a command script and reload it"
        },
        {
            "location": "/proteins/#7-select-the-atoms-in-a-sphere",
            "text": "File => Close  Load the file 1LDM.pdb  Discover and analyze the molecule (number of channels, ligands,  etc .)  To select all the atoms in a 3\u00c5 radius sphere centered on a ligand\n( e.g.  NAD)  SELECT ALL\n\nCOLOR CHAIN\n\nSELECT WITHIN (3.0, NAD)\n\nCPK  Option => Slab Mode (comment).",
            "title": "7. Select the atoms in a sphere"
        },
        {
            "location": "/unix/",
            "text": "Introduction to Unix\n\n\nMost of the introduction to Unix material can be found at \nhttps://software-carpentry.org\n\n\nMany thanks to them for existing!\n\n\nUseful resources\n\n\nBelow you will find links to various useful resources for learning or using the UNIX shell.\n\n\n\n\nLink to the course material from software carpentry\n\n\n\n\nreference of concepts and commands seen during the lesson\n\n\n\n\n\n\nshell commands explained\n - a website that shows the help text of any command\n\n\n\n\nawesome bash\n - an awesome list of resources about the bash shell\n\n\ntldp\n - the linux documentation project (the books can be hard to digest but are very thorough)",
            "title": "Unix"
        },
        {
            "location": "/unix/#introduction-to-unix",
            "text": "Most of the introduction to Unix material can be found at  https://software-carpentry.org  Many thanks to them for existing!",
            "title": "Introduction to Unix"
        },
        {
            "location": "/unix/#useful-resources",
            "text": "Below you will find links to various useful resources for learning or using the UNIX shell.   Link to the course material from software carpentry   reference of concepts and commands seen during the lesson    shell commands explained  - a website that shows the help text of any command   awesome bash  - an awesome list of resources about the bash shell  tldp  - the linux documentation project (the books can be hard to digest but are very thorough)",
            "title": "Useful resources"
        },
        {
            "location": "/cloud/",
            "text": "Introduction to Cloud Computing\n\n\nIn this lesson you'll learn how to connect and use a linux server.\nMost bioinformaticians worldwide connect daily to cloud computing services to perform their analyses.\n\n\nThere are several reasons for this.\nFirstly biology - like most other areas of science - is dealing with a deluge of data due to the rapid advancement of data collection methods.\nIt is now common that data collected for an experiment doesn't fit on a researcher's laptop and that the resources needed for running an analysis far exceed a desktop computer's computing power.\n\n\nSecondly the vast majority of research software are developed and released for linux. Most people run MacOS or Windows on their desktop computers and laptop, which makes the installation of some software difficult or at the very least inconvenient.\n\n\nWhat is the cloud anyway?\n\n\nThe cloud is basically lots of servers (thing big big computers) stacked together in a giant, powerful infrastructure. You can lend part of this infrastructure for your computing needs. While it is not cheap, it is generally scalable and guarantees a stable environment.\n\n\n\n\nIn research there are two approaches to lend computing time and power: either (a) you lend computing time and resources from a commercial provider or you obtain access to a research computing infrastructure. Some countries have built national infrastructures where you can apply for computing time for your research projects.\nMost academic institutions or departments also have their own computing resources.\n\n\nPopular cloud/HPC services\n\n\n\n\n\n\nAcademic:\n\n\n\n\n\ud83c\uddf8\ud83c\uddea \nUPPMAX\n\n\n\ud83c\uddfa\ud83c\uddf8 \nJetstream\n\n\n\n\n\n\n\n\nCommercial:\n\n\n\n\namazon web services\n\n\ngoogle cloud\n\n\nmicrosoft azure\n\n\n\n\n\n\n\n\nFor this tutorial, we'll use Microsoft azure.\n\n\nConnecting to another computer\n\n\nConnecting to another computer is usually done using the \nSSH\n protocol, which is an encrypted way to connect over the network.\nBefore connecting to our cloud computers, we need to create them.\n\n\n\n\nNote\n\n\nWhile the cloud is effectively \"someone else's computer\" the way we use commercial cloud infrastructures is by create a virtual computer with the computing resources that we pay for.\n\n\n\n\n\n\nNote\n\n\nFor this course, your instructor will create a virtual instance on the Azure cloud for you\n\n\n\n\nAuthentication\n\n\nThere are two main ways to authenticate to a remote server via SSH: using a password or using a cryptographic key.\nUsing a key prevent people to try to guess your password and since brute-force attacks are very common on machines that have public IPs, we'll use keys.\n\n\n\n\nNote\n\n\nHow do keys work?\nThe keys we will use to connect to our machine work by pairs: a \npublic\n key and a \nprivate\n key.\nAny machine you want to connect to using keys has to contain your public key, while the private key should always stay on your computer.\nWhen you try to connect to a machine and the two keys match, you successfully connect!\nSince your instructor will create a virtual machine for you, he will also provide you with a private key for this machine.\n\n\n\n\nPut the private key your instructor gave you in the \n~/.ssh\n folder:\n\n\nmkdir -p ~/.ssh\nchmod 700 ~/ssh\nmv ~/Downloads/azure_rsa ~/.ssh\nchmod 600 ~/.ssh/azure_rsa\n\n\n\n\nFirst connection\n\n\nNow you can connect to the virtual machine that was assigned to you\n\n\nssh -i .ssh/azure_rsa student@IP_ADDRESS\n\n\n\n\ndo not forget to replace \nIP_ADDRESS\n by your virtual machine ip in the above command!\n\n\nGetting around\n\n\nNow that you are connected to the cloud, there is a few things you should know.\n\n\n\n\nFor all intents and purposes it is \nalmost\n like being in the terminal of your own linux machine.\nAll commands we've seen during the \nunix shell\n lesson will work\n\n\nyou are administrator on your cloud machine. You have the power to break things...\n\n\n... but do not freak out! the machine is not actually real, so anything you break can be rebuilt in a matter of minutes\n\n\nto exit the virtual machine, press \n^D\n or type \nexit\n\n\n\n\nTransferring files\n\n\nOne thing that \nwill\n happen sooner while working in the cloud is that you will want to transfer files to or from your machine.\n\n\nThe command to transfer files over \nssh\n is very similar to \ncp\nand is called \nscp\n, for \nsecure\n copy.\n\n\nCopying a file from your computer to the server\n\n\nOn your computer, firstly create a file:\n\n\necho \"I will put that file on my cloud machine!\" > my_file.txt\n\n\n\n\nthen use \nscp\n to transfer the file\n\n\nscp my_file.txt student@IP_ADDRESS:/home/student/\n\n\n\n\nCopying a file from the server to your computer\n\n\nFirst, remove \nmy_file.txt\n from your local computer\n\n\nrm my_file.txt\n\n\n\n\nthen copy it back from the server\n\n\nscp student@IP_ADDRESS:/home/student/my_file.txt .",
            "title": "Cloud Computing"
        },
        {
            "location": "/cloud/#introduction-to-cloud-computing",
            "text": "In this lesson you'll learn how to connect and use a linux server.\nMost bioinformaticians worldwide connect daily to cloud computing services to perform their analyses.  There are several reasons for this.\nFirstly biology - like most other areas of science - is dealing with a deluge of data due to the rapid advancement of data collection methods.\nIt is now common that data collected for an experiment doesn't fit on a researcher's laptop and that the resources needed for running an analysis far exceed a desktop computer's computing power.  Secondly the vast majority of research software are developed and released for linux. Most people run MacOS or Windows on their desktop computers and laptop, which makes the installation of some software difficult or at the very least inconvenient.",
            "title": "Introduction to Cloud Computing"
        },
        {
            "location": "/cloud/#what-is-the-cloud-anyway",
            "text": "The cloud is basically lots of servers (thing big big computers) stacked together in a giant, powerful infrastructure. You can lend part of this infrastructure for your computing needs. While it is not cheap, it is generally scalable and guarantees a stable environment.   In research there are two approaches to lend computing time and power: either (a) you lend computing time and resources from a commercial provider or you obtain access to a research computing infrastructure. Some countries have built national infrastructures where you can apply for computing time for your research projects.\nMost academic institutions or departments also have their own computing resources.",
            "title": "What is the cloud anyway?"
        },
        {
            "location": "/cloud/#popular-cloudhpc-services",
            "text": "Academic:   \ud83c\uddf8\ud83c\uddea  UPPMAX  \ud83c\uddfa\ud83c\uddf8  Jetstream     Commercial:   amazon web services  google cloud  microsoft azure     For this tutorial, we'll use Microsoft azure.",
            "title": "Popular cloud/HPC services"
        },
        {
            "location": "/cloud/#connecting-to-another-computer",
            "text": "Connecting to another computer is usually done using the  SSH  protocol, which is an encrypted way to connect over the network.\nBefore connecting to our cloud computers, we need to create them.   Note  While the cloud is effectively \"someone else's computer\" the way we use commercial cloud infrastructures is by create a virtual computer with the computing resources that we pay for.    Note  For this course, your instructor will create a virtual instance on the Azure cloud for you",
            "title": "Connecting to another computer"
        },
        {
            "location": "/cloud/#authentication",
            "text": "There are two main ways to authenticate to a remote server via SSH: using a password or using a cryptographic key.\nUsing a key prevent people to try to guess your password and since brute-force attacks are very common on machines that have public IPs, we'll use keys.   Note  How do keys work?\nThe keys we will use to connect to our machine work by pairs: a  public  key and a  private  key.\nAny machine you want to connect to using keys has to contain your public key, while the private key should always stay on your computer.\nWhen you try to connect to a machine and the two keys match, you successfully connect!\nSince your instructor will create a virtual machine for you, he will also provide you with a private key for this machine.   Put the private key your instructor gave you in the  ~/.ssh  folder:  mkdir -p ~/.ssh\nchmod 700 ~/ssh\nmv ~/Downloads/azure_rsa ~/.ssh\nchmod 600 ~/.ssh/azure_rsa",
            "title": "Authentication"
        },
        {
            "location": "/cloud/#first-connection",
            "text": "Now you can connect to the virtual machine that was assigned to you  ssh -i .ssh/azure_rsa student@IP_ADDRESS  do not forget to replace  IP_ADDRESS  by your virtual machine ip in the above command!",
            "title": "First connection"
        },
        {
            "location": "/cloud/#getting-around",
            "text": "Now that you are connected to the cloud, there is a few things you should know.   For all intents and purposes it is  almost  like being in the terminal of your own linux machine.\nAll commands we've seen during the  unix shell  lesson will work  you are administrator on your cloud machine. You have the power to break things...  ... but do not freak out! the machine is not actually real, so anything you break can be rebuilt in a matter of minutes  to exit the virtual machine, press  ^D  or type  exit",
            "title": "Getting around"
        },
        {
            "location": "/cloud/#transferring-files",
            "text": "One thing that  will  happen sooner while working in the cloud is that you will want to transfer files to or from your machine.  The command to transfer files over  ssh  is very similar to  cp and is called  scp , for  secure  copy.",
            "title": "Transferring files"
        },
        {
            "location": "/cloud/#copying-a-file-from-your-computer-to-the-server",
            "text": "On your computer, firstly create a file:  echo \"I will put that file on my cloud machine!\" > my_file.txt  then use  scp  to transfer the file  scp my_file.txt student@IP_ADDRESS:/home/student/",
            "title": "Copying a file from your computer to the server"
        },
        {
            "location": "/cloud/#copying-a-file-from-the-server-to-your-computer",
            "text": "First, remove  my_file.txt  from your local computer  rm my_file.txt  then copy it back from the server  scp student@IP_ADDRESS:/home/student/my_file.txt .",
            "title": "Copying a file from the server to your computer"
        },
        {
            "location": "/software/",
            "text": "Installing software\n\n\nBioinformatics is a relatively new (It's younger that Erik!) and fast-progressing field.\nTherefore new software as well as new versions of existing software are released on a regular basis.\n\n\nDuring this course as well as during your future career as a bioinformatician ( ;-) ) you will be confronted quite often to the installation of new software on UNIX platforms (i.e. the server you are using at the moment)\n\n\nCompiled and Interpreted languages\n\n\nProgramming languages in the bioinformatics world - and in general - can be separated in two categories: \nintepreted\n languages, and \ncompiled\n languages. While with \ninterpreted\n languages you write scripts, and execute them (as we saw with the \nbash\n scripts during the UNIX lesson) it is different for compiled languages: an extra step is required\n\n\nCompilation\n\n\nAs from \nWikipedia\n, compilation is the translation of source code into object code by a compiler.\n\n\nThat's right.\nThe extra step required by compiled languages is translating the source code, that is the lines of code the programmer(s) wrote into a language that your computer understand better, usually binary (1s and 0s).\n\n\nThe big advantage of compiled languages is that they are much faster than interpreted languages.\nHowever, programming in them is usually slower and more difficult than in interpreted languages. Using them or not for a software project is a trade-off between development-time, and how much faster your software could run if it was programmed using a compiled language.\n\n\nThe most popular compiled language is the C programming language, which Linux is mainly written in.\n\n\nPackage Managers\n\n\nAll modern linux distributions come with a \npackage manager\n, i.e. a tool that automates installation of software.\nIn most cases the software manager download already compiled binaries and installs them in your system. We'll see how it works in a moment\n\n\nLet us install our first package!\n\n\nThe package manager for \nUbuntu\n is called \nAPT\n. Like most package managers, the syntax will look like this:\n\n\n[package_manager] [action] [package_name]\n\n\n\n\nWe'll use apt to install a local version of \nncbi-blast\n that you've use previously.\n\n\nFirst we search if the package is available\n\n\napt search ncbi-blast\n\n\n\n\nThere seems to be two versions of it. The legacy version is probably outdated, so let us investigate the other one\n\n\napt show ncbi-blast+\n\n\n\n\nIt seems to be what we are looking for, we install it with:\n\n\napt install ncbi-blast+\n\n\n\n\n\n\nQuestion\n\n\nDid it work? What could have been wrong?\n\n\n\n\nYou should have gotten an error message asking if you are \nroot\n.\n\nThe user \nroot\n is the most powerful user in a linux system and usually has extra rights that a regular user does not have.\n\nTo install software in the default system location with apt, you have to have special permissions.\n\nWe can \"borrow\" those permissions from \nroot\n by prefixing our command with \nsudo\n.\n\n\nsudo apt install ncbi-blast+\n\n\n\n\nNow if you execute\n\n\nblastn -help\n\n\n\n\nit should print the (rather long) error message of the blastn command.\n\n\n\n\nQuestion\n\n\nWhy does blast has different executable?\n\nWhat is the difference between blastn and blastp?  \n\n\n\n\nDownloading and unpacking\n\n\nAlthough most popular software can be installed with your distribution's package manager, sometimes (especially in some fast-growing areas of bioinformatics) the software you want isn't available through a package manager.\n\n\nWe'll install \nspades\n, a popular genome assembly tool. Let's imagine it is not available in the apt sources. We'd have to:\n\n\n\n\ndownload the source code\n\n\ncompile the software\n\n\nmove it at the right place on our system\n\n\n\n\nWhich is quite cumbersome, especially the compilation.\nLuckily, it is fairly common for developers to make linux binaries - that is compiled version of the software - already available for download.\n\n\nFirst let us create a directory for all our future installs:\n\n\nmkdir -p ~/install\ncd ~/install\n\n\n\n\nThe spades binaries are available on their website, \nhttp://cab.spbu.ru/software/spades/\n\n\nDownload them with\n\n\nwget http://cab.spbu.ru/files/release3.11.1/SPAdes-3.11.1-Linux.tar.gz\n\n\n\n\nand uncompress\n\n\ntar xvf SPAdes-3.11.1-Linux.tar.gz\n\n\n\n\ncd SPAdes-3.11.1-Linux/bin/\n\n\n\n\nand now if we execute \nspades.py\n\n\n./spades.py\n\n\n\n\nwe get the help of the spades assembler!\n\n\nA minor inconvenience is that right now\n\n\npwd\n# /home/hadrien/install/SPAdes-3.11.1-Linux/bin\n\n\n\n\nwe have to always go to this directory to run \nspades.py\n, or call the software with the full path.\nWe'd like to be able to execute \nspades\n from anywhere, like we do with \nls\n and \ncd\n.\n\n\nIn most linux distributions, which directory can contain software that are executed from anywhere is defined by an environment variable: \n$PATH\n\n\nLet us take a look:\n\n\necho $PATH\n# /home/hadrien/bin:/home/hadrien/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n\n\n\n\nTo make \nspades.py\n available from anywhere we have to put it in one of the above locations.\n\n\n\n\nNote\n\n\nWhen \napt\n installs software it usually places it in \n/usr/bin\n, which requires administration privileges.\nThis is why we needed \nsudo\n for installing packages earlier.\n\n\n\n\nmkdir -p ~/.local/bin\nmv * ~/.local/bin/\n\n\n\n\nEt voil\u00e0! Now you can execute \nspades.py\n from anywhere!\n\n\nInstalling from source\n\n\nFor some bioinformatics software, binaries are not available. In that case you have to download the source code, and compile it yourself for your system.\n\n\nThis is the case of \nsamtools\n per example. \nsamtools\n is one of the most popular bioinformatics software and allows you to deal with \nbam\n and \nsam\n files (more about that later)\n\n\nWe'll need a few things to be able to compile samtools, notably \nmake\n and a C compiler, \ngcc\n\n\nsudo apt install make gcc\n\n\n\n\nsamtools also need some libraries that are not installed by default on an ubuntu system.\n\n\nsudo apt install libncurses5-dev libbz2-dev liblzma-dev libcurl4-gnutls-dev\n\n\n\n\nNow we can download and unpack the source code:\n\n\ncd ~/install\nwget https://github.com/samtools/samtools/releases/download/1.6/samtools-1.6.tar.bz2\ntar xvf samtools-1.6.tar.bz2\ncd samtools-1.6\n\n\n\n\nCompiling software written in C usually follows the same 3 steps.\n\n\n\n\n./configure\n to configure the compilation options to our machine architecture\n\n\nwe run \nmake\n to compile the software\n\n\nwe run \nmake install\n to move the compiled binaries into a location in the \n$PATH\n\n\n\n\n./configure\nmake\nmake install\n\n\n\n\n\n\nWarning\n\n\nDid \nmake install\n succeed? Why not?\n\n\n\n\nAs we saw before, we need \nsudo\n to install packages to system locations with \napt\n.\n\nmake install\n follows the same principle and tries by default to install software in \n/usr/bin\n\n\nWe can change that default behavior by passing options to \nconfigure\n, but first we have to clean our installation:\n\n\nmake clean\n\n\n\n\nthan we can run configure, make and make install again\n\n\n./configure --prefix=/home/$(whoami)/.local/\nmake\nmake install\n\n\n\n\nsamtools\n\n\n\n\n\n\nQuestion\n\n\nThe bwa source code is available on github, a popular code sharing platform (more on this in the git lesson!).\nNavigate to \nhttps://github.com/lh3/bwa\n then in release copy the link behind \nbwa-0.7.17.tar.bz2\n\n- Install bwa!\n\n\n\n\nInstalling python packages\n\n\nWhile compiled languages are faster than interpreted languages, they are usually harder to learn, code in and debug.\nFor theses reasons you'll often find many bioinformatics packages written in interpreted languages such as \npython\n or \nruby\n.\n\n\nWhile historically it has been a pain to install software written in interpreted languages, most modern languages now come with their own package managers! For example:\n\n\n\n\nPython has \npip\n\n\nRuby has \ngem\n\n\nJavascript has \nnpm\n\n\n...\n\n\n\n\nMost of theses package managers have similar syntaxes.\nWe will focus on python here since it's one of the most popular languages in bioinformatics.\n\n\n\n\nNote\n\n\nYou will notice the absence of R here.\nR is mostly used interactively and installing packages in R will be part of the R part of the course.\n\n\n\n\nYour ubuntu comes with an old version of python. We start with installing a newer one\n\n\ncd ~/install\nwget https://www.python.org/ftp/python/3.6.4/Python-3.6.4.tar.xz\ntar xvf Python-3.6.4.tar.xz\ncd Python-3.6.4\n./configure --prefix=/home/$(whoami)/.local/\nmake -j2\nmake install\n\n\n\n\n\n\nQuestion\n\n\nWhat does the \nmake\n option \n-j2\n do?\n\n\n\n\nwhich python3\nwhich pip3\n\n\n\n\nWe now have the newest python installed.\n\n\nLet us install our first python package\n\n\npip3 install multiqc\n\n\n\n\nit should take a while and install \nmultiqc\n as well as all the necessary dependencies.\n\n\nto see if multiqc was properly installed:\n\n\nmultiqc -h\n\n\n\n\nExercises\n\n\nDuring the following weeks we'll use a lot of different bioinformatics software to perform a variety of tasks.\n\n\n\n\nTip\n\n\nMost software come with a file named \nINSTALL\n or \nREADME\n.\nSuch file usually contains instructions on how to install!\n\n\n\n\n\n\nNote\n\n\nunless indicated otherwise, try with \napt\n first\n\n\n\n\n\n\nNote\n\n\ndo not hesitate to ask your teacher for help!\n\n\n\n\nLet's install a few:\n\n\n\n\nfastqc\n\n\nscythe\n\n\nsickle\n\n\nbowtie2\n\n\nmegahit\n\n\nquast\n\n\nprokka",
            "title": "Installing Software"
        },
        {
            "location": "/software/#installing-software",
            "text": "Bioinformatics is a relatively new (It's younger that Erik!) and fast-progressing field.\nTherefore new software as well as new versions of existing software are released on a regular basis.  During this course as well as during your future career as a bioinformatician ( ;-) ) you will be confronted quite often to the installation of new software on UNIX platforms (i.e. the server you are using at the moment)",
            "title": "Installing software"
        },
        {
            "location": "/software/#compiled-and-interpreted-languages",
            "text": "Programming languages in the bioinformatics world - and in general - can be separated in two categories:  intepreted  languages, and  compiled  languages. While with  interpreted  languages you write scripts, and execute them (as we saw with the  bash  scripts during the UNIX lesson) it is different for compiled languages: an extra step is required",
            "title": "Compiled and Interpreted languages"
        },
        {
            "location": "/software/#compilation",
            "text": "As from  Wikipedia , compilation is the translation of source code into object code by a compiler.  That's right.\nThe extra step required by compiled languages is translating the source code, that is the lines of code the programmer(s) wrote into a language that your computer understand better, usually binary (1s and 0s).  The big advantage of compiled languages is that they are much faster than interpreted languages.\nHowever, programming in them is usually slower and more difficult than in interpreted languages. Using them or not for a software project is a trade-off between development-time, and how much faster your software could run if it was programmed using a compiled language.  The most popular compiled language is the C programming language, which Linux is mainly written in.",
            "title": "Compilation"
        },
        {
            "location": "/software/#package-managers",
            "text": "All modern linux distributions come with a  package manager , i.e. a tool that automates installation of software.\nIn most cases the software manager download already compiled binaries and installs them in your system. We'll see how it works in a moment  Let us install our first package!  The package manager for  Ubuntu  is called  APT . Like most package managers, the syntax will look like this:  [package_manager] [action] [package_name]  We'll use apt to install a local version of  ncbi-blast  that you've use previously.  First we search if the package is available  apt search ncbi-blast  There seems to be two versions of it. The legacy version is probably outdated, so let us investigate the other one  apt show ncbi-blast+  It seems to be what we are looking for, we install it with:  apt install ncbi-blast+   Question  Did it work? What could have been wrong?   You should have gotten an error message asking if you are  root . \nThe user  root  is the most powerful user in a linux system and usually has extra rights that a regular user does not have. \nTo install software in the default system location with apt, you have to have special permissions. \nWe can \"borrow\" those permissions from  root  by prefixing our command with  sudo .  sudo apt install ncbi-blast+  Now if you execute  blastn -help  it should print the (rather long) error message of the blastn command.   Question  Why does blast has different executable? \nWhat is the difference between blastn and blastp?",
            "title": "Package Managers"
        },
        {
            "location": "/software/#downloading-and-unpacking",
            "text": "Although most popular software can be installed with your distribution's package manager, sometimes (especially in some fast-growing areas of bioinformatics) the software you want isn't available through a package manager.  We'll install  spades , a popular genome assembly tool. Let's imagine it is not available in the apt sources. We'd have to:   download the source code  compile the software  move it at the right place on our system   Which is quite cumbersome, especially the compilation.\nLuckily, it is fairly common for developers to make linux binaries - that is compiled version of the software - already available for download.  First let us create a directory for all our future installs:  mkdir -p ~/install\ncd ~/install  The spades binaries are available on their website,  http://cab.spbu.ru/software/spades/  Download them with  wget http://cab.spbu.ru/files/release3.11.1/SPAdes-3.11.1-Linux.tar.gz  and uncompress  tar xvf SPAdes-3.11.1-Linux.tar.gz  cd SPAdes-3.11.1-Linux/bin/  and now if we execute  spades.py  ./spades.py  we get the help of the spades assembler!  A minor inconvenience is that right now  pwd\n# /home/hadrien/install/SPAdes-3.11.1-Linux/bin  we have to always go to this directory to run  spades.py , or call the software with the full path.\nWe'd like to be able to execute  spades  from anywhere, like we do with  ls  and  cd .  In most linux distributions, which directory can contain software that are executed from anywhere is defined by an environment variable:  $PATH  Let us take a look:  echo $PATH\n# /home/hadrien/bin:/home/hadrien/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin  To make  spades.py  available from anywhere we have to put it in one of the above locations.   Note  When  apt  installs software it usually places it in  /usr/bin , which requires administration privileges.\nThis is why we needed  sudo  for installing packages earlier.   mkdir -p ~/.local/bin\nmv * ~/.local/bin/  Et voil\u00e0! Now you can execute  spades.py  from anywhere!",
            "title": "Downloading and unpacking"
        },
        {
            "location": "/software/#installing-from-source",
            "text": "For some bioinformatics software, binaries are not available. In that case you have to download the source code, and compile it yourself for your system.  This is the case of  samtools  per example.  samtools  is one of the most popular bioinformatics software and allows you to deal with  bam  and  sam  files (more about that later)  We'll need a few things to be able to compile samtools, notably  make  and a C compiler,  gcc  sudo apt install make gcc  samtools also need some libraries that are not installed by default on an ubuntu system.  sudo apt install libncurses5-dev libbz2-dev liblzma-dev libcurl4-gnutls-dev  Now we can download and unpack the source code:  cd ~/install\nwget https://github.com/samtools/samtools/releases/download/1.6/samtools-1.6.tar.bz2\ntar xvf samtools-1.6.tar.bz2\ncd samtools-1.6  Compiling software written in C usually follows the same 3 steps.   ./configure  to configure the compilation options to our machine architecture  we run  make  to compile the software  we run  make install  to move the compiled binaries into a location in the  $PATH   ./configure\nmake\nmake install   Warning  Did  make install  succeed? Why not?   As we saw before, we need  sudo  to install packages to system locations with  apt . make install  follows the same principle and tries by default to install software in  /usr/bin  We can change that default behavior by passing options to  configure , but first we have to clean our installation:  make clean  than we can run configure, make and make install again  ./configure --prefix=/home/$(whoami)/.local/\nmake\nmake install  samtools   Question  The bwa source code is available on github, a popular code sharing platform (more on this in the git lesson!).\nNavigate to  https://github.com/lh3/bwa  then in release copy the link behind  bwa-0.7.17.tar.bz2 \n- Install bwa!",
            "title": "Installing from source"
        },
        {
            "location": "/software/#installing-python-packages",
            "text": "While compiled languages are faster than interpreted languages, they are usually harder to learn, code in and debug.\nFor theses reasons you'll often find many bioinformatics packages written in interpreted languages such as  python  or  ruby .  While historically it has been a pain to install software written in interpreted languages, most modern languages now come with their own package managers! For example:   Python has  pip  Ruby has  gem  Javascript has  npm  ...   Most of theses package managers have similar syntaxes.\nWe will focus on python here since it's one of the most popular languages in bioinformatics.   Note  You will notice the absence of R here.\nR is mostly used interactively and installing packages in R will be part of the R part of the course.   Your ubuntu comes with an old version of python. We start with installing a newer one  cd ~/install\nwget https://www.python.org/ftp/python/3.6.4/Python-3.6.4.tar.xz\ntar xvf Python-3.6.4.tar.xz\ncd Python-3.6.4\n./configure --prefix=/home/$(whoami)/.local/\nmake -j2\nmake install   Question  What does the  make  option  -j2  do?   which python3\nwhich pip3  We now have the newest python installed.  Let us install our first python package  pip3 install multiqc  it should take a while and install  multiqc  as well as all the necessary dependencies.  to see if multiqc was properly installed:  multiqc -h",
            "title": "Installing python packages"
        },
        {
            "location": "/software/#exercises",
            "text": "During the following weeks we'll use a lot of different bioinformatics software to perform a variety of tasks.   Tip  Most software come with a file named  INSTALL  or  README .\nSuch file usually contains instructions on how to install!    Note  unless indicated otherwise, try with  apt  first    Note  do not hesitate to ask your teacher for help!   Let's install a few:   fastqc  scythe  sickle  bowtie2  megahit  quast  prokka",
            "title": "Exercises"
        },
        {
            "location": "/git/",
            "text": "Introduction to Git\n\n\nMost of the introduction to Git material can be found at \nhttps://software-carpentry.org\n\n\nMany thanks to them for existing!\n\n\nUseful resources\n\n\n\n\nLink to the course material from software carpentry\n\n\nreference of concepts and commands seen during the lesson\n\n\nThe official git website\n\n\nComparison of popular git hosting services - Medium article\n\n\nhttps://choosealicense.com",
            "title": "Git"
        },
        {
            "location": "/git/#introduction-to-git",
            "text": "Most of the introduction to Git material can be found at  https://software-carpentry.org  Many thanks to them for existing!",
            "title": "Introduction to Git"
        },
        {
            "location": "/git/#useful-resources",
            "text": "Link to the course material from software carpentry  reference of concepts and commands seen during the lesson  The official git website  Comparison of popular git hosting services - Medium article  https://choosealicense.com",
            "title": "Useful resources"
        },
        {
            "location": "/blast/",
            "text": "Command-line Blast\n\n\nInstalling blast\n\n\nWhile you should have installed blast during the \ninstalling software\n tutorial, you can copy/paste the code block below to reinstall it if needed\n\n\nsudo apt install ncbi-blast+\n\n\n\n\nGetting data\n\n\nWe will download some cows and human proteins from RefSeq\n\n\nwget ftp://ftp.ncbi.nih.gov/refseq/B_taurus/mRNA_Prot/cow.1.protein.faa.gz\nwget ftp://ftp.ncbi.nih.gov/refseq/H_sapiens/mRNA_Prot/human.1.protein.faa.gz\n\n\n\n\nBoth these files are compressed. They are not \ntar\n archives, like we encountered earlier, but \ngzip\n files.\nTo uncompress:\n\n\ngzip -d *.gz\n\n\n\n\nLet us take a look at the human file\n\n\nhead human.1.protein.faa\n\n\n\n\nBoth files contain protein sequences in the \nFASTA format\n\n\n\n\nQuestion\n\n\nHow many sequences do I have in each file?\n\n\n\n\nThe files are slightly too big for our first time blasting things at the command-line.\nLet's downsize the cow file\n\n\nhead -6 cow.1.protein.faa > cow.small.faa\n\n\n\n\nOur first blast\n\n\nNow we can blast these two cow sequences against the set of human sequences.\n\n\nFirst we need to build a blast database with our human sequences\n\n\nmakeblastdb -in human.1.protein.faa -dbtype prot\nls\n\n\n\n\nThe \nmakeblastdb\n produced a lot of extra files. Those files are indexes and necessary for blast to function.\n\n\nNow we can run blast\n\n\nblastp -query cow.small.faa -db human.1.protein.faa -out cow_vs_human_blast_results.txt\n\n\n\n\nWe can look at the results using \nless\n\n\nless cow_vs_human_blast_results.txt\n\n\n\n\nTo know about the various options that we can use with blastp:\n\n\nblastp -help\n\n\n\n\nand for easier reading\n\n\nblastp -help | less\n\n\n\n\n\n\nQuestion\n\n\nHow could I modify the previous blast command to filter the hits with an e-value of 1e-5\n\n\n\n\nBigger dataset\n\n\nNow that we succeeded using a small dataset of two proteins, let's try with a slightly bigger one.\n\n\nhead -199 cow.1.protein.faa > cow.medium.faa\n\n\n\n\n\n\nQuestion\n\n\nHow many protein sequences does \ncow.medium.faa\n contain?\n\n\n\n\nWe run blast again\n\n\nblastp -query cow.medium.faa -db human.1.protein.faa \\\n    -out cow_vs_human_blast_results.tab -evalue 1e-5 \\\n    -outfmt 6 -max_target_seqs 1\n\n\n\n\n\n\nQuestion\n\n\nWhat do \n-outfmt\n and \n-max_target_seqs\n do?",
            "title": "Command-line Blast"
        },
        {
            "location": "/blast/#command-line-blast",
            "text": "",
            "title": "Command-line Blast"
        },
        {
            "location": "/blast/#installing-blast",
            "text": "While you should have installed blast during the  installing software  tutorial, you can copy/paste the code block below to reinstall it if needed  sudo apt install ncbi-blast+",
            "title": "Installing blast"
        },
        {
            "location": "/blast/#getting-data",
            "text": "We will download some cows and human proteins from RefSeq  wget ftp://ftp.ncbi.nih.gov/refseq/B_taurus/mRNA_Prot/cow.1.protein.faa.gz\nwget ftp://ftp.ncbi.nih.gov/refseq/H_sapiens/mRNA_Prot/human.1.protein.faa.gz  Both these files are compressed. They are not  tar  archives, like we encountered earlier, but  gzip  files.\nTo uncompress:  gzip -d *.gz  Let us take a look at the human file  head human.1.protein.faa  Both files contain protein sequences in the  FASTA format   Question  How many sequences do I have in each file?   The files are slightly too big for our first time blasting things at the command-line.\nLet's downsize the cow file  head -6 cow.1.protein.faa > cow.small.faa",
            "title": "Getting data"
        },
        {
            "location": "/blast/#our-first-blast",
            "text": "Now we can blast these two cow sequences against the set of human sequences.  First we need to build a blast database with our human sequences  makeblastdb -in human.1.protein.faa -dbtype prot\nls  The  makeblastdb  produced a lot of extra files. Those files are indexes and necessary for blast to function.  Now we can run blast  blastp -query cow.small.faa -db human.1.protein.faa -out cow_vs_human_blast_results.txt  We can look at the results using  less  less cow_vs_human_blast_results.txt  To know about the various options that we can use with blastp:  blastp -help  and for easier reading  blastp -help | less   Question  How could I modify the previous blast command to filter the hits with an e-value of 1e-5",
            "title": "Our first blast"
        },
        {
            "location": "/blast/#bigger-dataset",
            "text": "Now that we succeeded using a small dataset of two proteins, let's try with a slightly bigger one.  head -199 cow.1.protein.faa > cow.medium.faa   Question  How many protein sequences does  cow.medium.faa  contain?   We run blast again  blastp -query cow.medium.faa -db human.1.protein.faa \\\n    -out cow_vs_human_blast_results.tab -evalue 1e-5 \\\n    -outfmt 6 -max_target_seqs 1   Question  What do  -outfmt  and  -max_target_seqs  do?",
            "title": "Bigger dataset"
        },
        {
            "location": "/project_organisation/",
            "text": "Project organization and management\n\n\nMost of the the project organization material can be found at \nhttps://software-carpentry.org\n and \nhttp://www.datacarpentry.org\n\n\nMany thanks to them for existing!\n\n\nStructure or architecture of a data science project\n\n\nSome good practice when you will organise your project directory on the server, on the cloud or any other machine where you will compute:\n\n\n\n\nCreate 3 or 4 different directories within you project directory (use \nmkdir\n):\n\n\ndata/\n for keeping the raw data\n\n\nresults/\n for all the outputs from the multiple analyses that you will perform\n\n\ndocs/\n for all the notes written about the analyses carried out (ex: \nhistory > 20180125.logs\n for the commands executed today)\n\n\nscripts/\n for all the scripts that you will use to produce the results\n\n\n\n\n\n\n\nNote\n\n\nYou should always have the raw data in (at least) one place and not modify them\n\n\n\n\nMore about data structure and metadata\n\n\n\n\n\n\ndirect link to the tutorial used fo the lesson: \nShell genomics: project organisation\n\n\n\n\n\n\ngood practice for the structure of data and metadata of a genomics project: \nOrganisation of genomics project\n\n\n\n\n\n\nSome extra material: \nSpreadsheet ecology lesson\n\n\n\n\n\n\nExercise\n\n\nThis exercise combines the knowledge you have acquired during the \nunix\n, \ngit\n and \nproject organisation\n lessons.\n\n\nYou have designed an experiment where you are studying the species and weight of animals caught in plots in a study area.\nData was collected by a third party a deposited in \nfigshare\n, a public database.\n\n\nOur goals are to download and exploring the data, while keeping an organised project directory that we will version control using git!\n\n\nSet up\n\n\nFirst we go to our Desktop and create a project directory\n\n\ncd ~/Desktop\nmkdir 2018_animals\ncd 2018_animals\n\n\n\n\nand initialize 2018_animals as a git repository\n\n\ngit init\n\n\n\n\nAs we saw during the project organization tutorial, it is good practice to separate data, results and scripts.\nLet us create those three directories\n\n\nmkdir data results scripts\n\n\n\n\nDownloading the data\n\n\nFirst we go to our \ndata\n directory\n\n\ncd data\n\n\n\n\nthen we download our data file and give it a more appropriate name\n\n\nwget https://ndownloader.figshare.com/files/2292169\nmv 2292169 survey_data.csv\n\n\n\n\nSince we'll never modify our raw data file (or at least we \ndo not want to!\n) it is safer to remove the writing permissions\n\n\nchmod -w survey_data.csv\n\n\n\n\nAdditionally since we are now unable to modify it, we do not want to track it in our git repository.\nWe add a .gitignore and tell git to not track the \ndata/\n directory\n\n\nnano .gitignore\n\n\n\n\n\n\nNote\n\n\nwhat if my data is really big?\nUsually when you download data that is several gigabytes large, they will usually be compressed.\nYou learnt about compression during the \ninstalling software\n lesson.\n\n\n\n\nLet us look at the first few lines of our file:\n\n\nhead data/data_joined.csv\n\n\n\n\nOur data file is a \n.csv\n file, that is a file where fields are separated by commas \n,\n.\nEach row represent an animal that was caught in a plot, and each column contains information about that animal.\n\n\n\n\nQuestion\n\n\nHow many animals do we have?\n\n\n\n\nwc -l data/data_joined.csv\n# 34787 data/data_joined.csv\n\n\n\n\nIt seems that our dataset contains 34787 lines.\nSince each line is an animals, we caught a grand total of 34787 animals over the course of our study.\n\n\nOur first analysis script\n\n\nwe saw when we did the \nhead\n command that all 10 first plots captured rodents.\n\n\n\n\nQuestion\n\n\nIs rodent the only taxon that we have captured?\n\n\n\n\nIn our csv file, we can see that \"taxa\" is the 12th column.\nWe can print only that column using the \ncut\n command\n\n\ncut -d ',' -f 12 data/data_joined.csv | head\n\n\n\n\nWe still pipe in in head because we do not want to print 34787 line to our screen.\nAdditionally \nhead\n makes us notice that we still have the column header printed out\n\n\ncut -d ',' -f 12 data/data_joined.csv | tail -n +2 | uniq -c\n\n\n\n\nBut while \nuniq\n is supposed to count all occurrence of a word, it only count similar \nadjacent\n occurrences.\nBefore counting, we need to sort our input:\n\n\ncut -d ',' -f 12 data/data_joined.csv | tail -n +2 | sort | uniq -c\n\n\n\n\nWe see that although we caught a vast majority of rodents, we also caught reptiles, birds and rabbits!\n\n\nNow that we have a working one-liner, let us put it into a script\n\n\nnano scripts/taxa_count.sh\n\n\n\n\nand write\n\n\n# script that prints the count of species for csv files\ncut -d ',' -f 12 \"$1\" | tail -n +2 | sort | uniq -c\n\n\n\n\nKeeping track of things\n\n\nNow keep track of your script in git\n\n\ngit add scripts/taxa_count.sh\ngit commit -m 'added taxa_count'\n\n\n\n\nas well as your gitignore\n\n\ngit add .gitignore\ngit commit -m 'added gitignore'\n\n\n\n\nSaving the result\n\n\nbash scripts/taxa_count.sh data/data_joined.csv > results/taxa_count.txt\n\n\n\n\ncat results/taxa_count.txt\n\n\n\n\ngit add results/taxa_count.txt\ngit commit -m 'added results of taxa_count.sh'\n\n\n\n\nImproving our script\n\n\nWe would also like to know the distribution of the numbers of animals caught in plots each year.\nThe year is the 4th column in our dataset and our script, in its current state, always selects the 12th columns of a file.\n\n\nWe can change our script to make it flexible so that the user can chose which columns they wishes to work on.\n\n\nnano scripts/taxa_count.sh\n\n\n\n\n# script that prints the count of occurrence in one column for csv files\ncut -d ',' -f \"$2\" \"$1\" | tail -n +2 | sort | uniq -c\n\n\n\n\nNow it doesn't make much sense to have it named \ntaxa_count.sh\n\n\nmv scripts/taxa_count.sh scripts/column_count.sh\n\n\n\n\nand let us not forget to keep track of our changes in git!\n\n\ngit add -A\ngit commit -m 'made script more flexible about which column to cut on'\n\n\n\n\n\n\nQuestion\n\n\nwhich year did we catch the most animals?\ntry to answer programmatically.\n\n\n\n\n\n\nQuestion\n\n\nsave the sorted output to a file in the \nresults\n directory and keep track of it in git.\n\n\n\n\nInvestigating further\n\n\nWe'd like to refine our animal count and knowing how many animals of each taxon were captured every year\n\n\nwe can use \ncut\n on several columns like this:\n\n\ncut -d ',' -f 4,12 \"data/data_joined.csv\" | tail -n +2 | sort | uniq -c\n\n\n\n\nNow that we are ahhpy with our one-liner, let us save it in a script:\n\n\nnano scripts/taxa_per_year.sh\n\n\n\n\nthen save the output to \nresults/taxa_per_year.txt\n\n\nbash scripts/taxa_per_year.sh > results/taxa_per_year.txt\n\n\n\n\n\n\nQuestion\n\n\nWhich year was the first reptile captured?\n\n\n\n\nThe next step would be to refine our analysis by year. We will save one individual output for each year count\n\n\nThe seq command\n\n\nTo perform what we want to do, we need to be able to loop over the years.\nThe \nseq\n command can help us with that.\n\n\nFirst we try\n\n\nseq 1 10\n\n\n\n\nthen\n\n\nseq 1997 2002\n\n\n\n\nand what about the span of years we are interested in?\n\n\nseq 1977 2002\n\n\n\n\nGreat! So now does it work with a for loop?\n\n\nfor year in $(seq 1977 2002)\n    do\n        echo $year\n    done\n\n\n\n\nIt does!\n\n\nBefore doing our analysis on each year, we still have to figure out how to do it on one year.\n\n\ngrep 1998 results/taxa_per_year.txt\n\n\n\n\n\"Grepping\" the year seems to work.\nNow we need to save it into a file containing the year\n\n\nFirst let's create a directory where to store our results\n\n\nmkdir results/years\n\n\n\n\nand we try to redirect our yearly count into a file\n\n\ngrep 1998 results/taxa_per_year.txt > results/years/1998-count.txt\n\n\n\n\nbash\ncat results/years/1998-count.txt\n\n\n\n\nIt seems to have worked.\nNow with the loop\n\n\nfor year in $(seq 1977 2002)\n    do\n        grep $year results/taxa_per_year.txt > results/years/$year-count.txt\n    done\n\n\n\n\nls results/years\n\n\n\n\n\n\nQuestion\n\n\nPut your loop in a script, and commit everything with \ngit",
            "title": "Project Organisation"
        },
        {
            "location": "/project_organisation/#project-organization-and-management",
            "text": "Most of the the project organization material can be found at  https://software-carpentry.org  and  http://www.datacarpentry.org  Many thanks to them for existing!",
            "title": "Project organization and management"
        },
        {
            "location": "/project_organisation/#structure-or-architecture-of-a-data-science-project",
            "text": "Some good practice when you will organise your project directory on the server, on the cloud or any other machine where you will compute:   Create 3 or 4 different directories within you project directory (use  mkdir ):  data/  for keeping the raw data  results/  for all the outputs from the multiple analyses that you will perform  docs/  for all the notes written about the analyses carried out (ex:  history > 20180125.logs  for the commands executed today)  scripts/  for all the scripts that you will use to produce the results    Note  You should always have the raw data in (at least) one place and not modify them",
            "title": "Structure or architecture of a data science project"
        },
        {
            "location": "/project_organisation/#more-about-data-structure-and-metadata",
            "text": "direct link to the tutorial used fo the lesson:  Shell genomics: project organisation    good practice for the structure of data and metadata of a genomics project:  Organisation of genomics project    Some extra material:  Spreadsheet ecology lesson",
            "title": "More about data structure and metadata"
        },
        {
            "location": "/project_organisation/#exercise",
            "text": "This exercise combines the knowledge you have acquired during the  unix ,  git  and  project organisation  lessons.  You have designed an experiment where you are studying the species and weight of animals caught in plots in a study area.\nData was collected by a third party a deposited in  figshare , a public database.  Our goals are to download and exploring the data, while keeping an organised project directory that we will version control using git!",
            "title": "Exercise"
        },
        {
            "location": "/project_organisation/#set-up",
            "text": "First we go to our Desktop and create a project directory  cd ~/Desktop\nmkdir 2018_animals\ncd 2018_animals  and initialize 2018_animals as a git repository  git init  As we saw during the project organization tutorial, it is good practice to separate data, results and scripts.\nLet us create those three directories  mkdir data results scripts",
            "title": "Set up"
        },
        {
            "location": "/project_organisation/#downloading-the-data",
            "text": "First we go to our  data  directory  cd data  then we download our data file and give it a more appropriate name  wget https://ndownloader.figshare.com/files/2292169\nmv 2292169 survey_data.csv  Since we'll never modify our raw data file (or at least we  do not want to! ) it is safer to remove the writing permissions  chmod -w survey_data.csv  Additionally since we are now unable to modify it, we do not want to track it in our git repository.\nWe add a .gitignore and tell git to not track the  data/  directory  nano .gitignore   Note  what if my data is really big?\nUsually when you download data that is several gigabytes large, they will usually be compressed.\nYou learnt about compression during the  installing software  lesson.   Let us look at the first few lines of our file:  head data/data_joined.csv  Our data file is a  .csv  file, that is a file where fields are separated by commas  , .\nEach row represent an animal that was caught in a plot, and each column contains information about that animal.   Question  How many animals do we have?   wc -l data/data_joined.csv\n# 34787 data/data_joined.csv  It seems that our dataset contains 34787 lines.\nSince each line is an animals, we caught a grand total of 34787 animals over the course of our study.",
            "title": "Downloading the data"
        },
        {
            "location": "/project_organisation/#our-first-analysis-script",
            "text": "we saw when we did the  head  command that all 10 first plots captured rodents.   Question  Is rodent the only taxon that we have captured?   In our csv file, we can see that \"taxa\" is the 12th column.\nWe can print only that column using the  cut  command  cut -d ',' -f 12 data/data_joined.csv | head  We still pipe in in head because we do not want to print 34787 line to our screen.\nAdditionally  head  makes us notice that we still have the column header printed out  cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | uniq -c  But while  uniq  is supposed to count all occurrence of a word, it only count similar  adjacent  occurrences.\nBefore counting, we need to sort our input:  cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | sort | uniq -c  We see that although we caught a vast majority of rodents, we also caught reptiles, birds and rabbits!  Now that we have a working one-liner, let us put it into a script  nano scripts/taxa_count.sh  and write  # script that prints the count of species for csv files\ncut -d ',' -f 12 \"$1\" | tail -n +2 | sort | uniq -c",
            "title": "Our first analysis script"
        },
        {
            "location": "/project_organisation/#keeping-track-of-things",
            "text": "Now keep track of your script in git  git add scripts/taxa_count.sh\ngit commit -m 'added taxa_count'  as well as your gitignore  git add .gitignore\ngit commit -m 'added gitignore'",
            "title": "Keeping track of things"
        },
        {
            "location": "/project_organisation/#saving-the-result",
            "text": "bash scripts/taxa_count.sh data/data_joined.csv > results/taxa_count.txt  cat results/taxa_count.txt  git add results/taxa_count.txt\ngit commit -m 'added results of taxa_count.sh'",
            "title": "Saving the result"
        },
        {
            "location": "/project_organisation/#improving-our-script",
            "text": "We would also like to know the distribution of the numbers of animals caught in plots each year.\nThe year is the 4th column in our dataset and our script, in its current state, always selects the 12th columns of a file.  We can change our script to make it flexible so that the user can chose which columns they wishes to work on.  nano scripts/taxa_count.sh  # script that prints the count of occurrence in one column for csv files\ncut -d ',' -f \"$2\" \"$1\" | tail -n +2 | sort | uniq -c  Now it doesn't make much sense to have it named  taxa_count.sh  mv scripts/taxa_count.sh scripts/column_count.sh  and let us not forget to keep track of our changes in git!  git add -A\ngit commit -m 'made script more flexible about which column to cut on'   Question  which year did we catch the most animals?\ntry to answer programmatically.    Question  save the sorted output to a file in the  results  directory and keep track of it in git.",
            "title": "Improving our script"
        },
        {
            "location": "/project_organisation/#investigating-further",
            "text": "We'd like to refine our animal count and knowing how many animals of each taxon were captured every year  we can use  cut  on several columns like this:  cut -d ',' -f 4,12 \"data/data_joined.csv\" | tail -n +2 | sort | uniq -c  Now that we are ahhpy with our one-liner, let us save it in a script:  nano scripts/taxa_per_year.sh  then save the output to  results/taxa_per_year.txt  bash scripts/taxa_per_year.sh > results/taxa_per_year.txt   Question  Which year was the first reptile captured?   The next step would be to refine our analysis by year. We will save one individual output for each year count",
            "title": "Investigating further"
        },
        {
            "location": "/project_organisation/#the-seq-command",
            "text": "To perform what we want to do, we need to be able to loop over the years.\nThe  seq  command can help us with that.  First we try  seq 1 10  then  seq 1997 2002  and what about the span of years we are interested in?  seq 1977 2002  Great! So now does it work with a for loop?  for year in $(seq 1977 2002)\n    do\n        echo $year\n    done  It does!  Before doing our analysis on each year, we still have to figure out how to do it on one year.  grep 1998 results/taxa_per_year.txt  \"Grepping\" the year seems to work.\nNow we need to save it into a file containing the year  First let's create a directory where to store our results  mkdir results/years  and we try to redirect our yearly count into a file  grep 1998 results/taxa_per_year.txt > results/years/1998-count.txt  bash\ncat results/years/1998-count.txt  It seems to have worked.\nNow with the loop  for year in $(seq 1977 2002)\n    do\n        grep $year results/taxa_per_year.txt > results/years/$year-count.txt\n    done  ls results/years   Question  Put your loop in a script, and commit everything with  git",
            "title": "The seq command"
        },
        {
            "location": "/seq_tech/",
            "text": "Sequencing Technologies\n\n\n\n\nThis is an embedded \nMicrosoft Office\n presentation, powered by \nOffice Online\n.\n\n\n\n\n\nExercise\n\n\nBegin by reading \nGoodwin \net. al.\n 2016\n\n\nYou will be divided into four groups; each group will focus on one sequencing technology: Illumina, Ion Torrent, PacBio and Oxford Nanopore.\n\n\nEach group will present a short review of the technology, pros and cons as well as how they are used currently.\n\n\n\n\nWhat are the unique characteristics of the technology?\n\n\nWhat are the major use-cases for the technology?\n\n\nDescribe the significant landmarks of the technology.\n\n\n\n\nWrite a short presentation about the technology and present for the group. 10 min in total, 5-7 min presentation and 3 min questions.",
            "title": "Sequencing Technologies"
        },
        {
            "location": "/seq_tech/#sequencing-technologies",
            "text": "This is an embedded  Microsoft Office  presentation, powered by  Office Online .",
            "title": "Sequencing Technologies"
        },
        {
            "location": "/seq_tech/#exercise",
            "text": "Begin by reading  Goodwin  et. al.  2016  You will be divided into four groups; each group will focus on one sequencing technology: Illumina, Ion Torrent, PacBio and Oxford Nanopore.  Each group will present a short review of the technology, pros and cons as well as how they are used currently.   What are the unique characteristics of the technology?  What are the major use-cases for the technology?  Describe the significant landmarks of the technology.   Write a short presentation about the technology and present for the group. 10 min in total, 5-7 min presentation and 3 min questions.",
            "title": "Exercise"
        },
        {
            "location": "/tutorials/docs/qc/",
            "text": "Quality Control and Trimming\n\n\nLecture\n\n\n\n\n\nPractical\n\n\nIn this practical you will learn to import, view and check the quality of raw high thoughput sequencing sequencing data.\n\n\nThe first dataset you will be working with is from an Illumina MiSeq dataset.\nThe sequenced organism is an enterohaemorrhagic E. coli (EHEC) of the serotype O157, a potentially fatal gastrointestinal pathogen.\nThe sequenced bacterium was part of an outbreak investigation in the St. Louis area, USA in 2011.\nThe sequencing was done as paired-end 2x150bp.\n\n\nDownloading the data\n\n\nThe raw data were deposited at the European Nucleotide Archive, under the accession number SRR957824.\nYou could go to the ENA \nwebsite\n and search for the run with the accession SRR957824.\n\n\nHowever these files contain about 3 million reads and are therefore quite big.\nWe are only gonna use a subset of the original dataset for this tutorial.\n\n\nFirst create a \ndata/\n directory in your home folder\n\n\nmkdir ~/data\n\n\n\n\nnow let's download the subset\n\n\ncd ~/data\ncurl -O -J -L https://osf.io/shqpv/download\ncurl -O -J -L https://osf.io/9m3ch/download\n\n\n\n\nLet\u2019s make sure we downloaded all of our data using md5sum.\n\n\nmd5sum SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz\n\n\n\n\nyou should see this\n\n\n1e8cf249e3217a5a0bcc0d8a654585fb  SRR957824_500K_R1.fastq.gz\n70c726a31f05f856fe942d727613adb7  SRR957824_500K_R2.fastq.gz\n\n\n\n\nand now look at the file names and their size\n\n\nls -l\n\n\n\n\ntotal 97M\n-rw-r--r-- 1 hadrien 48M Nov 19 18:44 SRR957824_500K_R1.fastq.gz\n-rw-r--r-- 1 hadrien 50M Nov 19 18:53 SRR957824_500K_R2.fastq.gz\n\n\n\n\nThere are 500 000 paired-end reads taken randomly from the original data\n\n\nOne last thing before we get to the quality control: those files are writeable.\nBy default, UNIX makes things writeable by the file owner.\nThis poses an issue with creating typos or errors in raw data.\nWe fix that before going further\n\n\nchmod u-w *\n\n\n\n\nWorking Directory\n\n\nFirst we make a work directory: a directory where we can play around with a copy of the data without messing with the original\n\n\nmkdir ~/work\ncd ~/work\n\n\n\n\nNow we make a link of the data in our working directory\n\n\nln -s ~/data/* .\n\n\n\n\nThe files that we've downloaded are FASTQ files. Take a look at one of them with\n\n\nzless SRR957824_500K_R1.fastq.gz\n\n\n\n\n\n\nTip\n\n\nUse the spacebar to scroll down, and type \u2018q\u2019 to exit \u2018less\u2019\n\n\n\n\nYou can read more on the FASTQ format in the \nFile Formats\n lesson.\n\n\n\n\nQuestion\n\n\nWhere does the filename come from?\n\n\n\n\n\n\nQuestion\n\n\nWhy are there 1 and 2 in the file names?    \n\n\n\n\nFastQC\n\n\nTo check the quality of the sequence data we will use a tool called FastQC.\n\n\nFastQC has a graphical interface and can be downloaded and run on a Windows or Linux computer without installation.\nIt is available \nhere\n.\n\n\nHowever, FastQC is also available as a command line utility on the training server you are using.\nTo run FastQC on our two files\n\n\nfastqc SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz\n\n\n\n\nand look what FastQC has produced\n\n\nls *fastqc*\n\n\n\n\nFor each file, FastQC has produced both a .zip archive containing all the plots, and a html report.\n\n\nDownload and open the html files with your favourite web browser.\n\n\nAlternatively you can look a these copies of them:\n\n\n\n\nSRR957824_500K_R1_fastqc.html\n\n\nSRR957824_500K_R2_fastqc.html\n\n\n\n\n\n\nQuestion\n\n\nWhat should you pay attention to in the FastQC report?\n\n\n\n\n\n\nQuestion\n\n\nWhich file is of better quality?\n\n\n\n\nPay special attention to the per base sequence quality and sequence length distribution.\nExplanations for the various quality modules can be found \nhere\n.\nAlso, have a look at examples of a \ngood\n and a \nbad\n illumina read set for comparison.\n\n\nYou will note that the reads in your uploaded dataset have fairly poor quality (<20) towards the end. There are also outlier reads that have very poor quality for most of the second half of the reads.\n\n\nScythe\n\n\nNow we'll do some trimming!\n\n\nScythe uses a Naive Bayesian approach to classify contaminant substrings in sequence reads.\nIt considers quality information, which can make it robust in picking out 3'-end adapters, which often include poor quality bases.\n\n\nThe first thing we need is the adapters to trim off\n\n\ncurl -O -J -L https://osf.io/v24pt/download\n\n\n\n\nNow we run scythe on both our read files\n\n\nscythe -a adapters.fasta -o SRR957824_adapt_R1.fastq SRR957824_500K_R1.fastq.gz\nscythe -a adapters.fasta -o SRR957824_adapt_R2.fastq SRR957824_500K_R2.fastq.gz\n\n\n\n\n\n\nQuestion\n\n\nWhat adapters do you use?\n\n\n\n\nSickle\n\n\nMost modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well.\nIncorrectly called bases in both regions negatively impact assembles, mapping, and downstream bioinformatics analyses.\n\n\nWe will trim each read individually down to the good quality part to keep the bad part from interfering with downstream applications.\n\n\nTo do so, we will use sickle. Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It will also discard reads based upon a length threshold.\n\n\nTo run sickle\n\n\nsickle pe -f SRR957824_adapt_R1.fastq -r SRR957824_adapt_R2.fastq \\\n    -t sanger -o SRR957824_trimmed_R1.fastq -p SRR957824_trimmed_R2.fastq \\\n    -s /dev/null -q 25\n\n\n\n\nwhich should output something like\n\n\nPE forward file: SRR957824_trimmed_R1.fastq\nPE reverse file: SRR957824_trimmed_R2.fastq\n\nTotal input FastQ records: 1000000 (500000 pairs)\n\nFastQ paired records kept: 834570 (417285 pairs)\nFastQ single records kept: 13263 (from PE1: 11094, from PE2: 2169)\nFastQ paired records discarded: 138904 (69452 pairs)\nFastQ single records discarded: 13263 (from PE1: 2169, from PE2: 11094)\n\n\n\n\nFastQC again\n\n\nRun fastqc again on the filtered reads\n\n\nfastqc SRR957824_trimmed_R1.fastq SRR957824_trimmed_R2.fastq\n\n\n\n\nand look at the reports\n\n\n\n\nSRR957824_trimmed_R1_fastqc.html\n\n\nSRR957824_trimmed_R2_fastqc.html\n\n\n\n\nMultiQC\n\n\nMultiQC\n is a tool that aggreagtes results from several popular QC bioinformatics software into one html report.\n\n\nLet's run MultiQC in our current directory\n\n\nmultiqc .\n\n\n\n\nYou can download the report or view it by clickinh on the link below\n\n\n\n\nmultiqc_report.html\n\n\n\n\n\n\nQuestion\n\n\nWhat did the trimming do to the per-base sequence quality, the per sequence quality scores and the sequence length distribution?",
            "title": "Quality Control"
        },
        {
            "location": "/tutorials/docs/qc/#quality-control-and-trimming",
            "text": "",
            "title": "Quality Control and Trimming"
        },
        {
            "location": "/tutorials/docs/qc/#lecture",
            "text": "",
            "title": "Lecture"
        },
        {
            "location": "/tutorials/docs/qc/#practical",
            "text": "In this practical you will learn to import, view and check the quality of raw high thoughput sequencing sequencing data.  The first dataset you will be working with is from an Illumina MiSeq dataset.\nThe sequenced organism is an enterohaemorrhagic E. coli (EHEC) of the serotype O157, a potentially fatal gastrointestinal pathogen.\nThe sequenced bacterium was part of an outbreak investigation in the St. Louis area, USA in 2011.\nThe sequencing was done as paired-end 2x150bp.",
            "title": "Practical"
        },
        {
            "location": "/tutorials/docs/qc/#downloading-the-data",
            "text": "The raw data were deposited at the European Nucleotide Archive, under the accession number SRR957824.\nYou could go to the ENA  website  and search for the run with the accession SRR957824.  However these files contain about 3 million reads and are therefore quite big.\nWe are only gonna use a subset of the original dataset for this tutorial.  First create a  data/  directory in your home folder  mkdir ~/data  now let's download the subset  cd ~/data\ncurl -O -J -L https://osf.io/shqpv/download\ncurl -O -J -L https://osf.io/9m3ch/download  Let\u2019s make sure we downloaded all of our data using md5sum.  md5sum SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz  you should see this  1e8cf249e3217a5a0bcc0d8a654585fb  SRR957824_500K_R1.fastq.gz\n70c726a31f05f856fe942d727613adb7  SRR957824_500K_R2.fastq.gz  and now look at the file names and their size  ls -l  total 97M\n-rw-r--r-- 1 hadrien 48M Nov 19 18:44 SRR957824_500K_R1.fastq.gz\n-rw-r--r-- 1 hadrien 50M Nov 19 18:53 SRR957824_500K_R2.fastq.gz  There are 500 000 paired-end reads taken randomly from the original data  One last thing before we get to the quality control: those files are writeable.\nBy default, UNIX makes things writeable by the file owner.\nThis poses an issue with creating typos or errors in raw data.\nWe fix that before going further  chmod u-w *",
            "title": "Downloading the data"
        },
        {
            "location": "/tutorials/docs/qc/#working-directory",
            "text": "First we make a work directory: a directory where we can play around with a copy of the data without messing with the original  mkdir ~/work\ncd ~/work  Now we make a link of the data in our working directory  ln -s ~/data/* .  The files that we've downloaded are FASTQ files. Take a look at one of them with  zless SRR957824_500K_R1.fastq.gz   Tip  Use the spacebar to scroll down, and type \u2018q\u2019 to exit \u2018less\u2019   You can read more on the FASTQ format in the  File Formats  lesson.   Question  Where does the filename come from?    Question  Why are there 1 and 2 in the file names?",
            "title": "Working Directory"
        },
        {
            "location": "/tutorials/docs/qc/#fastqc",
            "text": "To check the quality of the sequence data we will use a tool called FastQC.  FastQC has a graphical interface and can be downloaded and run on a Windows or Linux computer without installation.\nIt is available  here .  However, FastQC is also available as a command line utility on the training server you are using.\nTo run FastQC on our two files  fastqc SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz  and look what FastQC has produced  ls *fastqc*  For each file, FastQC has produced both a .zip archive containing all the plots, and a html report.  Download and open the html files with your favourite web browser.  Alternatively you can look a these copies of them:   SRR957824_500K_R1_fastqc.html  SRR957824_500K_R2_fastqc.html    Question  What should you pay attention to in the FastQC report?    Question  Which file is of better quality?   Pay special attention to the per base sequence quality and sequence length distribution.\nExplanations for the various quality modules can be found  here .\nAlso, have a look at examples of a  good  and a  bad  illumina read set for comparison.  You will note that the reads in your uploaded dataset have fairly poor quality (<20) towards the end. There are also outlier reads that have very poor quality for most of the second half of the reads.",
            "title": "FastQC"
        },
        {
            "location": "/tutorials/docs/qc/#scythe",
            "text": "Now we'll do some trimming!  Scythe uses a Naive Bayesian approach to classify contaminant substrings in sequence reads.\nIt considers quality information, which can make it robust in picking out 3'-end adapters, which often include poor quality bases.  The first thing we need is the adapters to trim off  curl -O -J -L https://osf.io/v24pt/download  Now we run scythe on both our read files  scythe -a adapters.fasta -o SRR957824_adapt_R1.fastq SRR957824_500K_R1.fastq.gz\nscythe -a adapters.fasta -o SRR957824_adapt_R2.fastq SRR957824_500K_R2.fastq.gz   Question  What adapters do you use?",
            "title": "Scythe"
        },
        {
            "location": "/tutorials/docs/qc/#sickle",
            "text": "Most modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well.\nIncorrectly called bases in both regions negatively impact assembles, mapping, and downstream bioinformatics analyses.  We will trim each read individually down to the good quality part to keep the bad part from interfering with downstream applications.  To do so, we will use sickle. Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It will also discard reads based upon a length threshold.  To run sickle  sickle pe -f SRR957824_adapt_R1.fastq -r SRR957824_adapt_R2.fastq \\\n    -t sanger -o SRR957824_trimmed_R1.fastq -p SRR957824_trimmed_R2.fastq \\\n    -s /dev/null -q 25  which should output something like  PE forward file: SRR957824_trimmed_R1.fastq\nPE reverse file: SRR957824_trimmed_R2.fastq\n\nTotal input FastQ records: 1000000 (500000 pairs)\n\nFastQ paired records kept: 834570 (417285 pairs)\nFastQ single records kept: 13263 (from PE1: 11094, from PE2: 2169)\nFastQ paired records discarded: 138904 (69452 pairs)\nFastQ single records discarded: 13263 (from PE1: 2169, from PE2: 11094)",
            "title": "Sickle"
        },
        {
            "location": "/tutorials/docs/qc/#fastqc-again",
            "text": "Run fastqc again on the filtered reads  fastqc SRR957824_trimmed_R1.fastq SRR957824_trimmed_R2.fastq  and look at the reports   SRR957824_trimmed_R1_fastqc.html  SRR957824_trimmed_R2_fastqc.html",
            "title": "FastQC again"
        },
        {
            "location": "/tutorials/docs/qc/#multiqc",
            "text": "MultiQC  is a tool that aggreagtes results from several popular QC bioinformatics software into one html report.  Let's run MultiQC in our current directory  multiqc .  You can download the report or view it by clickinh on the link below   multiqc_report.html    Question  What did the trimming do to the per-base sequence quality, the per sequence quality scores and the sequence length distribution?",
            "title": "MultiQC"
        },
        {
            "location": "/assembly_challenge/",
            "text": "Assembly Challenge\n\n\nDuring this session, you will be asked to produce the best assembly possible of \nEscherichia coli\n str. K-12 MG1655.\n\n\nThere are various assemblers already installed on your virtual machines but feel free to try and install others.\nBelow you will find the commands needed to download the data, as well as links to the websites of some well-known assemblers and quality assessment tools.\n\n\nGood luck!\n\n\nDownload the Data\n\n\ncurl -O -J -L https://osf.io/qszpw/download\ncurl -O -J -L https://osf.io/jaf6d/download\n\n\n\n\nAssemblers available\n\n\n\n\nmegahit\n\n\nSPAdes\n\n\nsga\n\n\nAbyss\n\n\nUnicycler\n\n\n\n\nQuality assessment\n\n\n\n\nquast\n\n\nbusco\n\n\nbowtie2\n\n\nmultiqc",
            "title": "Assembly Challenge"
        },
        {
            "location": "/assembly_challenge/#assembly-challenge",
            "text": "During this session, you will be asked to produce the best assembly possible of  Escherichia coli  str. K-12 MG1655.  There are various assemblers already installed on your virtual machines but feel free to try and install others.\nBelow you will find the commands needed to download the data, as well as links to the websites of some well-known assemblers and quality assessment tools.  Good luck!",
            "title": "Assembly Challenge"
        },
        {
            "location": "/assembly_challenge/#download-the-data",
            "text": "curl -O -J -L https://osf.io/qszpw/download\ncurl -O -J -L https://osf.io/jaf6d/download",
            "title": "Download the Data"
        },
        {
            "location": "/assembly_challenge/#assemblers-available",
            "text": "megahit  SPAdes  sga  Abyss  Unicycler",
            "title": "Assemblers available"
        },
        {
            "location": "/assembly_challenge/#quality-assessment",
            "text": "quast  busco  bowtie2  multiqc",
            "title": "Quality assessment"
        },
        {
            "location": "/tutorials/docs/assembly/",
            "text": "De-novo Genome Assembly\n\n\nLecture\n\n\n\n\n\nPractical\n\n\nIn this practical we will perform the assembly of \nM. genitalium\n, a bacterium published in 1995 by Fraser et al in Science (\nabstract link\n).\n\n\nGetting the data\n\n\nM. genitalium\n was sequenced using the MiSeq platform (2 * 150bp).\nThe reads were deposited in the ENA Short Read Archive under the accession \nERR486840\n\n\nDownload the 2 fastq files associated with the run.\n\n\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_1.fastq.gz\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_2.fastq.gz\n\n\n\n\nThe files that were deposited in ENA were already trimmed, so we do not have to trim ourselves!\n\n\n\n\nQuestion\n\n\nHow many reads are in the files?\n\n\n\n\nDe-novo assembly\n\n\nWe will be using the MEGAHIT assembler to assemble our bacterium\n\n\nmegahit -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz -o m_genitalium\n\n\n\n\nThis will take a few minutes.\n\n\nThe result of the assembly is in the directory m_genitalium under the name \nfinal.contigs.fa\n\n\nLet's make a copy of it\n\n\ncp m_genitalium/final.contigs.fa m_genitalium.fasta\n\n\n\n\nand look at it\n\n\nhead m_genitalium.fasta\n\n\n\n\nQuality of the Assembly\n\n\nQUAST is a software evaluating the quality of genome assemblies by computing various metrics, including\n\n\nRun Quast on your assembly\n\n\nquast.py m_genitalium.fasta -o m_genitalium_report\n\n\n\n\nand take a look at the text report\n\n\ncat m_genitalium_report/report.txt\n\n\n\n\nYou should see something like\n\n\nAll statistics are based on contigs of size >= 500 bp, unless otherwise noted (e.g., \"# contigs (>= 0 bp)\" and \"Total length (>= 0 bp)\" include all contigs).\n\nAssembly                    m_genitalium\n# contigs (>= 0 bp)         17          \n# contigs (>= 1000 bp)      8           \n# contigs (>= 5000 bp)      7           \n# contigs (>= 10000 bp)     6           \n# contigs (>= 25000 bp)     5           \n# contigs (>= 50000 bp)     2           \nTotal length (>= 0 bp)      584267      \nTotal length (>= 1000 bp)   580160      \nTotal length (>= 5000 bp)   577000      \nTotal length (>= 10000 bp)  570240      \nTotal length (>= 25000 bp)  554043      \nTotal length (>= 50000 bp)  446481      \n# contigs                   11          \nLargest contig              368542      \nTotal length                582257      \nGC (%)                      31.71       \nN50                         368542      \nN75                         77939       \nL50                         1           \nL75                         2           \n# N's per 100 kbp           0.00    \n\n\n\n\nwhich is a summary stats about our assembly.\nAdditionally, the file \nm_genitalium_report/report.html\n\n\nYou can either download it and open it in your own web browser, or we make it available for your convenience:\n\n\n\n\nm_genitalium_report/report.html\n\n\n\n\n\n\nNote\n\n\nN50: length for which the collection of all contigs of that length or longer covers at least 50% of assembly length\n\n\n\n\n\n\nQuestion\n\n\nHow well does the assembly total consensus size and coverage correspond to your earlier estimation?\n\n\n\n\n\n\nQuestion\n\n\nHow many contigs in total did the assembly produce?\n\n\n\n\n\n\nQuestion\n\n\nWhat is the N50 of the assembly? What does this mean?\n\n\n\n\nFixing misassemblies\n\n\nPilon is a software tool which can be used to automatically improve draft assemblies.\nIt attempts to make improvements to the input genome, including:\n\n\n\n\nSingle base differences\n\n\nSmall Indels\n\n\nLarger Indels or block substitution events\n\n\nGap filling\n\n\nIdentification of local misassemblies, including optional opening of new gaps\n\n\n\n\nPilon then outputs a FASTA file containing an improved representation of the genome from the read data and an optional VCF file detailing variation seen between the read data and the input genome.\n\n\nBefore running Pilon itself, we have to align our reads against the assembly\n\n\nbowtie2-build m_genitalium.fasta m_genitalium\nbowtie2 -x m_genitalium -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz | \\\n    samtools view -bS -o m_genitalium.bam\nsamtools sort m_genitalium.bam -o m_genitalium.sorted.bam\nsamtools index m_genitalium.sorted.bam\n\n\n\n\nthen we run Pilon\n\n\npilon --genome m_genitalium.fasta --frags m_genitalium.sorted.bam --output m_genitalium_improved\n\n\n\n\nwhich will correct eventual mismatches in our assembly and write the new improved assembly to \nm_genitalium_improved.fasta\n\n\nAssembly Completeness\n\n\nAlthough quast output a range of metric to assess how contiguous our assembly is, having a long N50 does not guarantee a good assembly: it could be riddled by misassemblies!\n\n\nWe will run \nbusco\n to try to find marker genes in our assembly. Marker genes are conserved across a range of species and finding intact conserved genes in our assembly would be a good indication of its quality\n\n\nFirst we need to download and unpack the bacterial datasets used by \nbusco\n\n\nwget http://busco.ezlab.org/datasets/bacteria_odb9.tar.gz\ntar xzf bacteria_odb9.tar.gz\n\n\n\n\nthen we can run \nbusco\n with\n\n\nBUSCO.py -i m_genitalium.fasta -l bacteria_odb9 -o busco_genitalium -m genome\n\n\n\n\n\n\nQuestion\n\n\nHow many marker genes has \nbusco\n found?\n\n\n\n\nCourse literature\n\n\nCourse litteraturer for today is:",
            "title": "De-novo Genome Assembly"
        },
        {
            "location": "/tutorials/docs/assembly/#de-novo-genome-assembly",
            "text": "",
            "title": "De-novo Genome Assembly"
        },
        {
            "location": "/tutorials/docs/assembly/#lecture",
            "text": "",
            "title": "Lecture"
        },
        {
            "location": "/tutorials/docs/assembly/#practical",
            "text": "In this practical we will perform the assembly of  M. genitalium , a bacterium published in 1995 by Fraser et al in Science ( abstract link ).",
            "title": "Practical"
        },
        {
            "location": "/tutorials/docs/assembly/#getting-the-data",
            "text": "M. genitalium  was sequenced using the MiSeq platform (2 * 150bp).\nThe reads were deposited in the ENA Short Read Archive under the accession  ERR486840  Download the 2 fastq files associated with the run.  wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_1.fastq.gz\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_2.fastq.gz  The files that were deposited in ENA were already trimmed, so we do not have to trim ourselves!   Question  How many reads are in the files?",
            "title": "Getting the data"
        },
        {
            "location": "/tutorials/docs/assembly/#de-novo-assembly",
            "text": "We will be using the MEGAHIT assembler to assemble our bacterium  megahit -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz -o m_genitalium  This will take a few minutes.  The result of the assembly is in the directory m_genitalium under the name  final.contigs.fa  Let's make a copy of it  cp m_genitalium/final.contigs.fa m_genitalium.fasta  and look at it  head m_genitalium.fasta",
            "title": "De-novo assembly"
        },
        {
            "location": "/tutorials/docs/assembly/#quality-of-the-assembly",
            "text": "QUAST is a software evaluating the quality of genome assemblies by computing various metrics, including  Run Quast on your assembly  quast.py m_genitalium.fasta -o m_genitalium_report  and take a look at the text report  cat m_genitalium_report/report.txt  You should see something like  All statistics are based on contigs of size >= 500 bp, unless otherwise noted (e.g., \"# contigs (>= 0 bp)\" and \"Total length (>= 0 bp)\" include all contigs).\n\nAssembly                    m_genitalium\n# contigs (>= 0 bp)         17          \n# contigs (>= 1000 bp)      8           \n# contigs (>= 5000 bp)      7           \n# contigs (>= 10000 bp)     6           \n# contigs (>= 25000 bp)     5           \n# contigs (>= 50000 bp)     2           \nTotal length (>= 0 bp)      584267      \nTotal length (>= 1000 bp)   580160      \nTotal length (>= 5000 bp)   577000      \nTotal length (>= 10000 bp)  570240      \nTotal length (>= 25000 bp)  554043      \nTotal length (>= 50000 bp)  446481      \n# contigs                   11          \nLargest contig              368542      \nTotal length                582257      \nGC (%)                      31.71       \nN50                         368542      \nN75                         77939       \nL50                         1           \nL75                         2           \n# N's per 100 kbp           0.00      which is a summary stats about our assembly.\nAdditionally, the file  m_genitalium_report/report.html  You can either download it and open it in your own web browser, or we make it available for your convenience:   m_genitalium_report/report.html    Note  N50: length for which the collection of all contigs of that length or longer covers at least 50% of assembly length    Question  How well does the assembly total consensus size and coverage correspond to your earlier estimation?    Question  How many contigs in total did the assembly produce?    Question  What is the N50 of the assembly? What does this mean?",
            "title": "Quality of the Assembly"
        },
        {
            "location": "/tutorials/docs/assembly/#fixing-misassemblies",
            "text": "Pilon is a software tool which can be used to automatically improve draft assemblies.\nIt attempts to make improvements to the input genome, including:   Single base differences  Small Indels  Larger Indels or block substitution events  Gap filling  Identification of local misassemblies, including optional opening of new gaps   Pilon then outputs a FASTA file containing an improved representation of the genome from the read data and an optional VCF file detailing variation seen between the read data and the input genome.  Before running Pilon itself, we have to align our reads against the assembly  bowtie2-build m_genitalium.fasta m_genitalium\nbowtie2 -x m_genitalium -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz | \\\n    samtools view -bS -o m_genitalium.bam\nsamtools sort m_genitalium.bam -o m_genitalium.sorted.bam\nsamtools index m_genitalium.sorted.bam  then we run Pilon  pilon --genome m_genitalium.fasta --frags m_genitalium.sorted.bam --output m_genitalium_improved  which will correct eventual mismatches in our assembly and write the new improved assembly to  m_genitalium_improved.fasta",
            "title": "Fixing misassemblies"
        },
        {
            "location": "/tutorials/docs/assembly/#assembly-completeness",
            "text": "Although quast output a range of metric to assess how contiguous our assembly is, having a long N50 does not guarantee a good assembly: it could be riddled by misassemblies!  We will run  busco  to try to find marker genes in our assembly. Marker genes are conserved across a range of species and finding intact conserved genes in our assembly would be a good indication of its quality  First we need to download and unpack the bacterial datasets used by  busco  wget http://busco.ezlab.org/datasets/bacteria_odb9.tar.gz\ntar xzf bacteria_odb9.tar.gz  then we can run  busco  with  BUSCO.py -i m_genitalium.fasta -l bacteria_odb9 -o busco_genitalium -m genome   Question  How many marker genes has  busco  found?",
            "title": "Assembly Completeness"
        },
        {
            "location": "/tutorials/docs/assembly/#course-literature",
            "text": "Course litteraturer for today is:",
            "title": "Course literature"
        },
        {
            "location": "/tutorials/docs/annotation/",
            "text": "Genome Annotation\n\n\nAfter you have de novo assembled your genome sequencing reads into contigs, it is useful to know what genomic features are on those contigs. The process of identifying and labelling those features is called genome annotation.\n\n\nProkka is a \u201cwrapper\u201d; it collects together several pieces of software (from various authors), and so avoids \u201cre-inventing the wheel\u201d.\n\n\nProkka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using \nProdigal\n; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found \nhere\n.\n\n\nInput data\n\n\nProkka requires assembled contigs. You will need your best assembly from the assembly tutorial.\n\n\nAlternatively, you can download an assembly \nhere\n\n\nRunning prokka\n\n\nmodule load prokka\nawk '/^>/{print \">ctg\" ++i; next}{print}' < assembly.fasta > good_contigs.fasta\nprokka --outdir annotation --kingdom Bacteria \\\n--proteins m_genitalium.faa good_contigs.fasta\n\n\n\n\nOnce Prokka has finished, examine each of its output files.\n\n\n\n\nThe GFF and GBK files contain all of the information about the features annotated (in different formats.)\n\n\nThe .txt file contains a summary of the number of features annotated.\n\n\nThe .faa file contains the protein sequences of the genes annotated.\n\n\nThe .ffn file contains the nucleotide sequences of the genes annotated.\n\n\n\n\nVisualising the annotation\n\n\nArtemis is a graphical Java program to browse annotated genomes. Download it \nhere\n and install it on your local computer.\n\n\nCopy the .gff file produced by prokka on your computer, and open it with artemis.\n\n\nYou will be overwhelmed and/or confused at first, and possibly permanently. Here are some tips:\n\n\n\n\nThere are 3 panels: feature map (top), sequence (middle), feature list (bottom)\n\n\nClick right-mouse-button on bottom panel and select Show products\n\n\nZooming is done via the verrtical scroll bars in the two top panels",
            "title": "Genome Annotation"
        },
        {
            "location": "/tutorials/docs/annotation/#genome-annotation",
            "text": "After you have de novo assembled your genome sequencing reads into contigs, it is useful to know what genomic features are on those contigs. The process of identifying and labelling those features is called genome annotation.  Prokka is a \u201cwrapper\u201d; it collects together several pieces of software (from various authors), and so avoids \u201cre-inventing the wheel\u201d.  Prokka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using  Prodigal ; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found  here .",
            "title": "Genome Annotation"
        },
        {
            "location": "/tutorials/docs/annotation/#input-data",
            "text": "Prokka requires assembled contigs. You will need your best assembly from the assembly tutorial.  Alternatively, you can download an assembly  here",
            "title": "Input data"
        },
        {
            "location": "/tutorials/docs/annotation/#running-prokka",
            "text": "module load prokka\nawk '/^>/{print \">ctg\" ++i; next}{print}' < assembly.fasta > good_contigs.fasta\nprokka --outdir annotation --kingdom Bacteria \\\n--proteins m_genitalium.faa good_contigs.fasta  Once Prokka has finished, examine each of its output files.   The GFF and GBK files contain all of the information about the features annotated (in different formats.)  The .txt file contains a summary of the number of features annotated.  The .faa file contains the protein sequences of the genes annotated.  The .ffn file contains the nucleotide sequences of the genes annotated.",
            "title": "Running prokka"
        },
        {
            "location": "/tutorials/docs/annotation/#visualising-the-annotation",
            "text": "Artemis is a graphical Java program to browse annotated genomes. Download it  here  and install it on your local computer.  Copy the .gff file produced by prokka on your computer, and open it with artemis.  You will be overwhelmed and/or confused at first, and possibly permanently. Here are some tips:   There are 3 panels: feature map (top), sequence (middle), feature list (bottom)  Click right-mouse-button on bottom panel and select Show products  Zooming is done via the verrtical scroll bars in the two top panels",
            "title": "Visualising the annotation"
        },
        {
            "location": "/tutorials/docs/pan_genome/",
            "text": "Pan-Genome Analysis\n\n\nIn this tutorial we will learn how to determine a pan-genome from a collection of isolate genomes.\n\n\nThis tutorial is inspired from \nGenome annotation and Pangenome Analysis\n from the CBIB in Santiago, Chile\n\n\nGetting the data\n\n\nWe'll use six \nListeria monocytogenes\n genomes in this tutorial.\n\n\nwget https://github.com/HadrienG/tutorials/blob/master/data/pangenome.tar.gz\ntar xzf pangenome.tar.gz\ncd pangenome\n\n\n\n\nThese genomes correspond to isolates of \nL. monocytogenes\n reported in\n\n\n\n\nXiangyu Deng, Adam M Phillippy, Zengxin Li, Steven L Salzberg and Wei Zhang. (2010) Probing the pan-genome of Listeria monocytogenes: new insights into intraspecific niche expansion and genomic diversification. doi:10.1186/1471-2164-11-500\n\n\n\n\nThe six genomes you downloaded were selected based on their level of completeness (finished; contigs, etc) and their genotype (type I-III):\n\n\n\n\n\n\n\n\nGenome Assembly\n\n\nGenome Accession\n\n\nGenotype\n\n\nSequenced by\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\nGCA_000026705\n\n\nFM242711\n\n\ntype I\n\n\nInstitut_Pasteur\n\n\nFinished\n\n\n\n\n\n\nGCA_000008285\n\n\nAE017262\n\n\ntype I\n\n\nTIGR\n\n\nFinished\n\n\n\n\n\n\nGCA_000168815\n\n\nAATL00000000\n\n\ntype I\n\n\nBroad Institute\n\n\n79 contigs\n\n\n\n\n\n\nGCA_000196035\n\n\nAL591824\n\n\ntype II\n\n\nEuropean Consortium\n\n\nFinished\n\n\n\n\n\n\nGCA_000168635\n\n\nAARW00000000\n\n\ntype II\n\n\nBroad Institute\n\n\n25 contigs\n\n\n\n\n\n\nGCA_000021185\n\n\nCP001175\n\n\ntype III\n\n\nMSU\n\n\nFinished\n\n\n\n\n\n\n\n\nAnnotation of the genomes\n\n\nBy annotating the genomes we mean to add information regarding genes, their location, strandedness, and features and attributes. Now that you have the genomes, we need to annotate them to determine the location and attributes of the genes contained in them. We will use Prokka for the annotation.\n\n\nprokka --kingdom Bacteria --outdir prokka_GCA_000008285 --genus Listeria --locustag GCA_000008285 GCA_000008285.1_ASM828v1_genomic.fna\n\n\n\n\nAnnotate the 6 genomes, by replacing the \n-outdir\n and \n-locustag\n and \nfasta file\n accordingly.\n\n\nPan-genome analysis\n\n\nput all the .gff files in the same folder (e.g., ./gff) and run Roary\n\n\nroary -f results -e -n -v gff/*.gff\n\n\nRoary will get all the coding sequences, convert them into protein, and create pre-clusters. Then, using BLASTP and MCL, Roary will create clusters, and check for paralogs. Finally, Roary will take every isolate and order them by presence/absence of orthologs. The summary output is present in the \nsummary_statistics.txt\n file.\n\n\nAdditionally, Roary produces a \ngene_presence_absence.csv\n file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not.\n\n\nPlotting the result\n\n\nRoary comes with a python script that allows you to generate a few plots to graphically assess your analysis output.\n\n\nFirst, we need to generate a tree file from the alignment generated by Roary:\n\n\nFastTree \u2013nt \u2013gtr core_gene_alignment.aln > my_tree.newick\n\n\n\n\nThen we can plot the Roary results:\n\n\nroary_plots.py my_tree.newick gene_presence_absence.csv",
            "title": "Pan-genome Analysis"
        },
        {
            "location": "/tutorials/docs/pan_genome/#pan-genome-analysis",
            "text": "In this tutorial we will learn how to determine a pan-genome from a collection of isolate genomes.  This tutorial is inspired from  Genome annotation and Pangenome Analysis  from the CBIB in Santiago, Chile",
            "title": "Pan-Genome Analysis"
        },
        {
            "location": "/tutorials/docs/pan_genome/#getting-the-data",
            "text": "We'll use six  Listeria monocytogenes  genomes in this tutorial.  wget https://github.com/HadrienG/tutorials/blob/master/data/pangenome.tar.gz\ntar xzf pangenome.tar.gz\ncd pangenome  These genomes correspond to isolates of  L. monocytogenes  reported in   Xiangyu Deng, Adam M Phillippy, Zengxin Li, Steven L Salzberg and Wei Zhang. (2010) Probing the pan-genome of Listeria monocytogenes: new insights into intraspecific niche expansion and genomic diversification. doi:10.1186/1471-2164-11-500   The six genomes you downloaded were selected based on their level of completeness (finished; contigs, etc) and their genotype (type I-III):     Genome Assembly  Genome Accession  Genotype  Sequenced by  Status      GCA_000026705  FM242711  type I  Institut_Pasteur  Finished    GCA_000008285  AE017262  type I  TIGR  Finished    GCA_000168815  AATL00000000  type I  Broad Institute  79 contigs    GCA_000196035  AL591824  type II  European Consortium  Finished    GCA_000168635  AARW00000000  type II  Broad Institute  25 contigs    GCA_000021185  CP001175  type III  MSU  Finished",
            "title": "Getting the data"
        },
        {
            "location": "/tutorials/docs/pan_genome/#annotation-of-the-genomes",
            "text": "By annotating the genomes we mean to add information regarding genes, their location, strandedness, and features and attributes. Now that you have the genomes, we need to annotate them to determine the location and attributes of the genes contained in them. We will use Prokka for the annotation.  prokka --kingdom Bacteria --outdir prokka_GCA_000008285 --genus Listeria --locustag GCA_000008285 GCA_000008285.1_ASM828v1_genomic.fna  Annotate the 6 genomes, by replacing the  -outdir  and  -locustag  and  fasta file  accordingly.",
            "title": "Annotation of the genomes"
        },
        {
            "location": "/tutorials/docs/pan_genome/#pan-genome-analysis_1",
            "text": "put all the .gff files in the same folder (e.g., ./gff) and run Roary  roary -f results -e -n -v gff/*.gff  Roary will get all the coding sequences, convert them into protein, and create pre-clusters. Then, using BLASTP and MCL, Roary will create clusters, and check for paralogs. Finally, Roary will take every isolate and order them by presence/absence of orthologs. The summary output is present in the  summary_statistics.txt  file.  Additionally, Roary produces a  gene_presence_absence.csv  file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not.",
            "title": "Pan-genome analysis"
        },
        {
            "location": "/tutorials/docs/pan_genome/#plotting-the-result",
            "text": "Roary comes with a python script that allows you to generate a few plots to graphically assess your analysis output.  First, we need to generate a tree file from the alignment generated by Roary:  FastTree \u2013nt \u2013gtr core_gene_alignment.aln > my_tree.newick  Then we can plot the Roary results:  roary_plots.py my_tree.newick gene_presence_absence.csv",
            "title": "Plotting the result"
        },
        {
            "location": "/tutorials/docs/nanopore/",
            "text": "Introduction to Nanopore Sequencing\n\n\nIn this tutorial we will assemble the \nE. coli\n genome using a mix of long, error-prone reads from the MinION (Oxford Nanopore) and short reads from a HiSeq instrument (Illumina).\n\n\nThe MinION data used in this tutorial come a test run by the \nLoman lab\n.  \n\nThe Illumina data were simulated using \nInSilicoSeq\n\n\nGet the Data\n\n\nFirst download the nanopore data\n\n\nfastq-dump ERR1147227\n\n\n\n\nYou will not need the HiSeq data right away, but you can start the download in another window\n\n\ncurl -O -J -L https://osf.io/pxk7f/download\ncurl -O -J -L https://osf.io/zax3c/download\n\n\n\n\nlook at basic stats of the nanopore reads\n\n\nassembly-stats ERR1147227.fastq\n\n\n\n\n\n\nQuestion\n\n\nHow many nanopore reads do we have?\n\n\n\n\n\n\nQuestion\n\n\nHow long is the longest read?\n\n\n\n\n\n\nQuestion\n\n\nWhat is the average read length?\n\n\n\n\nAdapter trimming\n\n\nWe'll use \nporechop\n to remove the adapters from the reads.\nAdditionally to trim the adapters at the 3' and 5' ends, porechop can split the reads if it finds adapters in the middle.\n\n\nporechop -i ERR1147227.fastq -o ERR1147227_trimmed.fastq\n\n\n\n\nAssembly\n\n\nWe assemble the reads using miniasm\n\n\nminimap2 -x ava-ont ERR1147227_trimmed.fastq ERR1147227_trimmed.fastq | \\\n    gzip -1 > ERR1147227.paf.gz\nminiasm -f ERR1147227_trimmed.fastq ERR1147227.paf.gz > ERR1147227.gfa\nawk '/^S/{print \">\"$2\"\\n\"$3}' ERR1147227.gfa | fold > ERR1147227.fasta\n\n\n\n\n\n\nNote\n\n\nMiniasm is a fast but has no consensus step.\nThe accuracy of the assembly will be equal to the base accuracy.\n\n\n\n\nPolishing\n\n\nSince the miniasm assembly likely contains a lot if errors, we correct it with  Illumina reads.\n\n\nFirst we map the short reads against the assembly\n\n\nbowtie2-build ERR1147227.fasta ERR1147227\nbowtie2 -x ERR1147227 -1 ecoli_hiseq_R1.fastq.gz -2 ecoli_hiseq_R2.fastq.gz | \\\n    samtools view -bS -o ERR1147227.bam\nsamtools sort ERR1147227.bam -o ERR1147227.sorted.bam\nsamtools index ERR1147227.sorted.bam\n\n\n\n\nthen we run Pilon\n\n\npilon --genome ERR1147227.fasta --frags ERR1147227.sorted.bam \\\n    --output ERR1147227_improved\n\n\n\n\nwhich will correct eventual misamatches in our assembly and write the new improved assembly to \nERR1147227_improved.fasta\n\n\nFor better results we should perform more than one round of polishing.\n\n\nCompare with the existing assembly\n\n\nGo to \nhttps://www.ncbi.nlm.nih.gov\n and search for NC_000913.\nDownload the associated genome in fasta format and rename it to \necoli_ref.fasta\n\n\nnucmer --maxmatch -c 100 -p ecoli ERR1147227_trimmed.fastq ecoli_ref.fasta\nmummerplot --fat --filter --png --large -p ecoli ecoli.delta\n\n\n\n\nthen take a look at \necoli.png\n\n\nAnnotation\n\n\nawk '/^>/{print \">ctg\" ++i; next}{print}' < ERR1147227_improved.fasta \\\n    > ERR1147227_formatted.fasta\nprokka --outdir annotation --kingdom Bacteria ERR1147227_formatted.fasta\n\n\n\n\nYou can open the output to see how it went\n\n\ncat annotation/PROKKA_11232017.txt\n\n\n\n\n\n\nQuestion\n\n\nDoes it fit your expecations? How many genes were you expecting?",
            "title": "Nanopore Sequencing"
        },
        {
            "location": "/tutorials/docs/nanopore/#introduction-to-nanopore-sequencing",
            "text": "In this tutorial we will assemble the  E. coli  genome using a mix of long, error-prone reads from the MinION (Oxford Nanopore) and short reads from a HiSeq instrument (Illumina).  The MinION data used in this tutorial come a test run by the  Loman lab .   \nThe Illumina data were simulated using  InSilicoSeq",
            "title": "Introduction to Nanopore Sequencing"
        },
        {
            "location": "/tutorials/docs/nanopore/#get-the-data",
            "text": "First download the nanopore data  fastq-dump ERR1147227  You will not need the HiSeq data right away, but you can start the download in another window  curl -O -J -L https://osf.io/pxk7f/download\ncurl -O -J -L https://osf.io/zax3c/download  look at basic stats of the nanopore reads  assembly-stats ERR1147227.fastq   Question  How many nanopore reads do we have?    Question  How long is the longest read?    Question  What is the average read length?",
            "title": "Get the Data"
        },
        {
            "location": "/tutorials/docs/nanopore/#adapter-trimming",
            "text": "We'll use  porechop  to remove the adapters from the reads.\nAdditionally to trim the adapters at the 3' and 5' ends, porechop can split the reads if it finds adapters in the middle.  porechop -i ERR1147227.fastq -o ERR1147227_trimmed.fastq",
            "title": "Adapter trimming"
        },
        {
            "location": "/tutorials/docs/nanopore/#assembly",
            "text": "We assemble the reads using miniasm  minimap2 -x ava-ont ERR1147227_trimmed.fastq ERR1147227_trimmed.fastq | \\\n    gzip -1 > ERR1147227.paf.gz\nminiasm -f ERR1147227_trimmed.fastq ERR1147227.paf.gz > ERR1147227.gfa\nawk '/^S/{print \">\"$2\"\\n\"$3}' ERR1147227.gfa | fold > ERR1147227.fasta   Note  Miniasm is a fast but has no consensus step.\nThe accuracy of the assembly will be equal to the base accuracy.",
            "title": "Assembly"
        },
        {
            "location": "/tutorials/docs/nanopore/#polishing",
            "text": "Since the miniasm assembly likely contains a lot if errors, we correct it with  Illumina reads.  First we map the short reads against the assembly  bowtie2-build ERR1147227.fasta ERR1147227\nbowtie2 -x ERR1147227 -1 ecoli_hiseq_R1.fastq.gz -2 ecoli_hiseq_R2.fastq.gz | \\\n    samtools view -bS -o ERR1147227.bam\nsamtools sort ERR1147227.bam -o ERR1147227.sorted.bam\nsamtools index ERR1147227.sorted.bam  then we run Pilon  pilon --genome ERR1147227.fasta --frags ERR1147227.sorted.bam \\\n    --output ERR1147227_improved  which will correct eventual misamatches in our assembly and write the new improved assembly to  ERR1147227_improved.fasta  For better results we should perform more than one round of polishing.",
            "title": "Polishing"
        },
        {
            "location": "/tutorials/docs/nanopore/#compare-with-the-existing-assembly",
            "text": "Go to  https://www.ncbi.nlm.nih.gov  and search for NC_000913.\nDownload the associated genome in fasta format and rename it to  ecoli_ref.fasta  nucmer --maxmatch -c 100 -p ecoli ERR1147227_trimmed.fastq ecoli_ref.fasta\nmummerplot --fat --filter --png --large -p ecoli ecoli.delta  then take a look at  ecoli.png",
            "title": "Compare with the existing assembly"
        },
        {
            "location": "/tutorials/docs/nanopore/#annotation",
            "text": "awk '/^>/{print \">ctg\" ++i; next}{print}' < ERR1147227_improved.fasta \\\n    > ERR1147227_formatted.fasta\nprokka --outdir annotation --kingdom Bacteria ERR1147227_formatted.fasta  You can open the output to see how it went  cat annotation/PROKKA_11232017.txt   Question  Does it fit your expecations? How many genes were you expecting?",
            "title": "Annotation"
        },
        {
            "location": "/tutorials/docs/rna/",
            "text": "RNA-Seq\n\n\nDownloading the data\n\n\nFor this tutorial we will use the test data from \nthis\n paper:\n\n\n\n\nMalachi Griffith\n, Jason R. Walker, Nicholas C. Spies, Benjamin J. Ainscough, Obi L. Griffith\n. 2015. Informatics for RNA-seq: A web resource for analysis on the cloud. PLoS Comp Biol. 11(8):e1004393.\n\n\n\n\nThe test data consists of two commercially available RNA samples: Universal Human Reference (UHR) and Human Brain Reference (HBR). The UHR is total RNA isolated from a diverse set of 10 cancer cell lines. The HBR is total RNA isolated from the brains of 23 Caucasians, male and female, of varying age but mostly 60-80 years old.\n\n\nIn addition, a spike-in control was used. Specifically we added an aliquot of the ERCC ExFold RNA Spike-In Control Mixes to each sample. The spike-in consists of 92 transcripts that are present in known concentrations across a wide abundance range (from very few copies to many copies). This range allows us to test the degree to which the RNA-seq assay (including all laboratory and analysis steps) accurately reflects the relative abundance of transcript species within a sample. There are two 'mixes' of these transcripts to allow an assessment of differential expression output between samples if you put one mix in each of your two comparisons. In our case, Mix1 was added to the UHR sample, and Mix2 was added to the HBR sample. We also have 3 complete experimental replicates for each sample. This allows us to assess the technical variability of our overall process of producing RNA-seq data in the lab.\n\n\nFor all libraries we prepared low-throughput (Set A) TruSeq Stranded Total RNA Sample Prep Kit libraries with Ribo-Zero Gold to remove both cytoplasmic and mitochondrial rRNA. Triplicate, indexed libraries were made starting with 100ng Agilent/Strategene Universal Human Reference total RNA and 100ng Ambion Human Brain Reference total RNA. The Universal Human Reference replicates received 2 ul of 1:1000 ERCC Mix 1. The Human Brain Reference replicates received 1:1000 ERCC Mix 2. The libraries were quantified with KAPA Library Quantification qPCR and adjusted to the appropriate concentration for sequencing. The triplicate, indexed libraries were then pooled prior to sequencing. Each pool of three replicate libraries were sequenced across 2 lanes of a HiSeq 2000 using paired-end sequence chemistry with 100bp read lengths.\n\n\nSo to summarize we have:\n\n\n\n\nUHR + ERCC Spike-In Mix1, Replicate 1\n\n\nUHR + ERCC Spike-In Mix1, Replicate 2\n\n\nUHR + ERCC Spike-In Mix1, Replicate 3\n\n\nHBR + ERCC Spike-In Mix2, Replicate 1\n\n\nHBR + ERCC Spike-In Mix2, Replicate 2\n\n\nHBR + ERCC Spike-In Mix2, Replicate 3\n\n\n\n\nYou can download the data from \nhere\n.\n\n\nDownload and unpack the data\n\n\ncurl -O -J -L https://osf.io/7zepj/download\ntar xzf toy_rna.tar.gz\ncd toy_rna\n\n\n\n\nIndexing transcriptome\n\n\nsalmon index -t chr22_transcripts.fa -i chr22_index\n\n\n\n\nQuantify reads using salmon\n\n\nfor i in *_R1.fastq.gz\ndo\n   prefix=$(basename $i _R1.fastq.gz)\n   salmon quant -i chr22_index --libType A \\\n          -1 ${prefix}_R1.fastq.gz -2 ${prefix}_R2.fastq.gz -o quant/${prefix};\ndone\n\n\n\n\nThis loop simply goes through each sample and invokes salmon using fairly basic options:\n\n\n\n\nThe -i argument tells salmon where to find the index\n\n\n--libType A tells salmon that it should automatically determine the library type of the sequencing reads (e.g. stranded vs. unstranded etc.)\n\n\nThe -1 and -2 arguments tell salmon where to find the left and right reads for this sample (notice, salmon will accept gzipped FASTQ files directly).\n\n\nthe -o argument specifies the directory where salmon\u2019s quantification results sould be written.\n\n\n\n\nSalmon exposes many different options to the user that enable extra features or modify default behavior.\nHowever, the purpose and behavior of all of those options is beyond the scope of this introductory tutorial.\nYou can read about salmon\u2019s many options in the \ndocumentation\n.\n\n\nAfter the salmon commands finish running, you should have a directory named \nquant\n, which will have a sub-directory for each sample.\nThese sub-directories contain the quantification results of salmon, as well as a lot of other information salmon records about the sample and the run.\nThe main output file (called quant.sf) is rather self-explanatory. For example, take a peek at the quantification file for sample \nHBR_Rep1\n in \nquant/HBR_Rep1/quant.sf\n and you\u2019ll see a simple TSV format file listing the name (Name) of each transcript, its length (Length), effective length (EffectiveLength) (more details on this in the documentation), and its abundance in terms of Transcripts Per Million (TPM) and estimated number of reads (NumReads) originating from this transcript.\n\n\nImport read counts using tximport\n\n\nUsing the tximport R package, you can import salmon\u2019s transcript-level quantifications and optionally aggregate them to the gene level for gene-level differential expression analysis.\n\n\nFirst, go in Rstudio server by typing the address to your server in your browser:\n\n\nhttp://MY_IP_ADDRESS:8787/\n\n\nwhere you replace \nMY_IP_ADDRESS\n by the IP address of your Virtual Machine.\n\n\n\n\nNote\n\n\nTo access Rstudio server on the virtual machine, you'll need a password\nAsk your instructor for the password!\n\n\n\n\n\n\nNote\n\n\nIf you wish, you may work on Rstudio on your own laptop if it is powerful enough.\nYou will need an up-to-date version of R, and can install the necessary packages using \nthis script\n\n\nYou will also need to download the \ntoy_rna\n directory\n\n\n\n\nOnce in Rstudio, set your working directory\n\n\nsetwd('~/toy_rna')\n\n\n\n\nThen load the modules:\n\n\nlibrary(tximport)\nlibrary(GenomicFeatures)\nlibrary(readr)\n\n\n\n\nSalmon did the quantifiation of the transcript level.\nWe want to see which genes are differentially expressed, so we need to link the transcript names to the gene names.\nWe can use our .gtf annotation for that, and the GenomicFeatures package:\n\n\ntxdb <- makeTxDbFromGFF(\"chr22_genes.gtf\")\nk <- keys(txdb, keytype = \"GENEID\")\ntx2gene <- select(txdb, keys = k, keytype = \"GENEID\", columns = \"TXNAME\")\nhead(tx2gene)\n\n\n\n\nNow we can import the salmon quantification.\n\n\nsamples <- read.table(\"samples.txt\", header = TRUE)\nfiles <- file.path(\"quant\", samples$sample, \"quant.sf\")\nnames(files) <- paste0(samples$sample)\ntxi.salmon <- tximport(files, type = \"salmon\", tx2gene = tx2gene)\n\n\n\n\nTake a look at the data:\n\n\nhead(txi.salmon$counts)\n\n\n\n\nDifferential expression using DESeq2\n\n\nload DESeq2:\n\n\nlibrary(DESeq2)\n\n\n\n\nInstantiate the DESeqDataSet and generate result table. See \n?DESeqDataSetFromTximport\n and \n?DESeq\n for more information about the steps performed by the program.\n\n\ndds <- DESeqDataSetFromTximport(txi.salmon, samples, ~condition)\ndds <- DESeq(dds)\nres <- results(dds)\n\n\n\n\nRun the \nsummary\n command to get an idea of how many genes are up- and downregulated between the two conditions:\n\n\nsummary(res)\n\n\n\n\nDESeq uses a negative binomial distribution. Such distributions have two parameters: mean and dispersion. The dispersion is a parameter describing how much the variance deviates from the mean.\n\n\nYou can read more about the methods used by DESeq2 in the \npaper\n or the \nvignette\n\n\nPlot dispersions:\n\n\nplotDispEsts(dds, main=\"Dispersion plot\")\n\n\n\n\nFor clustering and heatmaps, we need to log transform our data:\n\n\nrld <- rlogTransformation(dds)\nhead(assay(rld))\n\n\n\n\nThen, we create a sample distance heatmap:\n\n\nlibrary(RColorBrewer)\nlibrary(gplots)\n\n(mycols <- brewer.pal(8, \"Dark2\")[1:length(unique(samples$condition))])\nsampleDists <- as.matrix(dist(t(assay(rld))))\nheatmap.2(as.matrix(sampleDists), key=F, trace=\"none\",\n          col=colorpanel(100, \"black\", \"white\"),\n          ColSideColors=mycols[samples$condition],\n          RowSideColors=mycols[samples$condition],\n          margin=c(10, 10), main=\"Sample Distance Matrix\")\n\n\n\n\nWe can also plot a PCA:\n\n\nDESeq2::plotPCA(rld, intgroup=\"condition\")\n\n\n\n\nIt is time to look at some p-values:\n\n\ntable(res$padj<0.05)\nres <- res[order(res$padj), ]\nresdata <- merge(as.data.frame(res), as.data.frame(counts(dds, normalized=TRUE)), by=\"row.names\", sort=FALSE)\nnames(resdata)[1] <- \"Gene\"\nhead(resdata)\n\n\n\n\nExamine plot of p-values, the MA plot and the Volcano Plot:\n\n\nhist(res$pvalue, breaks=50, col=\"grey\")\nDESeq2::plotMA(dds, ylim=c(-1,1), cex=1)\n\n# Volcano plot\nwith(res, plot(log2FoldChange, -log10(pvalue), pch=20, main=\"Volcano plot\", xlim=c(-2.5,2)))\nwith(subset(res, padj<.05 ), points(log2FoldChange, -log10(pvalue), pch=20, col=\"red\"))\n\n\n\n\nKEGG pathway analysis\n\n\nAs always, load the necessary packages:\n\n\nlibrary(AnnotationDbi)\nlibrary(org.Hs.eg.db)\nlibrary(pathview)\nlibrary(gage)\nlibrary(gageData)\n\n\n\n\nLet\u2019s use the \nmapIds\n function to add more columns to the results. The row.names of our results table has the Ensembl gene ID (our key), so we need to specify  \nkeytype=ENSEMBL\n. The column argument tells the \nmapIds\n function which information we want, and the \nmultiVals\n argument tells the function what to do if there are multiple possible values for a single input value. Here we ask to just give us back the first one that occurs in the database. Let\u2019s get the Entrez IDs, gene symbols, and full gene names.\n\n\nres$symbol <- mapIds(org.Hs.eg.db,\n                     keys=row.names(res),\n                     column=\"SYMBOL\",\n                     keytype=\"ENSEMBL\",\n                     multiVals=\"first\")\nres$entrez <- mapIds(org.Hs.eg.db,\n                     keys=row.names(res),\n                     column=\"ENTREZID\",\n                     keytype=\"ENSEMBL\",\n                     multiVals=\"first\")\nres$name <- mapIds(org.Hs.eg.db,\n                     keys=row.names(res),\n                     column=\"GENENAME\",\n                     keytype=\"ENSEMBL\",\n                     multiVals=\"first\")\n\nhead(res)\n\n\n\n\nWe\u2019re going to use the \ngage\n package for pathway analysis, and the \npathview\n package to draw a pathway diagram.\n\n\nThe gageData package has pre-compiled databases mapping genes to KEGG pathways and GO terms for common organisms:\n\n\ndata(kegg.sets.hs)\ndata(sigmet.idx.hs)\nkegg.sets.hs <- kegg.sets.hs[sigmet.idx.hs]\nhead(kegg.sets.hs, 3)\n\n\n\n\nRun the pathway analysis. See help on the gage function with \n?gage\n. Specifically, you might want to try changing the value of same.dir.\n\n\nfoldchanges <- res$log2FoldChange\nnames(foldchanges) <- res$entrez\nkeggres <- gage(foldchanges, gsets=kegg.sets.hs, same.dir=TRUE)\nlapply(keggres, head)\n\n\n\n\nPull out the top 5 upregulated pathways, then further process that just to get the IDs. We\u2019ll use these KEGG pathway IDs downstream for plotting. The \ndplyr\n package is required to use the pipe (\n%>%\n) construct.\n\n\nlibrary(dplyr)\n\n# Get the pathways\nkeggrespathways <- data.frame(id=rownames(keggres$greater), keggres$greater) %>%\n  tbl_df() %>%\n  filter(row_number()<=5) %>%\n  .$id %>%\n  as.character()\nkeggrespathways\n\n# Get the IDs.\nkeggresids <- substr(keggrespathways, start=1, stop=8)\nkeggresids\n\n\n\n\nFinally, the \npathview()\n function in the pathview package makes the plots. Let\u2019s write a function so we can loop through and draw plots for the top 5 pathways we created above.\n\n\n# Define plotting function for applying later\nplot_pathway <- function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=\"hsa\", new.signature=FALSE)\n\n# Unload dplyr since it conflicts with the next line\ndetach(\"package:dplyr\", unload=T)\n\n# plot multiple pathways (plots saved to disk and returns a throwaway list object)\ntmp <- sapply(keggresids, function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=\"hsa\"))\n\n\n\n\nThanks\n\n\nThis material was inspired by Stephen Turner's blog post:\n\n\n\n\nTutorial: RNA-seq differential expression & pathway analysis with Sailfish, DESeq2, GAGE, and Pathview: http://www.gettinggeneticsdone.com/2015/12/tutorial-rna-seq-differential.html",
            "title": "RNA Sequencing"
        },
        {
            "location": "/tutorials/docs/rna/#rna-seq",
            "text": "",
            "title": "RNA-Seq"
        },
        {
            "location": "/tutorials/docs/rna/#downloading-the-data",
            "text": "For this tutorial we will use the test data from  this  paper:   Malachi Griffith , Jason R. Walker, Nicholas C. Spies, Benjamin J. Ainscough, Obi L. Griffith . 2015. Informatics for RNA-seq: A web resource for analysis on the cloud. PLoS Comp Biol. 11(8):e1004393.   The test data consists of two commercially available RNA samples: Universal Human Reference (UHR) and Human Brain Reference (HBR). The UHR is total RNA isolated from a diverse set of 10 cancer cell lines. The HBR is total RNA isolated from the brains of 23 Caucasians, male and female, of varying age but mostly 60-80 years old.  In addition, a spike-in control was used. Specifically we added an aliquot of the ERCC ExFold RNA Spike-In Control Mixes to each sample. The spike-in consists of 92 transcripts that are present in known concentrations across a wide abundance range (from very few copies to many copies). This range allows us to test the degree to which the RNA-seq assay (including all laboratory and analysis steps) accurately reflects the relative abundance of transcript species within a sample. There are two 'mixes' of these transcripts to allow an assessment of differential expression output between samples if you put one mix in each of your two comparisons. In our case, Mix1 was added to the UHR sample, and Mix2 was added to the HBR sample. We also have 3 complete experimental replicates for each sample. This allows us to assess the technical variability of our overall process of producing RNA-seq data in the lab.  For all libraries we prepared low-throughput (Set A) TruSeq Stranded Total RNA Sample Prep Kit libraries with Ribo-Zero Gold to remove both cytoplasmic and mitochondrial rRNA. Triplicate, indexed libraries were made starting with 100ng Agilent/Strategene Universal Human Reference total RNA and 100ng Ambion Human Brain Reference total RNA. The Universal Human Reference replicates received 2 ul of 1:1000 ERCC Mix 1. The Human Brain Reference replicates received 1:1000 ERCC Mix 2. The libraries were quantified with KAPA Library Quantification qPCR and adjusted to the appropriate concentration for sequencing. The triplicate, indexed libraries were then pooled prior to sequencing. Each pool of three replicate libraries were sequenced across 2 lanes of a HiSeq 2000 using paired-end sequence chemistry with 100bp read lengths.  So to summarize we have:   UHR + ERCC Spike-In Mix1, Replicate 1  UHR + ERCC Spike-In Mix1, Replicate 2  UHR + ERCC Spike-In Mix1, Replicate 3  HBR + ERCC Spike-In Mix2, Replicate 1  HBR + ERCC Spike-In Mix2, Replicate 2  HBR + ERCC Spike-In Mix2, Replicate 3   You can download the data from  here .  Download and unpack the data  curl -O -J -L https://osf.io/7zepj/download\ntar xzf toy_rna.tar.gz\ncd toy_rna",
            "title": "Downloading the data"
        },
        {
            "location": "/tutorials/docs/rna/#indexing-transcriptome",
            "text": "salmon index -t chr22_transcripts.fa -i chr22_index",
            "title": "Indexing transcriptome"
        },
        {
            "location": "/tutorials/docs/rna/#quantify-reads-using-salmon",
            "text": "for i in *_R1.fastq.gz\ndo\n   prefix=$(basename $i _R1.fastq.gz)\n   salmon quant -i chr22_index --libType A \\\n          -1 ${prefix}_R1.fastq.gz -2 ${prefix}_R2.fastq.gz -o quant/${prefix};\ndone  This loop simply goes through each sample and invokes salmon using fairly basic options:   The -i argument tells salmon where to find the index  --libType A tells salmon that it should automatically determine the library type of the sequencing reads (e.g. stranded vs. unstranded etc.)  The -1 and -2 arguments tell salmon where to find the left and right reads for this sample (notice, salmon will accept gzipped FASTQ files directly).  the -o argument specifies the directory where salmon\u2019s quantification results sould be written.   Salmon exposes many different options to the user that enable extra features or modify default behavior.\nHowever, the purpose and behavior of all of those options is beyond the scope of this introductory tutorial.\nYou can read about salmon\u2019s many options in the  documentation .  After the salmon commands finish running, you should have a directory named  quant , which will have a sub-directory for each sample.\nThese sub-directories contain the quantification results of salmon, as well as a lot of other information salmon records about the sample and the run.\nThe main output file (called quant.sf) is rather self-explanatory. For example, take a peek at the quantification file for sample  HBR_Rep1  in  quant/HBR_Rep1/quant.sf  and you\u2019ll see a simple TSV format file listing the name (Name) of each transcript, its length (Length), effective length (EffectiveLength) (more details on this in the documentation), and its abundance in terms of Transcripts Per Million (TPM) and estimated number of reads (NumReads) originating from this transcript.",
            "title": "Quantify reads using salmon"
        },
        {
            "location": "/tutorials/docs/rna/#import-read-counts-using-tximport",
            "text": "Using the tximport R package, you can import salmon\u2019s transcript-level quantifications and optionally aggregate them to the gene level for gene-level differential expression analysis.  First, go in Rstudio server by typing the address to your server in your browser:  http://MY_IP_ADDRESS:8787/  where you replace  MY_IP_ADDRESS  by the IP address of your Virtual Machine.   Note  To access Rstudio server on the virtual machine, you'll need a password\nAsk your instructor for the password!    Note  If you wish, you may work on Rstudio on your own laptop if it is powerful enough.\nYou will need an up-to-date version of R, and can install the necessary packages using  this script  You will also need to download the  toy_rna  directory   Once in Rstudio, set your working directory  setwd('~/toy_rna')  Then load the modules:  library(tximport)\nlibrary(GenomicFeatures)\nlibrary(readr)  Salmon did the quantifiation of the transcript level.\nWe want to see which genes are differentially expressed, so we need to link the transcript names to the gene names.\nWe can use our .gtf annotation for that, and the GenomicFeatures package:  txdb <- makeTxDbFromGFF(\"chr22_genes.gtf\")\nk <- keys(txdb, keytype = \"GENEID\")\ntx2gene <- select(txdb, keys = k, keytype = \"GENEID\", columns = \"TXNAME\")\nhead(tx2gene)  Now we can import the salmon quantification.  samples <- read.table(\"samples.txt\", header = TRUE)\nfiles <- file.path(\"quant\", samples$sample, \"quant.sf\")\nnames(files) <- paste0(samples$sample)\ntxi.salmon <- tximport(files, type = \"salmon\", tx2gene = tx2gene)  Take a look at the data:  head(txi.salmon$counts)",
            "title": "Import read counts using tximport"
        },
        {
            "location": "/tutorials/docs/rna/#differential-expression-using-deseq2",
            "text": "load DESeq2:  library(DESeq2)  Instantiate the DESeqDataSet and generate result table. See  ?DESeqDataSetFromTximport  and  ?DESeq  for more information about the steps performed by the program.  dds <- DESeqDataSetFromTximport(txi.salmon, samples, ~condition)\ndds <- DESeq(dds)\nres <- results(dds)  Run the  summary  command to get an idea of how many genes are up- and downregulated between the two conditions:  summary(res)  DESeq uses a negative binomial distribution. Such distributions have two parameters: mean and dispersion. The dispersion is a parameter describing how much the variance deviates from the mean.  You can read more about the methods used by DESeq2 in the  paper  or the  vignette  Plot dispersions:  plotDispEsts(dds, main=\"Dispersion plot\")  For clustering and heatmaps, we need to log transform our data:  rld <- rlogTransformation(dds)\nhead(assay(rld))  Then, we create a sample distance heatmap:  library(RColorBrewer)\nlibrary(gplots)\n\n(mycols <- brewer.pal(8, \"Dark2\")[1:length(unique(samples$condition))])\nsampleDists <- as.matrix(dist(t(assay(rld))))\nheatmap.2(as.matrix(sampleDists), key=F, trace=\"none\",\n          col=colorpanel(100, \"black\", \"white\"),\n          ColSideColors=mycols[samples$condition],\n          RowSideColors=mycols[samples$condition],\n          margin=c(10, 10), main=\"Sample Distance Matrix\")  We can also plot a PCA:  DESeq2::plotPCA(rld, intgroup=\"condition\")  It is time to look at some p-values:  table(res$padj<0.05)\nres <- res[order(res$padj), ]\nresdata <- merge(as.data.frame(res), as.data.frame(counts(dds, normalized=TRUE)), by=\"row.names\", sort=FALSE)\nnames(resdata)[1] <- \"Gene\"\nhead(resdata)  Examine plot of p-values, the MA plot and the Volcano Plot:  hist(res$pvalue, breaks=50, col=\"grey\")\nDESeq2::plotMA(dds, ylim=c(-1,1), cex=1)\n\n# Volcano plot\nwith(res, plot(log2FoldChange, -log10(pvalue), pch=20, main=\"Volcano plot\", xlim=c(-2.5,2)))\nwith(subset(res, padj<.05 ), points(log2FoldChange, -log10(pvalue), pch=20, col=\"red\"))",
            "title": "Differential expression using DESeq2"
        },
        {
            "location": "/tutorials/docs/rna/#kegg-pathway-analysis",
            "text": "As always, load the necessary packages:  library(AnnotationDbi)\nlibrary(org.Hs.eg.db)\nlibrary(pathview)\nlibrary(gage)\nlibrary(gageData)  Let\u2019s use the  mapIds  function to add more columns to the results. The row.names of our results table has the Ensembl gene ID (our key), so we need to specify   keytype=ENSEMBL . The column argument tells the  mapIds  function which information we want, and the  multiVals  argument tells the function what to do if there are multiple possible values for a single input value. Here we ask to just give us back the first one that occurs in the database. Let\u2019s get the Entrez IDs, gene symbols, and full gene names.  res$symbol <- mapIds(org.Hs.eg.db,\n                     keys=row.names(res),\n                     column=\"SYMBOL\",\n                     keytype=\"ENSEMBL\",\n                     multiVals=\"first\")\nres$entrez <- mapIds(org.Hs.eg.db,\n                     keys=row.names(res),\n                     column=\"ENTREZID\",\n                     keytype=\"ENSEMBL\",\n                     multiVals=\"first\")\nres$name <- mapIds(org.Hs.eg.db,\n                     keys=row.names(res),\n                     column=\"GENENAME\",\n                     keytype=\"ENSEMBL\",\n                     multiVals=\"first\")\n\nhead(res)  We\u2019re going to use the  gage  package for pathway analysis, and the  pathview  package to draw a pathway diagram.  The gageData package has pre-compiled databases mapping genes to KEGG pathways and GO terms for common organisms:  data(kegg.sets.hs)\ndata(sigmet.idx.hs)\nkegg.sets.hs <- kegg.sets.hs[sigmet.idx.hs]\nhead(kegg.sets.hs, 3)  Run the pathway analysis. See help on the gage function with  ?gage . Specifically, you might want to try changing the value of same.dir.  foldchanges <- res$log2FoldChange\nnames(foldchanges) <- res$entrez\nkeggres <- gage(foldchanges, gsets=kegg.sets.hs, same.dir=TRUE)\nlapply(keggres, head)  Pull out the top 5 upregulated pathways, then further process that just to get the IDs. We\u2019ll use these KEGG pathway IDs downstream for plotting. The  dplyr  package is required to use the pipe ( %>% ) construct.  library(dplyr)\n\n# Get the pathways\nkeggrespathways <- data.frame(id=rownames(keggres$greater), keggres$greater) %>%\n  tbl_df() %>%\n  filter(row_number()<=5) %>%\n  .$id %>%\n  as.character()\nkeggrespathways\n\n# Get the IDs.\nkeggresids <- substr(keggrespathways, start=1, stop=8)\nkeggresids  Finally, the  pathview()  function in the pathview package makes the plots. Let\u2019s write a function so we can loop through and draw plots for the top 5 pathways we created above.  # Define plotting function for applying later\nplot_pathway <- function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=\"hsa\", new.signature=FALSE)\n\n# Unload dplyr since it conflicts with the next line\ndetach(\"package:dplyr\", unload=T)\n\n# plot multiple pathways (plots saved to disk and returns a throwaway list object)\ntmp <- sapply(keggresids, function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=\"hsa\"))",
            "title": "KEGG pathway analysis"
        },
        {
            "location": "/tutorials/docs/rna/#thanks",
            "text": "This material was inspired by Stephen Turner's blog post:   Tutorial: RNA-seq differential expression & pathway analysis with Sailfish, DESeq2, GAGE, and Pathview: http://www.gettinggeneticsdone.com/2015/12/tutorial-rna-seq-differential.html",
            "title": "Thanks"
        },
        {
            "location": "/tutorials/docs/16S/",
            "text": "Metabarcoding\n\n\nThis tutorial is aimed at being a walkthrough of the DADA2 pipeline.\nIt uses the data of the now famous \nMiSeq SOP\n by the Mothur authors but analyses the data using DADA2.\n\n\nDADA2 is a relatively new method to analyse amplicon data which uses exact variants instead of OTUs.    \n\n\nThe advantages of the DADA2 method is described in the \npaper\n\n\nBefore Starting\n\n\nThere are two ways to follow this tutorial: you can copy and paste all the codes blocks below in R directly, or you can download this document in the Rmarkdown format and execute the cells.\n\n\n\n\nLink to the document in Rmarkdown\n\n\n\n\nInstall and Load Packages\n\n\nFirst install DADA2 and other necessary packages\n\n\nsource('https://bioconductor.org/biocLite.R')\nbiocLite('dada2')\nbiocLite('phyloseq')\nbiocLite('DECIPHER')\ninstall.packages('ggplot2')\ninstall.packages('phangorn')\n\n\n\n\nNow load the packages and verify you have the correct DADA2 version\n\n\nlibrary(dada2)\nlibrary(ggplot2)\nlibrary(phyloseq)\nlibrary(phangorn)\nlibrary(DECIPHER)\npackageVersion('dada2')\n\n\n\n\nDownload the Data\n\n\nYou will also need to download the data, as well as the SILVA database\n\n\n\n\nWarning\n\n\nIf you are following the tutorial on the website, the following block of commands has to be executed outside of R.\nIf you run this tutorial with the R notebook, you can simply execute the cell block\n\n\n\n\nwget http://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip\nunzip MiSeqSOPData.zip\nrm -r __MACOSX/\ncd MiSeq_SOP\nwget https://zenodo.org/record/824551/files/silva_nr_v128_train_set.fa.gz\nwget https://zenodo.org/record/824551/files/silva_species_assignment_v128.fa.gz\ncd ..\n\n\n\n\nBack in R, check that you have downloaded the data\n\n\npath <- 'MiSeq_SOP'\nlist.files(path)\n\n\n\n\nFiltering and Trimming\n\n\nFirst we create two lists with the sorted name of the reads: one for the forward reads, one for the reverse reads\n\n\nraw_forward <- sort(list.files(path, pattern=\"_R1_001.fastq\",\n                               full.names=TRUE))\n\nraw_reverse <- sort(list.files(path, pattern=\"_R2_001.fastq\",\n                               full.names=TRUE))\n\n# we also need the sample names\nsample_names <- sapply(strsplit(basename(raw_forward), \"_\"),\n                       `[`,  # extracts the first element of a subset\n                       1)\n\n\n\n\nthen we visualise the quality of our reads\n\n\nplotQualityProfile(raw_forward[1:2])\nplotQualityProfile(raw_reverse[1:2])\n\n\n\n\n\n\nQuestion\n\n\nWhat do you think of the read quality?\n\n\n\n\nThe forward reads are good quality (although dropping a bit at the end as usual) while the reverse are way worse.\n\n\nBased on these profiles, we will truncate the forward reads at position 240 and the reverse reads at position 160 where the quality distribution crashes.\n\n\n\n\nNote\n\n\nin this tutorial we perform the trimming using DADA2's own functions.\nIf you wish to do it outside of DADA2, you can refer to the \nQuality Control tutorial\n\n\n\n\n\n\nDada2 requires us to define the name of our output files\n\n\n# place filtered files in filtered/ subdirectory\nfiltered_path <- file.path(path, \"filtered\")\n\nfiltered_forward <- file.path(filtered_path,\n                              paste0(sample_names, \"_R1_trimmed.fastq.gz\"))\n\nfiltered_reverse <- file.path(filtered_path,\n                              paste0(sample_names, \"_R2_trimmed.fastq.gz\"))\n\n\n\n\nWe\u2019ll use standard filtering parameters: \nmaxN=0\n (DADA22 requires no Ns), \ntruncQ=2\n, \nrm.phix=TRUE\n and \nmaxEE=2\n.\nThe maxEE parameter sets the maximum number of \u201cexpected errors\u201d allowed in a read, which \naccording to the USEARCH authors\n is a better filter than simply averaging quality scores.\n\n\nout <- filterAndTrim(raw_forward, filtered_forward, raw_reverse,\n                     filtered_reverse, truncLen=c(240,160), maxN=0,\n                     maxEE=c(2,2), truncQ=2, rm.phix=TRUE, compress=TRUE,\n                     multithread=TRUE)\nhead(out)\n\n\n\n\nLearn the Error Rates\n\n\nThe DADA2 algorithm depends on a parametric error model and every amplicon dataset has a slightly different error rate.\nThe \nlearnErrors\n of Dada2 learns the error model from the data and will help DADA2 to fits its method to your data\n\n\nerrors_forward <- learnErrors(filtered_forward, multithread=TRUE)\nerrors_reverse <- learnErrors(filtered_reverse, multithread=TRUE)\n\n\n\n\nthen we visualise the estimated error rates\n\n\nplotErrors(errors_forward, nominalQ=TRUE) +\n    theme_minimal()\n\n\n\n\n\n\nQuestion\n\n\nDo you think the error model fits your data correctly?\n\n\n\n\nDereplication\n\n\nFrom the Dada2 documentation:\n\n\n\n\nDereplication\n combines all identical sequencing reads into into \u201cunique sequences\u201d with a corresponding \u201cabundance\u201d: the number of reads with that unique sequence.\n\nDereplication\n substantially reduces computation time by eliminating redundant comparisons.\n\n\n\n\nderep_forward <- derepFastq(filtered_forward, verbose=TRUE)\nderep_reverse <- derepFastq(filtered_reverse, verbose=TRUE)\n# name the derep-class objects by the sample names\nnames(derep_forward) <- sample_names\nnames(derep_reverse) <- sample_names\n\n\n\n\nSample inference\n\n\nWe are now ready to apply the core sequence-variant inference algorithm to the dereplicated data.\n\n\ndada_forward <- dada(derep_forward, err=errors_forward, multithread=TRUE)\ndada_reverse <- dada(derep_reverse, err=errors_reverse, multithread=TRUE)\n\n# inspect the dada-class object\ndada_forward[[1]]\n\n\n\n\nThe DADA2 algorithm inferred 128 real sequence variants from the 1979 unique sequences in the first sample.\n\n\nMerge Paired-end Reads\n\n\nNow that the reads are trimmed, dereplicated and error-corrected we can merge them together\n\n\nmerged_reads <- mergePairs(dada_forward, derep_forward, dada_reverse,\n                           derep_reverse, verbose=TRUE)\n\n# inspect the merger data.frame from the first sample\nhead(merged_reads[[1]])\n\n\n\n\nConstruct Sequence Table\n\n\nWe can now construct a sequence table of our mouse samples, a higher-resolution version of the OTU table produced by traditional methods.\n\n\nseq_table <- makeSequenceTable(merged_reads)\ndim(seq_table)\n\n# inspect distribution of sequence lengths\ntable(nchar(getSequences(seq_table)))\n\n\n\n\nRemove Chimeras\n\n\nThe \ndada\n method used earlier removes substitutions and indel errors but chimeras remain.\nWe remove the chimeras with\n\n\nseq_table_nochim <- removeBimeraDenovo(seq_table, method='consensus',\n                                       multithread=TRUE, verbose=TRUE)\ndim(seq_table_nochim)\n\n# which percentage of our reads did we keep?\nsum(seq_table_nochim) / sum(seq_table)\n\n\n\n\nAs a final check of our progress, we\u2019ll look at the number of reads that made it through each step in the pipeline\n\n\nget_n <- function(x) sum(getUniques(x))\n\ntrack <- cbind(out, sapply(dada_forward, get_n), sapply(merged_reads, get_n),\n               rowSums(seq_table), rowSums(seq_table_nochim))\n\ncolnames(track) <- c('input', 'filtered', 'denoised', 'merged', 'tabled',\n                     'nonchim')\nrownames(track) <- sample_names\nhead(track)\n\n\n\n\nWe kept the majority of our reads!\n\n\nAssign Taxonomy\n\n\nNow we assign taxonomy to our sequences using the SILVA database\n\n\ntaxa <- assignTaxonomy(seq_table_nochim,\n                       'MiSeq_SOP/silva_nr_v128_train_set.fa.gz',\n                       multithread=TRUE)\ntaxa <- addSpecies(taxa, 'MiSeq_SOP/silva_species_assignment_v128.fa.gz')\n\n\n\n\nfor inspecting the classification\n\n\ntaxa_print <- taxa  # removing sequence rownames for display only\nrownames(taxa_print) <- NULL\nhead(taxa_print)\n\n\n\n\nPhylogenetic Tree\n\n\nDADA2 is reference-free so we have to build the tree ourselves\n\n\nWe first align our sequences\n\n\nsequences <- getSequences(seq_table)\nnames(sequences) <- sequences  # this propagates to the tip labels of the tree\nalignment <- AlignSeqs(DNAStringSet(sequences), anchor=NA)\n\n\n\n\nThen we build a neighbour-joining tree then fit a maximum likelihood tree using the neighbour-joining tree as a starting point\n\n\nphang_align <- phyDat(as(alignment, 'matrix'), type='DNA')\ndm <- dist.ml(phang_align)\ntreeNJ <- NJ(dm)  # note, tip order != sequence order\nfit = pml(treeNJ, data=phang_align)\n\n## negative edges length changed to 0!\n\nfitGTR <- update(fit, k=4, inv=0.2)\nfitGTR <- optim.pml(fitGTR, model='GTR', optInv=TRUE, optGamma=TRUE,\n                    rearrangement = 'stochastic',\n                    control = pml.control(trace = 0))\ndetach('package:phangorn', unload=TRUE)\n\n\n\n\nPhyloseq\n\n\nFirst load the metadata\n\n\nsample_data <- read.table(\n    'https://hadrieng.github.io/tutorials/data/16S_metadata.txt',\n    header=TRUE, row.names=\"sample_name\")\n\n\n\n\nWe can now construct a phyloseq object from our output and newly created metadata\n\n\nphyseq <- phyloseq(otu_table(seq_table_nochim, taxa_are_rows=FALSE),\n                   sample_data(sample_data),\n                   tax_table(taxa),\n                   phy_tree(fitGTR$tree))\n# remove mock sample\nphyseq <- prune_samples(sample_names(physeq) != 'Mock', physeq)\nphyseq\n\n\n\n\nLet's look at the alpha diversity of our samples\n\n\nplot_richness(physeq, x='day', measures=c('Shannon', 'Fisher'), color='when') +\n    theme_minimal()\n\n\n\n\nNo obvious differences. Let's look at ordination methods (beta diversity)\n\n\nWe can perform an MDS with euclidean distance (mathematically equivalent to a PCA)\n\n\nord <- ordinate(physeq, 'MDS', 'euclidean')\nplot_ordination(physeq, ord, type='samples', color='when',\n                title='PCA of the samples from the MiSeq SOP') +\n    theme_minimal()\n\n\n\n\nnow with the Bray-Curtis distance\n\n\nord <- ordinate(physeq, 'NMDS', 'bray')\nplot_ordination(physeq, ord, type='samples', color='when',\n                title='PCA of the samples from the MiSeq SOP') +\n    theme_minimal()\n\n\n\n\nThere we can see a clear difference between our samples.\n\n\nLet us take a look a the distribution of the most abundant families\n\n\ntop20 <- names(sort(taxa_sums(physeq), decreasing=TRUE))[1:20]\nphyseq_top20 <- transform_sample_counts(physeq, function(OTU) OTU/sum(OTU))\nphyseq_top20 <- prune_taxa(top20, physeq_top20)\nplot_bar(physeq_top20, x='day', fill='Family') +\n    facet_wrap(~when, scales='free_x') +\n    theme_minimal()\n\n\n\n\nWe can place them in a tree\n\n\nbacteroidetes <- subset_taxa(physeq, Phylum %in% c('Bacteroidetes'))\nplot_tree(bacteroidetes, ladderize='left', size='abundance',\n          color='when', label.tips='Family')",
            "title": "Metabarcoding"
        },
        {
            "location": "/tutorials/docs/16S/#metabarcoding",
            "text": "This tutorial is aimed at being a walkthrough of the DADA2 pipeline.\nIt uses the data of the now famous  MiSeq SOP  by the Mothur authors but analyses the data using DADA2.  DADA2 is a relatively new method to analyse amplicon data which uses exact variants instead of OTUs.      The advantages of the DADA2 method is described in the  paper",
            "title": "Metabarcoding"
        },
        {
            "location": "/tutorials/docs/16S/#before-starting",
            "text": "There are two ways to follow this tutorial: you can copy and paste all the codes blocks below in R directly, or you can download this document in the Rmarkdown format and execute the cells.   Link to the document in Rmarkdown",
            "title": "Before Starting"
        },
        {
            "location": "/tutorials/docs/16S/#install-and-load-packages",
            "text": "First install DADA2 and other necessary packages  source('https://bioconductor.org/biocLite.R')\nbiocLite('dada2')\nbiocLite('phyloseq')\nbiocLite('DECIPHER')\ninstall.packages('ggplot2')\ninstall.packages('phangorn')  Now load the packages and verify you have the correct DADA2 version  library(dada2)\nlibrary(ggplot2)\nlibrary(phyloseq)\nlibrary(phangorn)\nlibrary(DECIPHER)\npackageVersion('dada2')",
            "title": "Install and Load Packages"
        },
        {
            "location": "/tutorials/docs/16S/#download-the-data",
            "text": "You will also need to download the data, as well as the SILVA database   Warning  If you are following the tutorial on the website, the following block of commands has to be executed outside of R.\nIf you run this tutorial with the R notebook, you can simply execute the cell block   wget http://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip\nunzip MiSeqSOPData.zip\nrm -r __MACOSX/\ncd MiSeq_SOP\nwget https://zenodo.org/record/824551/files/silva_nr_v128_train_set.fa.gz\nwget https://zenodo.org/record/824551/files/silva_species_assignment_v128.fa.gz\ncd ..  Back in R, check that you have downloaded the data  path <- 'MiSeq_SOP'\nlist.files(path)",
            "title": "Download the Data"
        },
        {
            "location": "/tutorials/docs/16S/#filtering-and-trimming",
            "text": "First we create two lists with the sorted name of the reads: one for the forward reads, one for the reverse reads  raw_forward <- sort(list.files(path, pattern=\"_R1_001.fastq\",\n                               full.names=TRUE))\n\nraw_reverse <- sort(list.files(path, pattern=\"_R2_001.fastq\",\n                               full.names=TRUE))\n\n# we also need the sample names\nsample_names <- sapply(strsplit(basename(raw_forward), \"_\"),\n                       `[`,  # extracts the first element of a subset\n                       1)  then we visualise the quality of our reads  plotQualityProfile(raw_forward[1:2])\nplotQualityProfile(raw_reverse[1:2])   Question  What do you think of the read quality?   The forward reads are good quality (although dropping a bit at the end as usual) while the reverse are way worse.  Based on these profiles, we will truncate the forward reads at position 240 and the reverse reads at position 160 where the quality distribution crashes.   Note  in this tutorial we perform the trimming using DADA2's own functions.\nIf you wish to do it outside of DADA2, you can refer to the  Quality Control tutorial    Dada2 requires us to define the name of our output files  # place filtered files in filtered/ subdirectory\nfiltered_path <- file.path(path, \"filtered\")\n\nfiltered_forward <- file.path(filtered_path,\n                              paste0(sample_names, \"_R1_trimmed.fastq.gz\"))\n\nfiltered_reverse <- file.path(filtered_path,\n                              paste0(sample_names, \"_R2_trimmed.fastq.gz\"))  We\u2019ll use standard filtering parameters:  maxN=0  (DADA22 requires no Ns),  truncQ=2 ,  rm.phix=TRUE  and  maxEE=2 .\nThe maxEE parameter sets the maximum number of \u201cexpected errors\u201d allowed in a read, which  according to the USEARCH authors  is a better filter than simply averaging quality scores.  out <- filterAndTrim(raw_forward, filtered_forward, raw_reverse,\n                     filtered_reverse, truncLen=c(240,160), maxN=0,\n                     maxEE=c(2,2), truncQ=2, rm.phix=TRUE, compress=TRUE,\n                     multithread=TRUE)\nhead(out)",
            "title": "Filtering and Trimming"
        },
        {
            "location": "/tutorials/docs/16S/#learn-the-error-rates",
            "text": "The DADA2 algorithm depends on a parametric error model and every amplicon dataset has a slightly different error rate.\nThe  learnErrors  of Dada2 learns the error model from the data and will help DADA2 to fits its method to your data  errors_forward <- learnErrors(filtered_forward, multithread=TRUE)\nerrors_reverse <- learnErrors(filtered_reverse, multithread=TRUE)  then we visualise the estimated error rates  plotErrors(errors_forward, nominalQ=TRUE) +\n    theme_minimal()   Question  Do you think the error model fits your data correctly?",
            "title": "Learn the Error Rates"
        },
        {
            "location": "/tutorials/docs/16S/#dereplication",
            "text": "From the Dada2 documentation:   Dereplication  combines all identical sequencing reads into into \u201cunique sequences\u201d with a corresponding \u201cabundance\u201d: the number of reads with that unique sequence. Dereplication  substantially reduces computation time by eliminating redundant comparisons.   derep_forward <- derepFastq(filtered_forward, verbose=TRUE)\nderep_reverse <- derepFastq(filtered_reverse, verbose=TRUE)\n# name the derep-class objects by the sample names\nnames(derep_forward) <- sample_names\nnames(derep_reverse) <- sample_names",
            "title": "Dereplication"
        },
        {
            "location": "/tutorials/docs/16S/#sample-inference",
            "text": "We are now ready to apply the core sequence-variant inference algorithm to the dereplicated data.  dada_forward <- dada(derep_forward, err=errors_forward, multithread=TRUE)\ndada_reverse <- dada(derep_reverse, err=errors_reverse, multithread=TRUE)\n\n# inspect the dada-class object\ndada_forward[[1]]  The DADA2 algorithm inferred 128 real sequence variants from the 1979 unique sequences in the first sample.",
            "title": "Sample inference"
        },
        {
            "location": "/tutorials/docs/16S/#merge-paired-end-reads",
            "text": "Now that the reads are trimmed, dereplicated and error-corrected we can merge them together  merged_reads <- mergePairs(dada_forward, derep_forward, dada_reverse,\n                           derep_reverse, verbose=TRUE)\n\n# inspect the merger data.frame from the first sample\nhead(merged_reads[[1]])",
            "title": "Merge Paired-end Reads"
        },
        {
            "location": "/tutorials/docs/16S/#construct-sequence-table",
            "text": "We can now construct a sequence table of our mouse samples, a higher-resolution version of the OTU table produced by traditional methods.  seq_table <- makeSequenceTable(merged_reads)\ndim(seq_table)\n\n# inspect distribution of sequence lengths\ntable(nchar(getSequences(seq_table)))",
            "title": "Construct Sequence Table"
        },
        {
            "location": "/tutorials/docs/16S/#remove-chimeras",
            "text": "The  dada  method used earlier removes substitutions and indel errors but chimeras remain.\nWe remove the chimeras with  seq_table_nochim <- removeBimeraDenovo(seq_table, method='consensus',\n                                       multithread=TRUE, verbose=TRUE)\ndim(seq_table_nochim)\n\n# which percentage of our reads did we keep?\nsum(seq_table_nochim) / sum(seq_table)  As a final check of our progress, we\u2019ll look at the number of reads that made it through each step in the pipeline  get_n <- function(x) sum(getUniques(x))\n\ntrack <- cbind(out, sapply(dada_forward, get_n), sapply(merged_reads, get_n),\n               rowSums(seq_table), rowSums(seq_table_nochim))\n\ncolnames(track) <- c('input', 'filtered', 'denoised', 'merged', 'tabled',\n                     'nonchim')\nrownames(track) <- sample_names\nhead(track)  We kept the majority of our reads!",
            "title": "Remove Chimeras"
        },
        {
            "location": "/tutorials/docs/16S/#assign-taxonomy",
            "text": "Now we assign taxonomy to our sequences using the SILVA database  taxa <- assignTaxonomy(seq_table_nochim,\n                       'MiSeq_SOP/silva_nr_v128_train_set.fa.gz',\n                       multithread=TRUE)\ntaxa <- addSpecies(taxa, 'MiSeq_SOP/silva_species_assignment_v128.fa.gz')  for inspecting the classification  taxa_print <- taxa  # removing sequence rownames for display only\nrownames(taxa_print) <- NULL\nhead(taxa_print)",
            "title": "Assign Taxonomy"
        },
        {
            "location": "/tutorials/docs/16S/#phylogenetic-tree",
            "text": "DADA2 is reference-free so we have to build the tree ourselves  We first align our sequences  sequences <- getSequences(seq_table)\nnames(sequences) <- sequences  # this propagates to the tip labels of the tree\nalignment <- AlignSeqs(DNAStringSet(sequences), anchor=NA)  Then we build a neighbour-joining tree then fit a maximum likelihood tree using the neighbour-joining tree as a starting point  phang_align <- phyDat(as(alignment, 'matrix'), type='DNA')\ndm <- dist.ml(phang_align)\ntreeNJ <- NJ(dm)  # note, tip order != sequence order\nfit = pml(treeNJ, data=phang_align)\n\n## negative edges length changed to 0!\n\nfitGTR <- update(fit, k=4, inv=0.2)\nfitGTR <- optim.pml(fitGTR, model='GTR', optInv=TRUE, optGamma=TRUE,\n                    rearrangement = 'stochastic',\n                    control = pml.control(trace = 0))\ndetach('package:phangorn', unload=TRUE)",
            "title": "Phylogenetic Tree"
        },
        {
            "location": "/tutorials/docs/16S/#phyloseq",
            "text": "First load the metadata  sample_data <- read.table(\n    'https://hadrieng.github.io/tutorials/data/16S_metadata.txt',\n    header=TRUE, row.names=\"sample_name\")  We can now construct a phyloseq object from our output and newly created metadata  physeq <- phyloseq(otu_table(seq_table_nochim, taxa_are_rows=FALSE),\n                   sample_data(sample_data),\n                   tax_table(taxa),\n                   phy_tree(fitGTR$tree))\n# remove mock sample\nphyseq <- prune_samples(sample_names(physeq) != 'Mock', physeq)\nphyseq  Let's look at the alpha diversity of our samples  plot_richness(physeq, x='day', measures=c('Shannon', 'Fisher'), color='when') +\n    theme_minimal()  No obvious differences. Let's look at ordination methods (beta diversity)  We can perform an MDS with euclidean distance (mathematically equivalent to a PCA)  ord <- ordinate(physeq, 'MDS', 'euclidean')\nplot_ordination(physeq, ord, type='samples', color='when',\n                title='PCA of the samples from the MiSeq SOP') +\n    theme_minimal()  now with the Bray-Curtis distance  ord <- ordinate(physeq, 'NMDS', 'bray')\nplot_ordination(physeq, ord, type='samples', color='when',\n                title='PCA of the samples from the MiSeq SOP') +\n    theme_minimal()  There we can see a clear difference between our samples.  Let us take a look a the distribution of the most abundant families  top20 <- names(sort(taxa_sums(physeq), decreasing=TRUE))[1:20]\nphyseq_top20 <- transform_sample_counts(physeq, function(OTU) OTU/sum(OTU))\nphyseq_top20 <- prune_taxa(top20, physeq_top20)\nplot_bar(physeq_top20, x='day', fill='Family') +\n    facet_wrap(~when, scales='free_x') +\n    theme_minimal()  We can place them in a tree  bacteroidetes <- subset_taxa(physeq, Phylum %in% c('Bacteroidetes'))\nplot_tree(bacteroidetes, ladderize='left', size='abundance',\n          color='when', label.tips='Family')",
            "title": "Phyloseq"
        },
        {
            "location": "/tutorials/docs/meta_assembly/",
            "text": "Metagenome assembly and binning\n\n\nIn this tutorial you'll learn how to inspect assemble metagenomic data and retrieve draft genomes from assembled metagenomes\n\n\nWe'll use a mock community of 20 bacteria sequenced using the Illumina HiSeq.\nIn reality the data were simulated using \nInSilicoSeq\n.\n\n\nThe 20 bacteria in the dataset were selected from the \nTara Ocean study\n that recovered 957 distinct Metagenome-assembled-genomes (or MAGs) that were previsouly unknown! (full list on \nfigshare\n )\n\n\nGetting the Data\n\n\nmkdir -p ~/data\ncd ~/data\ncurl -O -J -L https://osf.io/th9z6/download\ncurl -O -J -L https://osf.io/k6vme/download\nchmod -w tara_reads_R*\n\n\n\n\nQuality Control\n\n\nwe'll use FastQC to check the quality of our data, as well as sickle for trimming the bad quality part of the reads.\nIf you need a refresher on how and why to check the quality of sequence data, please check the \nQuality Control and Trimming\n tutorial\n\n\nmkdir -p ~/results\ncd ~/results\nln -s ~/data/tara_reads_* .\nfastqc tara_reads_*.fastq.gz\n\n\n\n\n\n\nQuestion\n\n\nWhat is the average read length? The average quality?\n\n\n\n\n\n\nQuestion\n\n\nCompared to single genome sequencing, what graphs differ?\n\n\n\n\nNow we'll trim the reads using sickle\n\n\nsickle pe -f tara_reads_R1.fastq.gz -r tara_reads_R2.fastq.gz -t sanger \\\n    -o tara_trimmed_R1.fastq -p tara_trimmed_R2.fastq -s /dev/null\n\n\n\n\n\n\nQuestion\n\n\nHow many reads were trimmed?\n\n\n\n\nAssembly\n\n\nMegahit will be used for the assembly.\n\n\nmegahit -1 tara_trimmed_R1.fastq -2 tara_trimmed_R2.fastq -o tara_assembly\n\n\n\n\nthe resulting assenmbly can be found under \ntara_assembly/final.contigs.fa\n.\n\n\n\n\nQuestion\n\n\nHow many contigs does this assembly contain?\n\n\n\n\nBinning\n\n\nFirst we need to map the reads back against the assembly to get coverage information\n\n\nln -s tara_assembly/final.contigs.fa .\nbowtie2-build final.contigs.fa final.contigs\nbowtie2 -x final.contigs -1 tara_reads_R1.fastq.gz -2 tara_reads_R2.fastq.gz | \\\n    samtools view -bS -o tara_to_sort.bam\nsamtools sort tara_to_sort.bam -o tara.bam\nsamtools index tara.bam\n\n\n\n\nthen we run metabat\n\n\nrunMetaBat.sh -m 1500 final.contigs.fa tara.bam\nmv final.contigs.fa.metabat-bins1500 metabat\n\n\n\n\n\n\nQuestion\n\n\nHow many bins did we obtain?\n\n\n\n\nChecking the quality of the bins\n\n\nThe first time you run \ncheckm\n you have to create the database\n\n\nsudo checkm data setRoot ~/.local/data/checkm\n\n\n\n\ncheckm lineage_wf -x fa metabat checkm/\ncheckm bin_qa_plot -x fa checkm metabat plots\n\n\n\n\n\n\nQuestion\n\n\nWhich bins should we keep for downstream analysis?\n\n\n\n\n\n\nNote\n\n\ncheckm can plot a lot of metrics. If you have time, check the manual\nand try to produce different plots\n\n\n\n\n\n\nWarning\n\n\nif checkm fails at the phylogeny step, it is likely that your vm doesn't have enough RAM.\npplacer requires about 35G of RAM to place the bins in the tree of life.\n\n\nIn that case, execute the following\n\n\ncd ~/results\n\n\ncurl -O -J -L https://osf.io/xuzhn/download\n\n\ntar xzf checkm.tar.gz\n\n\ncheckm qa checkm/lineage.ms checkm\n  \n\n\n\n\nthen plot the completeness\n\n\ncheckm bin_qa_plot -x fa checkm metabat plots\n\n\n\n\nand take a look at \nplots/bin_qa_plot.png\n\n\nFurther reading\n\n\n\n\nRecovery of nearly 8,000 metagenome-assembled genomes substantially expands the tree of life\n\n\nThe reconstruction of 2,631 draft metagenome-assembled genomes from the global oceans",
            "title": "Metagenome assembly"
        },
        {
            "location": "/tutorials/docs/meta_assembly/#metagenome-assembly-and-binning",
            "text": "In this tutorial you'll learn how to inspect assemble metagenomic data and retrieve draft genomes from assembled metagenomes  We'll use a mock community of 20 bacteria sequenced using the Illumina HiSeq.\nIn reality the data were simulated using  InSilicoSeq .  The 20 bacteria in the dataset were selected from the  Tara Ocean study  that recovered 957 distinct Metagenome-assembled-genomes (or MAGs) that were previsouly unknown! (full list on  figshare  )",
            "title": "Metagenome assembly and binning"
        },
        {
            "location": "/tutorials/docs/meta_assembly/#getting-the-data",
            "text": "mkdir -p ~/data\ncd ~/data\ncurl -O -J -L https://osf.io/th9z6/download\ncurl -O -J -L https://osf.io/k6vme/download\nchmod -w tara_reads_R*",
            "title": "Getting the Data"
        },
        {
            "location": "/tutorials/docs/meta_assembly/#quality-control",
            "text": "we'll use FastQC to check the quality of our data, as well as sickle for trimming the bad quality part of the reads.\nIf you need a refresher on how and why to check the quality of sequence data, please check the  Quality Control and Trimming  tutorial  mkdir -p ~/results\ncd ~/results\nln -s ~/data/tara_reads_* .\nfastqc tara_reads_*.fastq.gz   Question  What is the average read length? The average quality?    Question  Compared to single genome sequencing, what graphs differ?   Now we'll trim the reads using sickle  sickle pe -f tara_reads_R1.fastq.gz -r tara_reads_R2.fastq.gz -t sanger \\\n    -o tara_trimmed_R1.fastq -p tara_trimmed_R2.fastq -s /dev/null   Question  How many reads were trimmed?",
            "title": "Quality Control"
        },
        {
            "location": "/tutorials/docs/meta_assembly/#assembly",
            "text": "Megahit will be used for the assembly.  megahit -1 tara_trimmed_R1.fastq -2 tara_trimmed_R2.fastq -o tara_assembly  the resulting assenmbly can be found under  tara_assembly/final.contigs.fa .   Question  How many contigs does this assembly contain?",
            "title": "Assembly"
        },
        {
            "location": "/tutorials/docs/meta_assembly/#binning",
            "text": "First we need to map the reads back against the assembly to get coverage information  ln -s tara_assembly/final.contigs.fa .\nbowtie2-build final.contigs.fa final.contigs\nbowtie2 -x final.contigs -1 tara_reads_R1.fastq.gz -2 tara_reads_R2.fastq.gz | \\\n    samtools view -bS -o tara_to_sort.bam\nsamtools sort tara_to_sort.bam -o tara.bam\nsamtools index tara.bam  then we run metabat  runMetaBat.sh -m 1500 final.contigs.fa tara.bam\nmv final.contigs.fa.metabat-bins1500 metabat   Question  How many bins did we obtain?",
            "title": "Binning"
        },
        {
            "location": "/tutorials/docs/meta_assembly/#checking-the-quality-of-the-bins",
            "text": "The first time you run  checkm  you have to create the database  sudo checkm data setRoot ~/.local/data/checkm  checkm lineage_wf -x fa metabat checkm/\ncheckm bin_qa_plot -x fa checkm metabat plots   Question  Which bins should we keep for downstream analysis?    Note  checkm can plot a lot of metrics. If you have time, check the manual\nand try to produce different plots    Warning  if checkm fails at the phylogeny step, it is likely that your vm doesn't have enough RAM.\npplacer requires about 35G of RAM to place the bins in the tree of life.  In that case, execute the following  cd ~/results  curl -O -J -L https://osf.io/xuzhn/download  tar xzf checkm.tar.gz  checkm qa checkm/lineage.ms checkm      then plot the completeness  checkm bin_qa_plot -x fa checkm metabat plots  and take a look at  plots/bin_qa_plot.png",
            "title": "Checking the quality of the bins"
        },
        {
            "location": "/tutorials/docs/meta_assembly/#further-reading",
            "text": "Recovery of nearly 8,000 metagenome-assembled genomes substantially expands the tree of life  The reconstruction of 2,631 draft metagenome-assembled genomes from the global oceans",
            "title": "Further reading"
        },
        {
            "location": "/tutorials/docs/wms/",
            "text": "Whole Metagenome Sequencin\n\n\nTable of Contents\n\n\n\n\nIntroduction\n\n\nThe Pig Microbiome\n\n\nWhole Metagenome Sequencing\n\n\n\n\n\n\nSoftwares Required for this Tutorial\n\n\nGetting the Data and Checking their Quality\n\n\nTaxonomic Classification\n\n\nVisualization\n\n\n\n\nIntroduction\n\n\nMicrobiome used\n\n\nIn this tutorial we will compare samples from the Pig Gut Microbiome to samples from the Human Gut Microbiome. Below you'll find a brief description of the two projects:\n\n\nThe Pig Microbiome:\n\n\n\n\nPig is a main species for livestock and biomedicine. The pig genome sequence was recently reported. To boost research, we established a catalogue of the genes of the gut microbiome based on faecal samples of 287 pigs from France, Denmark and China. More than 7.6 million non-redundant genes representing 719 metagenomic species were identified by deep metagenome sequencing, highlighting more similarities with the human than with the mouse catalogue. The pig and human catalogues share only 12.6 and 9.3 % of their genes, respectively, but 70 and 95% of their functional pathways. The pig gut microbiota is influenced by gender, age and breed. Analysis of the prevalence of antibiotics resistance genes (ARGs) reflected antibiotics supplementation in each farm system, and revealed that non-antibiotics-fed animals still harbour ARGs. The pig catalogue creates a resource for whole metagenomics-based studies, highly valuable for research in biomedicine and for sustainable knowledge-based pig farming\n\n\n\n\nThe Human Microbiome:\n\n\n\n\nWe are facing a global metabolic health crisis provoked by an obesity epidemic. Here we report the human gut microbial composition in a population sample of 123 non-obese and 169 obese Danish individuals. We find two groups of individuals that differ by the number of gut microbial genes and thus gut bacterial richness. They harbour known and previously unknown bacterial species at different proportions; individuals with a low bacterial richness (23% of the population) are characterized by more marked overall adiposity, insulin resistance and dyslipidaemia and a more pronounced inflammatory phenotype when compared with high bacterial richness individuals. The obese individuals among the former also gain more weight over time. Only a few bacterial species are sufficient to distinguish between individuals with high and low bacterial richness, and even between lean and obese. Our classifications based on variation in the gut microbiome identify subsets of individuals in the general white adult population who may be at increased risk of progressing to adiposity-associated co-morbidities\n\n\n\n\nWhole Metagenome Sequencing\n\n\nWhole Metagenome sequencing (WMS), or shotgun metagenome sequencing, is a relatively new and powerful sequencing approach that provides insight into community biodiversity and function. On the contrary of Metabarcoding, where only a specific region of the bacterial community (the 16s rRNA) is sequenced, WMS aims at sequencing all the genomic material present in the environment.\n\n\nThe choice of shotgun or 16S approaches is usually dictated by the nature of the studies being conducted. For instance, 16S is well suited for analysis of large number of samples, i.e., multiple patients, longitudinal studies, etc. but offers limited taxonomical and functional resolution. WMS is generally more expensive but offers increased resolution, and allows the discovery of viruses as well as other mobile genetic elements.\n\n\nSoftwares Required for this Tutorial\n\n\n\n\nFastQC\n\n\nKraken\n\n\nR\n\n\nPavian\n\n\n\n\nPrepare and organise your working directory\n\n\nYou will first login to your virtual machine using the IP provided by the teachers.\nAll the exercise will be performed on your VM in the cloud.\n\n\n\n\nNote\n\n\nWhen you login with the ssh command, please add the option -X at the end of it to be able to use graphical interface\n\n\n\n\nmkdir ~/wms\ncd ~/wms\nmkdir data\nmkdir results\nmkdir scripts\n\n\n\n\nGetting the Data and Checking their Quality\n\n\nAs the data were very big, we have prepared performed a downsampling on all 6 datasets (3 pigs and 3 humans).\nWe will first download and unpack the data.\n\n\ncd ~/wms/data\ncurl -O -J -L https://osf.io/h9x6e/download\ntar xvf subset_wms.tar.gz\ncd sub_100000\n\n\n\n\nWe'll use FastQC to check the quality of our data.\nFastQC should be already installed on your VM, so you need to type\n\n\nfastqc\n\n\n\n\nWhen FastQC has started you can select the fastq file you just downloaded with \nfile -> open\n\nWhat do you think about the quality of the reads? Do they need trimming? Are there still adapters\npresent? Overrepresented sequences?\n\n\n\n\nNote\n\n\nFastQC can be downloaded and run on a Windows or Linux computer without installation.\nIt is available \nhere\n\n\n\n\nAlternatively, run fastqc on the command-line:\n\n\nfastqc *.fastq\n\n\n\n\nIf the quality appears to be good, it's because it was probably the cleaned reads that were deposited into SRA.\nWe can directly move to the classification step.\n\n\nTaxonomic Classification\n\n\nKraken\n is a system for assigning taxonomic labels to short DNA sequences (i.e. reads)\n\nKraken aims to achieve high sensitivity and high speed by utilizing exact alignments of k-mers and a novel classification algorithm (sic).\n\n\nIn short, kraken uses a new approach with exact k-mer matching to assign taxonomy to short reads. It is \nextremely\n fast compared to traditional\napproaches (i.e. BLAST).\n\n\nBy default, the authors of kraken built their database based on RefSeq Bacteria, Archaea and Viruses. We'll use it for the purpose of this tutorial.\nWe will download a shrunk database (minikraken) provided by Kraken developers that is only 4GB.\n\n\n# First we create a databases directory in our home\ncd /mnt\nsudo mkdir databases\ncd databases\n# Then we download the minikraken database\nsudo wget https://ccb.jhu.edu/software/kraken/dl/minikraken_20171019_4GB.tgz\nsudo tar xzf minikraken_20171019_4GB.tgz\nKRAKEN_DB=/mnt/databases/minikraken_20171013_4GB\ncd\n\n\n\n\nNow run kraken on the reads\n\n\n# In the data/ directory\ncd ~/wms/data/sub_100000\nfor i in *_1.fastq\ndo\n    prefix=$(basename $i _1.fastq)\n    # print which sample is being processed\n    echo $prefix\n    kraken --db $KRAKEN_DB --threads 2 --fastq-input \\\n        ${prefix}_1.fastq ${prefix}_2.fastq > /home/student/wms/results/${prefix}.tab\n    kraken-report --db $KRAKEN_DB \\\n        /home/student/wms/results/${prefix}.tab > /home/student/wms/results/${prefix}_tax.txt\ndone\n\n\n\n\nwhich produces a tab-delimited file with an assigned TaxID for each read.\n\n\nKraken includes a script called \nkraken-report\n to transform this file into a \"tree\" view with the percentage of reads assigned to each taxa. We've run this script at each step in the loop. Take a look at the \n_tax.txt\n files!\n\n\nVisualization with Pavian\n\n\nPavian is a web application for exploring metagenomics classification results.\n\n\nFirst, go in Rstudio server by typing the address to your server in your browser:\n\n\nhttp://MY_IP_ADDRESS:8787/\n\n\nwhere you replace \nMY_IP_ADDRESS\n by the IP address of your Virtual Machine.\n\n\n\n\nNote\n\n\nTo access Rstudio server on the virtual machine, you'll need a password.\nAsk your instructor for the password!\n\n\n\n\n\n\nNote\n\n\nIf you wish, you may work on Rstudio on your own laptop if it is powerful enough.\nYou will need an up-to-date version of R, and can install the necessary packages using \nthis script\n\n\n\n\nInstall and run Pavian:\n\n\noptions(repos = c(CRAN = \"http://cran.rstudio.com\"))\nif (!require(remotes)) { install.packages(\"remotes\") }\nremotes::install_github(\"fbreitwieser/pavian\")\npavian::runApp(port=5000)\n\n\n\n\nThen you will explore and compare the results produced by Kraken.",
            "title": "Comparative metagenomics"
        },
        {
            "location": "/tutorials/docs/wms/#whole-metagenome-sequencin",
            "text": "",
            "title": "Whole Metagenome Sequencin"
        },
        {
            "location": "/tutorials/docs/wms/#table-of-contents",
            "text": "Introduction  The Pig Microbiome  Whole Metagenome Sequencing    Softwares Required for this Tutorial  Getting the Data and Checking their Quality  Taxonomic Classification  Visualization",
            "title": "Table of Contents"
        },
        {
            "location": "/tutorials/docs/wms/#introduction",
            "text": "",
            "title": "Introduction"
        },
        {
            "location": "/tutorials/docs/wms/#microbiome-used",
            "text": "In this tutorial we will compare samples from the Pig Gut Microbiome to samples from the Human Gut Microbiome. Below you'll find a brief description of the two projects:  The Pig Microbiome:   Pig is a main species for livestock and biomedicine. The pig genome sequence was recently reported. To boost research, we established a catalogue of the genes of the gut microbiome based on faecal samples of 287 pigs from France, Denmark and China. More than 7.6 million non-redundant genes representing 719 metagenomic species were identified by deep metagenome sequencing, highlighting more similarities with the human than with the mouse catalogue. The pig and human catalogues share only 12.6 and 9.3 % of their genes, respectively, but 70 and 95% of their functional pathways. The pig gut microbiota is influenced by gender, age and breed. Analysis of the prevalence of antibiotics resistance genes (ARGs) reflected antibiotics supplementation in each farm system, and revealed that non-antibiotics-fed animals still harbour ARGs. The pig catalogue creates a resource for whole metagenomics-based studies, highly valuable for research in biomedicine and for sustainable knowledge-based pig farming   The Human Microbiome:   We are facing a global metabolic health crisis provoked by an obesity epidemic. Here we report the human gut microbial composition in a population sample of 123 non-obese and 169 obese Danish individuals. We find two groups of individuals that differ by the number of gut microbial genes and thus gut bacterial richness. They harbour known and previously unknown bacterial species at different proportions; individuals with a low bacterial richness (23% of the population) are characterized by more marked overall adiposity, insulin resistance and dyslipidaemia and a more pronounced inflammatory phenotype when compared with high bacterial richness individuals. The obese individuals among the former also gain more weight over time. Only a few bacterial species are sufficient to distinguish between individuals with high and low bacterial richness, and even between lean and obese. Our classifications based on variation in the gut microbiome identify subsets of individuals in the general white adult population who may be at increased risk of progressing to adiposity-associated co-morbidities",
            "title": "Microbiome used"
        },
        {
            "location": "/tutorials/docs/wms/#whole-metagenome-sequencing",
            "text": "Whole Metagenome sequencing (WMS), or shotgun metagenome sequencing, is a relatively new and powerful sequencing approach that provides insight into community biodiversity and function. On the contrary of Metabarcoding, where only a specific region of the bacterial community (the 16s rRNA) is sequenced, WMS aims at sequencing all the genomic material present in the environment.  The choice of shotgun or 16S approaches is usually dictated by the nature of the studies being conducted. For instance, 16S is well suited for analysis of large number of samples, i.e., multiple patients, longitudinal studies, etc. but offers limited taxonomical and functional resolution. WMS is generally more expensive but offers increased resolution, and allows the discovery of viruses as well as other mobile genetic elements.",
            "title": "Whole Metagenome Sequencing"
        },
        {
            "location": "/tutorials/docs/wms/#softwares-required-for-this-tutorial",
            "text": "FastQC  Kraken  R  Pavian",
            "title": "Softwares Required for this Tutorial"
        },
        {
            "location": "/tutorials/docs/wms/#prepare-and-organise-your-working-directory",
            "text": "You will first login to your virtual machine using the IP provided by the teachers.\nAll the exercise will be performed on your VM in the cloud.   Note  When you login with the ssh command, please add the option -X at the end of it to be able to use graphical interface   mkdir ~/wms\ncd ~/wms\nmkdir data\nmkdir results\nmkdir scripts",
            "title": "Prepare and organise your working directory"
        },
        {
            "location": "/tutorials/docs/wms/#getting-the-data-and-checking-their-quality",
            "text": "As the data were very big, we have prepared performed a downsampling on all 6 datasets (3 pigs and 3 humans).\nWe will first download and unpack the data.  cd ~/wms/data\ncurl -O -J -L https://osf.io/h9x6e/download\ntar xvf subset_wms.tar.gz\ncd sub_100000  We'll use FastQC to check the quality of our data.\nFastQC should be already installed on your VM, so you need to type  fastqc  When FastQC has started you can select the fastq file you just downloaded with  file -> open \nWhat do you think about the quality of the reads? Do they need trimming? Are there still adapters\npresent? Overrepresented sequences?   Note  FastQC can be downloaded and run on a Windows or Linux computer without installation.\nIt is available  here   Alternatively, run fastqc on the command-line:  fastqc *.fastq  If the quality appears to be good, it's because it was probably the cleaned reads that were deposited into SRA.\nWe can directly move to the classification step.",
            "title": "Getting the Data and Checking their Quality"
        },
        {
            "location": "/tutorials/docs/wms/#taxonomic-classification",
            "text": "Kraken  is a system for assigning taxonomic labels to short DNA sequences (i.e. reads) \nKraken aims to achieve high sensitivity and high speed by utilizing exact alignments of k-mers and a novel classification algorithm (sic).  In short, kraken uses a new approach with exact k-mer matching to assign taxonomy to short reads. It is  extremely  fast compared to traditional\napproaches (i.e. BLAST).  By default, the authors of kraken built their database based on RefSeq Bacteria, Archaea and Viruses. We'll use it for the purpose of this tutorial.\nWe will download a shrunk database (minikraken) provided by Kraken developers that is only 4GB.  # First we create a databases directory in our home\ncd /mnt\nsudo mkdir databases\ncd databases\n# Then we download the minikraken database\nsudo wget https://ccb.jhu.edu/software/kraken/dl/minikraken_20171019_4GB.tgz\nsudo tar xzf minikraken_20171019_4GB.tgz\nKRAKEN_DB=/mnt/databases/minikraken_20171013_4GB\ncd  Now run kraken on the reads  # In the data/ directory\ncd ~/wms/data/sub_100000\nfor i in *_1.fastq\ndo\n    prefix=$(basename $i _1.fastq)\n    # print which sample is being processed\n    echo $prefix\n    kraken --db $KRAKEN_DB --threads 2 --fastq-input \\\n        ${prefix}_1.fastq ${prefix}_2.fastq > /home/student/wms/results/${prefix}.tab\n    kraken-report --db $KRAKEN_DB \\\n        /home/student/wms/results/${prefix}.tab > /home/student/wms/results/${prefix}_tax.txt\ndone  which produces a tab-delimited file with an assigned TaxID for each read.  Kraken includes a script called  kraken-report  to transform this file into a \"tree\" view with the percentage of reads assigned to each taxa. We've run this script at each step in the loop. Take a look at the  _tax.txt  files!",
            "title": "Taxonomic Classification"
        },
        {
            "location": "/tutorials/docs/wms/#visualization-with-pavian",
            "text": "Pavian is a web application for exploring metagenomics classification results.  First, go in Rstudio server by typing the address to your server in your browser:  http://MY_IP_ADDRESS:8787/  where you replace  MY_IP_ADDRESS  by the IP address of your Virtual Machine.   Note  To access Rstudio server on the virtual machine, you'll need a password.\nAsk your instructor for the password!    Note  If you wish, you may work on Rstudio on your own laptop if it is powerful enough.\nYou will need an up-to-date version of R, and can install the necessary packages using  this script   Install and run Pavian:  options(repos = c(CRAN = \"http://cran.rstudio.com\"))\nif (!require(remotes)) { install.packages(\"remotes\") }\nremotes::install_github(\"fbreitwieser/pavian\")\npavian::runApp(port=5000)  Then you will explore and compare the results produced by Kraken.",
            "title": "Visualization with Pavian"
        },
        {
            "location": "/nbis_annotation/schedule/",
            "text": "Introduction to genome annotation\n\n\nTeacher:\n  \n\nJacques Dainat, Ph.D. \n\nNBIS (National Bioinformatics Infrastructure Sweden)  \n\nGenome Annotation Service \n\n\nhttp://nbis.se/about/staff/jacques-dainat/\n  \n\n\nhttp://nbis.se\n    \n\n\nSchedule\n\n\nClick the heading of a topic to see the lecture slides or lab instructions.\n\n\n\n\nThursday 1st\n\n\n09.00-09.30:\n \nLecture: Structural annotation programs and pipelines\n \n\n\n10.00-10.15:\n Coffee break  \n\n\n10.15-11.30:\n \nPractical 1: Assembly assessment + Abinitio annotation\n \n\n\n11.30-12.30:\n Lunch \n\n\n12.30-13.00:\n Lecture: Structural annotation programs and pipelines (part 2) \n\n13.00-16.00:\n \nPractical 2: Structural annotation of eukaryote genomes (incl. coffee break)\n\n\n\n\nFriday 2nd\n\n\n09.00-09.15:\n Summary of yesterday\u2019s exercise\n\n\n09.15-09.30:\n  Lecture: Introduction to manual curation\n\n\n10.00-12.00:\n \nPractical 3: Manual curation(incl. coffee break)\n\n\n11.30-12.30:\n Lunch \n\n\n12.30-13.00:\n \nLecture: Functional annotation\n \n\n\n13.00-15.30:\n \nPractical 4: Functional annotation (incl. coffee break)\n\n\n15.30-16.00:\n Wrap-up",
            "title": "Schedule"
        },
        {
            "location": "/nbis_annotation/schedule/#introduction-to-genome-annotation",
            "text": "Teacher:    \nJacques Dainat, Ph.D.  \nNBIS (National Bioinformatics Infrastructure Sweden)   \nGenome Annotation Service   http://nbis.se/about/staff/jacques-dainat/     http://nbis.se",
            "title": "Introduction to genome annotation"
        },
        {
            "location": "/nbis_annotation/schedule/#schedule",
            "text": "Click the heading of a topic to see the lecture slides or lab instructions.   Thursday 1st  09.00-09.30:   Lecture: Structural annotation programs and pipelines    10.00-10.15:  Coffee break    10.15-11.30:   Practical 1: Assembly assessment + Abinitio annotation    11.30-12.30:  Lunch   12.30-13.00:  Lecture: Structural annotation programs and pipelines (part 2)  13.00-16.00:   Practical 2: Structural annotation of eukaryote genomes (incl. coffee break)   Friday 2nd  09.00-09.15:  Summary of yesterday\u2019s exercise  09.15-09.30:   Lecture: Introduction to manual curation  10.00-12.00:   Practical 3: Manual curation(incl. coffee break)  11.30-12.30:  Lunch   12.30-13.00:   Lecture: Functional annotation    13.00-15.30:   Practical 4: Functional annotation (incl. coffee break)  15.30-16.00:  Wrap-up",
            "title": "Schedule"
        },
        {
            "location": "/nbis_annotation/practical_session/practical1/",
            "text": "Foreword:\n\n\nWe will for all exercises use data for the fruit fly, Drosophila melanogaster, as that is one of the currently best annotated organisms and there is plenty of high quality data available. However, working on eukaryotes can be time consuming. Even a small genome like Drosophila would take too long to run within the time we have for this course. Thus to be sure to perform the practicals in good conditions, we will use the smallest chromosome of the drosophila (chromosome 4) like it was a whole genome.\nAn annotation project requires numerous tools and dependencies, which can take easily many days to install for a neophyte. For your convenience and in order to focus on the art of the ANNOTATION most of the tools are already installed on your machine (Thank you Hadrien :) ).\n\n\nFirst of all\n\n\nBefore going into the exercises below you need to connect to your virtual machine Ubuntu 16.04 following the instruction we will provide you.\nOnce connected you will move into the \nannotation_course\n folder, where all the magic will happen.\n\n\ncd ~/annotation_course\n\n\n\n\nNow you need the data !!\n You must download the archive of the data and uncompress it (it could take few minutes).\n\n\nwget https://u-ip-81-109.hpc2n.umu.se/tickets/La34or2kms3wMdf1Gp5HdbsmT4fFCIfayQeHvew8kaU/data.tar.gz/download\ntar xzvf download\nrm download\n\n\n\n\nNow move into the \npractical1\n folder and you are ready to start for this morning !\n\n\ncd ~/annotation_course/practical1\n\n\n\n\n1. Assembly Check\n\n\nBefore starting an annotation project, we need to carefully inspect the assembly to identify potential problems before running expensive computes.\nYou can look at i) the Fragmentation (N50, N90, how many short contigs); ii) the Sanity of the fasta file (Presence of Ns, presence of ambiguous nucleotides, presence of lowercase nucleotides, single line sequences vs multiline sequences); iii) completeness using BUSCO; iv) presence of organelles; v) Others (GC content, How distant the investigated species is from the others annotated species available).\nThe two next exercices will perform some of these checks.\n\n\n1.1 Checking the gene space of your assembly\n\n\nBUSCO provides measures for quantitative assessment of genome assembly, gene set, and transcriptome completeness. Genes that make up the BUSCO sets for each major lineage are selected from orthologous groups with genes present as single-copy orthologs in at least 90% of the species.\n\n\nNote:\n In a real-world scenario, this step should come first and foremost. Indeed, if the result is under your expectation you might be required to enhance your assembly before to go further.\n\n\nExercise 1\n - BUSCO -:\n\n\nYou will run BUSCO on chromosome 4 of Drosophila melanogaster.\n\n\nFirst create a busco folder where you work:\n\n\nmkdir busco\ncd busco\n\n\n\n\nThen visit the \nbusco website\n and choose the best data set among the vast choice. Once you know which one you want to use, right click on it and copy the link to it. Then download the dataset.\n/!\\ Replace \nhttp://busco.ezlab.org/datasets/metazoa_odb9.tar.gz\n link by the link to the dataset you have chosen.\n\n\nwget http://busco.ezlab.org/datasets/metazoa_odb9.tar.gz\ntar xzvf metazoa_odb9.tar.gz\n\n\n\n\nNow you are ready to launch BUSCO on our genome (reminder: the genome is the chormosome 4 called 4.fa).\n\n\nBUSCO.py -i ~/annotation_course/data/genome/4.fa -o 4_dmel_busco -m geno -c 8 -l metazoa_odb9\n\n\n\n\nWhile BUSCO is running, start the exercise 2.\nWhen done, check the short_summary_4_dmel_busco. How many proteins are reported as complete? Does this sound reasonable?\n\nTips\n: the chromosome 4 corresponds to less than 1% of the real size of the genome.\n\n\n1.2 Various Check of your Assembly\n\n\nExercise 2\n :\n\nLaunching the following script will provide you some useful information.\n\n\nfasta_statisticsAndPlot.pl -f ~/annotation_course/data/genome/4.fa\n\n\n\n\nIf you don't see any peculiarities, you can then decide to go forward and start to perform your first wonderful annotation.\n\n\n2. Running an ab initio gene finder\n\n\nNow we are satisfied by the quality of the assembly we can start the annotation.\n\n\nAb initio gene finders:\n These methods have been around for a very long time, and there are many different programs to try. We will in this exercise focus on the gene finder Augustus. These gene finders use likelihoods to find the most likely genes in the genome. They are aware of start and stop codons and splice sites, and will only try to predict genes that follow these rules. The most important factor here is that the gene finder needs to be trained on the organism you are running the program on, otherwise the probabilities for introns, exons, etc. will not be correct. Luckily, these training files are available for Drosophila.\n\n\nExercise 3\n - Augustus:\n\n\nRun Augustus on your genome file using:  \n\n\ncd ~/annotation_course/practical1\nmkdir augustus\ncd augustus\naugustus --species=fly ~/annotation_course/data/genome/4.fa --gff3=yes > augustus_drosophila.gff\n\n\n\n\nif you wish to annotate isoforms too, use the following command:\n\n\naugustus --species=fly ~/annotation_course/data/genome/4.fa --gff3=yes --alternatives-from-sampling=true > augustus_drosophila_isoform.gff\n\n\n\n\nTake a look at the gff result file using the command \u2018less augustus_drosophila.gff\u2019. What kinds of features have been annotated? Does it tell you anything about UTRs?\n\n\nTo better understand what contains your gff file you may use a script that will provide you some statistics like this one:\n\n\ngff3_sp_statistics.pl --gff augustus_drosophila.gff\n\n\n\n\nHow many genes have you annotated ?\n\n\nIt if of interest to view your annotation in a genome browser, this is more concrete and much nicer. A visual inspection is often the most effective way to assess the quality o your annotation.\n\n\nTransfer the augustus_drosophila.gff3 to your computer using scp in a new terminal:   \n\n\nscp -i ~/.ssh/azure_rsa student@__IP__:/home/student/annotation_course/practical1/augustus/augustus_drosophila.gff .\n\n\n\n\nWe have made a genome browser called Webapollo available for you on the address \nhttp://annotation-prod.scilifelab.se:8080/NBIS_course/\n.\nLoad the file in into the genome portal called \ndrosophila_melanogaster_chr4\n. \nHere find the WebApollo instruction\n\n\nThe official Ensembl annotation is available in the genome browser.\nHow does the Augustus annotation compare with the Ensembl annotation? Are they identical?\n\n\nExercise 4 -\n Augustus with yeast models:\n\nRun augustus on the same genome file but using settings for yeast instead (change species to Saccharomyces).\n\n\nLoad this result file into Webapollo and compare with your earlier results. Can you based on this draw any conclusions about how a typical yeast gene differs from a typical Drosophila gene?\n\n\nClosing remarks\n\n\nWe have seen how to assess the quality of the assembly and how to launch a quick annotation using an abinitio tool.\nWe have also seen the importance to use a species specific hmm model into the ab initio tool. Thus, the limitation of this approach is linked to the pre-trained species that are available.",
            "title": "Practical 1"
        },
        {
            "location": "/nbis_annotation/practical_session/practical1/#foreword",
            "text": "We will for all exercises use data for the fruit fly, Drosophila melanogaster, as that is one of the currently best annotated organisms and there is plenty of high quality data available. However, working on eukaryotes can be time consuming. Even a small genome like Drosophila would take too long to run within the time we have for this course. Thus to be sure to perform the practicals in good conditions, we will use the smallest chromosome of the drosophila (chromosome 4) like it was a whole genome.\nAn annotation project requires numerous tools and dependencies, which can take easily many days to install for a neophyte. For your convenience and in order to focus on the art of the ANNOTATION most of the tools are already installed on your machine (Thank you Hadrien :) ).",
            "title": "Foreword:"
        },
        {
            "location": "/nbis_annotation/practical_session/practical1/#first-of-all",
            "text": "Before going into the exercises below you need to connect to your virtual machine Ubuntu 16.04 following the instruction we will provide you.\nOnce connected you will move into the  annotation_course  folder, where all the magic will happen.  cd ~/annotation_course  Now you need the data !!  You must download the archive of the data and uncompress it (it could take few minutes).  wget https://u-ip-81-109.hpc2n.umu.se/tickets/La34or2kms3wMdf1Gp5HdbsmT4fFCIfayQeHvew8kaU/data.tar.gz/download\ntar xzvf download\nrm download  Now move into the  practical1  folder and you are ready to start for this morning !  cd ~/annotation_course/practical1",
            "title": "First of all"
        },
        {
            "location": "/nbis_annotation/practical_session/practical1/#1-assembly-check",
            "text": "Before starting an annotation project, we need to carefully inspect the assembly to identify potential problems before running expensive computes.\nYou can look at i) the Fragmentation (N50, N90, how many short contigs); ii) the Sanity of the fasta file (Presence of Ns, presence of ambiguous nucleotides, presence of lowercase nucleotides, single line sequences vs multiline sequences); iii) completeness using BUSCO; iv) presence of organelles; v) Others (GC content, How distant the investigated species is from the others annotated species available).\nThe two next exercices will perform some of these checks.",
            "title": "1. Assembly Check"
        },
        {
            "location": "/nbis_annotation/practical_session/practical1/#11-checking-the-gene-space-of-your-assembly",
            "text": "BUSCO provides measures for quantitative assessment of genome assembly, gene set, and transcriptome completeness. Genes that make up the BUSCO sets for each major lineage are selected from orthologous groups with genes present as single-copy orthologs in at least 90% of the species.  Note:  In a real-world scenario, this step should come first and foremost. Indeed, if the result is under your expectation you might be required to enhance your assembly before to go further.  Exercise 1  - BUSCO -:  You will run BUSCO on chromosome 4 of Drosophila melanogaster.  First create a busco folder where you work:  mkdir busco\ncd busco  Then visit the  busco website  and choose the best data set among the vast choice. Once you know which one you want to use, right click on it and copy the link to it. Then download the dataset.\n/!\\ Replace  http://busco.ezlab.org/datasets/metazoa_odb9.tar.gz  link by the link to the dataset you have chosen.  wget http://busco.ezlab.org/datasets/metazoa_odb9.tar.gz\ntar xzvf metazoa_odb9.tar.gz  Now you are ready to launch BUSCO on our genome (reminder: the genome is the chormosome 4 called 4.fa).  BUSCO.py -i ~/annotation_course/data/genome/4.fa -o 4_dmel_busco -m geno -c 8 -l metazoa_odb9  While BUSCO is running, start the exercise 2.\nWhen done, check the short_summary_4_dmel_busco. How many proteins are reported as complete? Does this sound reasonable? Tips : the chromosome 4 corresponds to less than 1% of the real size of the genome.",
            "title": "1.1 Checking the gene space of your assembly"
        },
        {
            "location": "/nbis_annotation/practical_session/practical1/#12-various-check-of-your-assembly",
            "text": "Exercise 2  : \nLaunching the following script will provide you some useful information.  fasta_statisticsAndPlot.pl -f ~/annotation_course/data/genome/4.fa  If you don't see any peculiarities, you can then decide to go forward and start to perform your first wonderful annotation.",
            "title": "1.2 Various Check of your Assembly"
        },
        {
            "location": "/nbis_annotation/practical_session/practical1/#2-running-an-ab-initio-gene-finder",
            "text": "Now we are satisfied by the quality of the assembly we can start the annotation.  Ab initio gene finders:  These methods have been around for a very long time, and there are many different programs to try. We will in this exercise focus on the gene finder Augustus. These gene finders use likelihoods to find the most likely genes in the genome. They are aware of start and stop codons and splice sites, and will only try to predict genes that follow these rules. The most important factor here is that the gene finder needs to be trained on the organism you are running the program on, otherwise the probabilities for introns, exons, etc. will not be correct. Luckily, these training files are available for Drosophila.  Exercise 3  - Augustus:  Run Augustus on your genome file using:    cd ~/annotation_course/practical1\nmkdir augustus\ncd augustus\naugustus --species=fly ~/annotation_course/data/genome/4.fa --gff3=yes > augustus_drosophila.gff  if you wish to annotate isoforms too, use the following command:  augustus --species=fly ~/annotation_course/data/genome/4.fa --gff3=yes --alternatives-from-sampling=true > augustus_drosophila_isoform.gff  Take a look at the gff result file using the command \u2018less augustus_drosophila.gff\u2019. What kinds of features have been annotated? Does it tell you anything about UTRs?  To better understand what contains your gff file you may use a script that will provide you some statistics like this one:  gff3_sp_statistics.pl --gff augustus_drosophila.gff  How many genes have you annotated ?  It if of interest to view your annotation in a genome browser, this is more concrete and much nicer. A visual inspection is often the most effective way to assess the quality o your annotation.  Transfer the augustus_drosophila.gff3 to your computer using scp in a new terminal:     scp -i ~/.ssh/azure_rsa student@__IP__:/home/student/annotation_course/practical1/augustus/augustus_drosophila.gff .  We have made a genome browser called Webapollo available for you on the address  http://annotation-prod.scilifelab.se:8080/NBIS_course/ .\nLoad the file in into the genome portal called  drosophila_melanogaster_chr4 .  Here find the WebApollo instruction  The official Ensembl annotation is available in the genome browser.\nHow does the Augustus annotation compare with the Ensembl annotation? Are they identical?  Exercise 4 -  Augustus with yeast models: \nRun augustus on the same genome file but using settings for yeast instead (change species to Saccharomyces).  Load this result file into Webapollo and compare with your earlier results. Can you based on this draw any conclusions about how a typical yeast gene differs from a typical Drosophila gene?",
            "title": "2. Running an ab initio gene finder"
        },
        {
            "location": "/nbis_annotation/practical_session/practical1/#closing-remarks",
            "text": "We have seen how to assess the quality of the assembly and how to launch a quick annotation using an abinitio tool.\nWe have also seen the importance to use a species specific hmm model into the ab initio tool. Thus, the limitation of this approach is linked to the pre-trained species that are available.",
            "title": "Closing remarks"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2/",
            "text": "Gathering evidence data for annotation (Optional)\n\n\nThis exercise is meant to get you acquainted with the type of data you would normally encounter in an annotation project.\n\n\n1. Protein, EST and RNA-seq data (Optional)\n\n\nHere will get an idea of where to download evidence sequences.\n\n\n2. Assembling transcripts based on RNA-seq data (Optional)\n\n\nHere will get an idea how to deal with RNA-seq data\n\n\nClosing remarks\n\n\nNow you know how to obtain evidence data, which will be useful to perform a nice annotation.\n\n\nRunning the Maker gene build pipeline\n\n\nOverview\n\n\nMAKER\n is a computational pipeline to automatically generate annotations from a range of input data - including proteins, ESTs, RNA-seq transcripts and ab-initio gene predictions. During this exercise, you will learn how to use Maker with different forms of input data, and how to judge the quality of the resulting annotations.\n\n\nThe Maker pipeline can work with any combination of the following data sets:\n\n\n\n\n\n\nProteins from the same species or related species  \n\n\n\n\n\n\nProteins from more distantly related organisms (e.g. Uniprot/Swissprot)  \n\n\n\n\n\n\nEST sequences from the same species or very closely related species  \n\n\n\n\n\n\nRNA-seq data from the same or very closely related species - in the form of splice sites or assembled transcripts  \n\n\n\n\n\n\nAb-initio predictions from one or more tools (directly supported are: Augustus, Snap, GeneMark, Fgenesh)  \n\n\n\n\n\n\nAt minimum, most annotation projects will run with a protein data set, possibly complemented by some RNA-seq data. Popular examples of this are most of the traditional model systems, including human. However, a potential shortcoming of such approaches is that the comprehensiveness of the annotation depends directly on the input data. This can become a problem if our genome of interest is taxonomically distant to well-sequenced taxonomic groups so that only few protein matches can be found. Likewise, not all genes will be expressed at all times, making the generation of a comprehensive RNA-seq data set for annotation challenging.\n\n\nWe will therefore first run our annotation project in the traditional way, with proteins and ESTs, and then repeat the process with a well-trained ab-initio gene predictor. You can then compare the output to get an idea of how crucial the use of a gene predictor is. However, before we get our hands dirty, we need to understand Maker a little better...\n\n\nMaker strings together a range of different tools into a complex pipeline (e.g. blast, exonerate, repeatmasker, augustus...), fortunately all its various dependencies have been already installed for you. \nCheck that everything is running smoothly by creating the MAKER config files:\n\n\ncd ~/annotation_course/practical2\nmkdir maker\ncd maker\nmaker -CTL\n\n\n\n\nUnderstanding Makers control files\n\n\nMakers behaviour and information on input data are specified in one of three control files. These are:\n\n\n\n\nmaker_opts.ctl  \n\n\nmaker_bopts.ctl  \n\n\nmaker_exe.ctl\n\n\n\n\nWhat are these files for?\n\n\n'maker_exe.ctl' holds information on the location of the various binaries required by Maker (including Blast, Repeatmasker etc). Normally, all information in this file will be extracted from $PATH, so if everything is set up correctly, you will never have to look into this file.\n\n\nNext, 'maker_bopts.ctl' provides access to a number of settings that control the behaviour of evidence aligners (blast, exonerate). The default settings will usually be fine, but if you want to try to annotate species with greater taxonomic distance to well-sequenced species, it may become necessary to decrease stringency of the e.g. blast alignments.\n\n\nFinally, 'maker_opts.ctl' holds information on the location of input files and some of the parameters controlling the decision making during the gene building.\n\n\nRunning Maker - Drosophila genome\n\n\nWe will annotate the genome of the fruit fly \nDrosophila melanogaster\n. First we will perforn a pure evidence based annotation (without ab-initio predictions) and afterwards with ab-initio.\n\n\n1. Creating an evidence based annotation\n\n\nRunning Maker with only evidence data\n\n\n2. Creating an abinition evidence-driven annotation\n\n\nRunning Maker with ab-initio predictions\n\n\n3. Inspecting the output\n\n\nThe running of an annotation pipeline like Maker is not actually very hard. But the complicated work is only beginning. How to we best inspect the gene builds? Count features? Visualize it? Most importantly, what steps do we need to take to create a 'finished' annotation that we can use for scientific analyses?\n\n\nComparing and evaluating annotations\n\n\nClosing remarks\n\n\nThis concludes the gene building part. We have learned how to use the Maker annotation pipeline and have created gene builds with and without ab-initio predictions. Moreover, we have employed some measures to describe and judge these annotations. An essential part that we decided to leave out is the training of ab-initio gene finders. The reason for this omission was that there isn't really any one best way to do this and your mileage may vary a lot based on your organism and input data. Perhaps the most direct approach available at the moment is a combination of evidence-based annotation with Maker and to use the resulting, crude gene models to train SNAP. Since Maker can improve ab-initio predictions 'on the fly', it can tolerate a bit of noise from a less-than-perfect ab-initio profile. If you are setting out on an annotation project, the NBIS annotation service would be happy to discuss the best approach for your data with you.\n\n\nWith that being said, generating a gene build is only one part of an annotation project. Next, we will inspect the annotation in genome browser and make an attempt at functional inference for the predicted gene models.",
            "title": "Practical 2"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2/#gathering-evidence-data-for-annotation-optional",
            "text": "This exercise is meant to get you acquainted with the type of data you would normally encounter in an annotation project.",
            "title": "Gathering evidence data for annotation (Optional)"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2/#1-protein-est-and-rna-seq-data-optional",
            "text": "Here will get an idea of where to download evidence sequences.",
            "title": "1. Protein, EST and RNA-seq data (Optional)"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2/#2-assembling-transcripts-based-on-rna-seq-data-optional",
            "text": "Here will get an idea how to deal with RNA-seq data",
            "title": "2. Assembling transcripts based on RNA-seq data (Optional)"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2/#closing-remarks",
            "text": "Now you know how to obtain evidence data, which will be useful to perform a nice annotation.",
            "title": "Closing remarks"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2/#running-the-maker-gene-build-pipeline",
            "text": "",
            "title": "Running the Maker gene build pipeline"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2/#overview",
            "text": "MAKER  is a computational pipeline to automatically generate annotations from a range of input data - including proteins, ESTs, RNA-seq transcripts and ab-initio gene predictions. During this exercise, you will learn how to use Maker with different forms of input data, and how to judge the quality of the resulting annotations.  The Maker pipeline can work with any combination of the following data sets:    Proteins from the same species or related species      Proteins from more distantly related organisms (e.g. Uniprot/Swissprot)      EST sequences from the same species or very closely related species      RNA-seq data from the same or very closely related species - in the form of splice sites or assembled transcripts      Ab-initio predictions from one or more tools (directly supported are: Augustus, Snap, GeneMark, Fgenesh)      At minimum, most annotation projects will run with a protein data set, possibly complemented by some RNA-seq data. Popular examples of this are most of the traditional model systems, including human. However, a potential shortcoming of such approaches is that the comprehensiveness of the annotation depends directly on the input data. This can become a problem if our genome of interest is taxonomically distant to well-sequenced taxonomic groups so that only few protein matches can be found. Likewise, not all genes will be expressed at all times, making the generation of a comprehensive RNA-seq data set for annotation challenging.  We will therefore first run our annotation project in the traditional way, with proteins and ESTs, and then repeat the process with a well-trained ab-initio gene predictor. You can then compare the output to get an idea of how crucial the use of a gene predictor is. However, before we get our hands dirty, we need to understand Maker a little better...  Maker strings together a range of different tools into a complex pipeline (e.g. blast, exonerate, repeatmasker, augustus...), fortunately all its various dependencies have been already installed for you. \nCheck that everything is running smoothly by creating the MAKER config files:  cd ~/annotation_course/practical2\nmkdir maker\ncd maker\nmaker -CTL",
            "title": "Overview"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2/#understanding-makers-control-files",
            "text": "Makers behaviour and information on input data are specified in one of three control files. These are:   maker_opts.ctl    maker_bopts.ctl    maker_exe.ctl   What are these files for?  'maker_exe.ctl' holds information on the location of the various binaries required by Maker (including Blast, Repeatmasker etc). Normally, all information in this file will be extracted from $PATH, so if everything is set up correctly, you will never have to look into this file.  Next, 'maker_bopts.ctl' provides access to a number of settings that control the behaviour of evidence aligners (blast, exonerate). The default settings will usually be fine, but if you want to try to annotate species with greater taxonomic distance to well-sequenced species, it may become necessary to decrease stringency of the e.g. blast alignments.  Finally, 'maker_opts.ctl' holds information on the location of input files and some of the parameters controlling the decision making during the gene building.",
            "title": "Understanding Makers control files"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2/#running-maker-drosophila-genome",
            "text": "We will annotate the genome of the fruit fly  Drosophila melanogaster . First we will perforn a pure evidence based annotation (without ab-initio predictions) and afterwards with ab-initio.",
            "title": "Running Maker - Drosophila genome"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2/#1-creating-an-evidence-based-annotation",
            "text": "Running Maker with only evidence data",
            "title": "1. Creating an evidence based annotation"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2/#2-creating-an-abinition-evidence-driven-annotation",
            "text": "Running Maker with ab-initio predictions",
            "title": "2. Creating an abinition evidence-driven annotation"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2/#3-inspecting-the-output",
            "text": "The running of an annotation pipeline like Maker is not actually very hard. But the complicated work is only beginning. How to we best inspect the gene builds? Count features? Visualize it? Most importantly, what steps do we need to take to create a 'finished' annotation that we can use for scientific analyses?  Comparing and evaluating annotations",
            "title": "3. Inspecting the output"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2/#closing-remarks_1",
            "text": "This concludes the gene building part. We have learned how to use the Maker annotation pipeline and have created gene builds with and without ab-initio predictions. Moreover, we have employed some measures to describe and judge these annotations. An essential part that we decided to leave out is the training of ab-initio gene finders. The reason for this omission was that there isn't really any one best way to do this and your mileage may vary a lot based on your organism and input data. Perhaps the most direct approach available at the moment is a combination of evidence-based annotation with Maker and to use the resulting, crude gene models to train SNAP. Since Maker can improve ab-initio predictions 'on the fly', it can tolerate a bit of noise from a less-than-perfect ab-initio profile. If you are setting out on an annotation project, the NBIS annotation service would be happy to discuss the best approach for your data with you.  With that being said, generating a gene build is only one part of an annotation project. Next, we will inspect the annotation in genome browser and make an attempt at functional inference for the predicted gene models.",
            "title": "Closing remarks"
        },
        {
            "location": "/nbis_annotation/practical_session/practical3_manualCuration/",
            "text": "Manual curation\n\n\nOverview\n\n\nIt is easy to understand that automated gene build pipelines will never reach 100% accuracy in their reconstruction. This is due to a number of factors, including ambiguous information from competing input data, inherent uncertainties of ab-initio predictions as well as simplified decision processes when synthesising all available information into a transcript structure. It is therefore always important to manually inspect a gene build - and in basically all cases manual curation is highly recommended.\n\n\nManual curation is a common step in any genome project, often referred to as a jamboree. All researchers involved in the project will meet - virtually or physically - and together inspect the gene build(s) to correct remaining issues prior to publication or downstream analyses. Here we will learn about manual curation tools and best practices, which you can then employ in your own annotation project.\n\n\nMeet: WebApollo\n\n\nYou have already encountered WebApollo in the previous exercise on gene building. There, you used its visualisation capabilities to look at several gene builds and compared them against the evidence alignments. However, what do you do if you find problems with your annotation? Basically, there are two options:\n\n\n\n\nThe problems seem systematic and related to issues with the input data or settings.\n\n\n\n\nIn this case the best is to investigate and eliminate the issue(s) from the raw data and re-run the pipeline. Examples would be poorly assembled RNA-seq data or incompletely or badly sampled protein data. Another issue may be severe problems with the genome assembly. This of course is outside of your annotation task - and a discussion with the assembly team may be necessary.\n\n\n\n\nThe problem is sporadic and looks otherwise non-systematic and complex\n\n\n\n\nComplex, non-systematic errors are harder to rectify by just rerunning the pipeline. The goal of the computational gene build should be to generate a solid basis on which to build future analyses. An error rate of 20% is well within the expected margins and it is important to remember that a computational prediction will always be of lesser quality than a manually curated annotation. A sensible suggestion is to under-shoot rather than over-shoot. In other words, it is often better to be a little more conservative rather than to include as much information as possible. This is controlled by e.g. the way you have compiled your input data and settings within maker.\n\n\nUsing WebApollo to curate gene models\n\n\nManual curation is an integral part of any annotation project. It reveals issues that exist in the gene build and can be used to add further detail - like references to external data sources, or isoforms etc.\n\n\nThe aim of manual curation is to compare a gene model against existing evidence from sources such as ab-initio predictions, protein alignments, RNA-seq as well as related species and fix those parts that are in clear conflict with the evidence. During the course, we will present a few basic features of WebApollo - but there is also a fairly comprehensive handbook available here: \nhttp://icebox.lbl.gov/webapollo/docs/webapollo_user_guide.pdf\n\n\nJamboree\n\n\nFor this exercise, we have set up a specific \nWebapollo\n instance of a drosophila melanogaster annotation of the chromosome 4. It is called \ndrosophila_melanogaster_chr4_jamboree\n.  \n\n\nThe tracks available are:  \n\n\n\n\nAugustus_drosophila : a pure ab initio annotation using Augustus with the drosophila model.\n\n\nMaker_evidence : A maker annotation using Evidence-based approach.\n\n\nMaker_abinitio : A maker annotation using Ab initio evidence-drived approach.  \n\n\nProteins : track of reviewed proteins aligned by Maker. \n\n\ntophat_larva4 : RNAseq data (bam file) aligned to the genome by tophat.  \n\n\nCufflinks_larva4 : A cufflinks transcript assembly aligned by MAKER.\n\n\nStringtie_ERR305399 : A stringtie transcript assembly aligned by MAKER.\n\n\nEST_from_NCBI : The ESTs aligned by maker during the annotation process. \n\n\n\n\nA genomic region of the chrosmosome is assigned to each of you. Your aim is to manualy annotate your assigned part using all the information available in the different tracks. Genomic region has been assigned without any biological consideration. So, if genes straddle two regions don't stop you at the end of yours :).  \n\n\nNOTES: Isoforms are allowed. Start each gene annotation by dragging-and-dropping the gene model that you think be the best. \n\n\n1 :                 50 000 - 140 500\n\n2  :        140 500    - 227 500\n\n3  :              227 500  - 314 500\n\n4  :            314 500 - 401 500\n\n5  :              401 500  - 488 500\n\n6 :             488 500    - 575 500\n\n7 :                575 500 - 662 500\n\n8  :            662 500    - 749 500\n\n9  :              749 500  - 836 500\n\n10 :                836 500    - 923 500\n\n11 :              923 500  - 1 010 500\n\n12 :              1 010 500    - 1 097 500 \n\n13 :                1 097 500 - 1 184 500\n\n14 :                  1 184 500 - 1 268 000\n\n\nThe work you performed was only on small genome portion (1,3 Mbp). That gives you a flavour of the time cost to do a manual curation on a small genome, and an idea of the amount of work needed to manually curate a big genome (>1 Gbp).\n\n\nCheck\n\n\nBefore the end of this practical session we will load the reference annotation of drosophila melanogaster allowing you to check your manual annotation. You should just refresh your web page to display this new track.\n\nDo not be disappointed if your annotation differs a lot from the reference one. Keep in mind that the reference annotation has been curated by experienced experts, and that have used more complete evidence.",
            "title": "Practical 3"
        },
        {
            "location": "/nbis_annotation/practical_session/practical3_manualCuration/#manual-curation",
            "text": "",
            "title": "Manual curation"
        },
        {
            "location": "/nbis_annotation/practical_session/practical3_manualCuration/#overview",
            "text": "It is easy to understand that automated gene build pipelines will never reach 100% accuracy in their reconstruction. This is due to a number of factors, including ambiguous information from competing input data, inherent uncertainties of ab-initio predictions as well as simplified decision processes when synthesising all available information into a transcript structure. It is therefore always important to manually inspect a gene build - and in basically all cases manual curation is highly recommended.  Manual curation is a common step in any genome project, often referred to as a jamboree. All researchers involved in the project will meet - virtually or physically - and together inspect the gene build(s) to correct remaining issues prior to publication or downstream analyses. Here we will learn about manual curation tools and best practices, which you can then employ in your own annotation project.",
            "title": "Overview"
        },
        {
            "location": "/nbis_annotation/practical_session/practical3_manualCuration/#meet-webapollo",
            "text": "You have already encountered WebApollo in the previous exercise on gene building. There, you used its visualisation capabilities to look at several gene builds and compared them against the evidence alignments. However, what do you do if you find problems with your annotation? Basically, there are two options:   The problems seem systematic and related to issues with the input data or settings.   In this case the best is to investigate and eliminate the issue(s) from the raw data and re-run the pipeline. Examples would be poorly assembled RNA-seq data or incompletely or badly sampled protein data. Another issue may be severe problems with the genome assembly. This of course is outside of your annotation task - and a discussion with the assembly team may be necessary.   The problem is sporadic and looks otherwise non-systematic and complex   Complex, non-systematic errors are harder to rectify by just rerunning the pipeline. The goal of the computational gene build should be to generate a solid basis on which to build future analyses. An error rate of 20% is well within the expected margins and it is important to remember that a computational prediction will always be of lesser quality than a manually curated annotation. A sensible suggestion is to under-shoot rather than over-shoot. In other words, it is often better to be a little more conservative rather than to include as much information as possible. This is controlled by e.g. the way you have compiled your input data and settings within maker.",
            "title": "Meet: WebApollo"
        },
        {
            "location": "/nbis_annotation/practical_session/practical3_manualCuration/#using-webapollo-to-curate-gene-models",
            "text": "Manual curation is an integral part of any annotation project. It reveals issues that exist in the gene build and can be used to add further detail - like references to external data sources, or isoforms etc.  The aim of manual curation is to compare a gene model against existing evidence from sources such as ab-initio predictions, protein alignments, RNA-seq as well as related species and fix those parts that are in clear conflict with the evidence. During the course, we will present a few basic features of WebApollo - but there is also a fairly comprehensive handbook available here:  http://icebox.lbl.gov/webapollo/docs/webapollo_user_guide.pdf",
            "title": "Using WebApollo to curate gene models"
        },
        {
            "location": "/nbis_annotation/practical_session/practical3_manualCuration/#jamboree",
            "text": "For this exercise, we have set up a specific  Webapollo  instance of a drosophila melanogaster annotation of the chromosome 4. It is called  drosophila_melanogaster_chr4_jamboree .    The tracks available are:     Augustus_drosophila : a pure ab initio annotation using Augustus with the drosophila model.  Maker_evidence : A maker annotation using Evidence-based approach.  Maker_abinitio : A maker annotation using Ab initio evidence-drived approach.    Proteins : track of reviewed proteins aligned by Maker.   tophat_larva4 : RNAseq data (bam file) aligned to the genome by tophat.    Cufflinks_larva4 : A cufflinks transcript assembly aligned by MAKER.  Stringtie_ERR305399 : A stringtie transcript assembly aligned by MAKER.  EST_from_NCBI : The ESTs aligned by maker during the annotation process.    A genomic region of the chrosmosome is assigned to each of you. Your aim is to manualy annotate your assigned part using all the information available in the different tracks. Genomic region has been assigned without any biological consideration. So, if genes straddle two regions don't stop you at the end of yours :).    NOTES: Isoforms are allowed. Start each gene annotation by dragging-and-dropping the gene model that you think be the best.   1 :                 50 000 - 140 500 2  :        140 500    - 227 500 3  :              227 500  - 314 500 4  :            314 500 - 401 500 5  :              401 500  - 488 500 6 :             488 500    - 575 500 7 :                575 500 - 662 500 8  :            662 500    - 749 500 9  :              749 500  - 836 500 10 :                836 500    - 923 500 11 :              923 500  - 1 010 500 12 :              1 010 500    - 1 097 500  13 :                1 097 500 - 1 184 500 14 :                  1 184 500 - 1 268 000  The work you performed was only on small genome portion (1,3 Mbp). That gives you a flavour of the time cost to do a manual curation on a small genome, and an idea of the amount of work needed to manually curate a big genome (>1 Gbp).",
            "title": "Jamboree"
        },
        {
            "location": "/nbis_annotation/practical_session/practical3_manualCuration/#check",
            "text": "Before the end of this practical session we will load the reference annotation of drosophila melanogaster allowing you to check your manual annotation. You should just refresh your web page to display this new track. \nDo not be disappointed if your annotation differs a lot from the reference one. Keep in mind that the reference annotation has been curated by experienced experts, and that have used more complete evidence.",
            "title": "Check"
        },
        {
            "location": "/nbis_annotation/practical_session/practical4_funcAnnotInterp/",
            "text": "Functional annotation\n\n\nFunctional annotation is the process during which we try to put names to faces - what do genes that we have annotated and curated? Basically all existing approaches accomplish this by means of similarity. If a translation product has strong similarity to a protein that has previously been assigned a function, the function in this newly annotated transcript is probably the same. Of course, this thinking is a bit problematic (where do other functional annotations come from...?) and the method will break down the more distant a newly annotated genome is to existing reference data. A complementary strategy is to scan for more limited similarity - specifically to look for the motifs of functionally characterized protein domains. It doesn't directly tell you what the protein is doing exactly, but it can provide some first indication.\n\n\nIn this exercise we will use an approach that combines the search for full-sequence simliarity by means of 'Blast' against large public databases with more targeted characterization of functional elements through the InterproScan pipeline. Interproscan is a meta-search engine that can compare protein queries against numerous databases. The output from Blast and Interproscan can then be used to add some information to our annotation.\n\n\nPrepare the input data\n\n\nSince we do not wish to spend too much time on this, we will again limit our analysis to chromosome 4. It is also probably best to choose the analysis with ab-initio predictions enabled (unless you found the other build to be more convincing). Maker produces a protein fasta file (called \"annotations.proteins.fa\") together with the annotation and this file should be located in your maker directory.\n\n\nMove in the proper folder:  \n\n\ncd ~/annotation_course/practical4\n\n\n\n\nNow link the annotation you choose to work with. The command will looks like:\n\n\nln -s ~/annotation_course/practical2/maker/maker_with_abinitio/annotationByType/maker.gff maker_with_abinitio.gff  \n\n\n\n\nExtract the corresponding proteins from the gff file:\n\n\nln -s ~/annotation_course/data/genome/4.fa\ngff3_sp_extract_sequences.pl --gff maker_with_abinitio.gff -f 4.fa -p --cfs -o AA.fa\n\n\n\n\nInterproscan approach\n\n\nInterproscan combines a number of searches for conserved motifs and curated data sets of protein clusters etc. This step may take fairly long time. It is recommended to paralellize it for huge amount of data by doing analysis of chunks of tens or hundreds proteins.\n\n\nPerform \nInterproScan\n analysis\n\n\nInterproScan can be run through a website or from the command line on a linux server. Here we are interested in the command line approach.\n\nInterproscan allows to look up pathways, families, domains, sites, repeats, structural domains and other sequence features.\n  \n\n\nLaunch Interproscan with the option -h if you want have a look about all the parameters.\n\n\n\n\nThe '-app' option allows defining the database used. Here we will use the PfamA,ProDom and SuperFamily databases.  \n\n\nInterproscan uses an internal database that related entries in public databases to established GO terms. By running the '-goterms' option, we can add this information to our data set.\n\n\nIf you enable the InterPro lookup ('-iprlookup'), you can also get the InterPro identifier corresponding to each motif retrieved: for example, the same motif is known as PF01623 in Pfam and as IPR002568 in InterPro.\n\n\nThe option '-pa' provides mappings from matches to pathway information (MetaCyc,UniPathway,KEGG,Reactome).\n\n\n\n\ninterproscan.sh -i AA.fa -t p -dp -pa -appl Pfam,ProDom-2006.1,SuperFamily-1.75 --goterms --iprlookup\n\n\n\n\nThe analysis shoud take 2-3 secs per protein request - depending on how many sequences you have submitted, you can make a fairly deducted guess regarding the running time.\n\nYou will obtain 3 result files with the following extension '.gff3', '.tsv' and '.xml'. Explanation of these output are available \n>>here<<\n.\n\n\nload the retrieved functional information in your annotation file:\n\n\nNext, you could write scripts of your own to merge interproscan output into your annotation. Incidentially, Maker comes with utility scripts that can take InterProscan output and add it to a Maker annotation file (you need to load maker).  \n\n\n\n\nipr_update_gff: adds searchable tags to the gene and mRNA features in the GFF3 files.  \n\n\niprscan2gff3: adds physical viewable features for domains that can be displayed in JBrowse, Gbrowse, and Web Apollo.\n\n\n\n\nipr_update_gff maker_with_abinitio.gff AA.fa.tsv >  maker_with_abinitio_with_interpro.gff\n\n\n\n\nWhere a match is found, the new file will now include features called Dbxref and/or Ontology_term in the gene and transcript feature field (9th column).\n\n\nBLAST approach\n\n\nBlast searches provide an indication about potential homology to known proteins.\nA 'full' Blast analysis can run for several days and consume several GB of Ram. Consequently, for a huge amount of data it is recommended to parallelize this step doing analysis of chunks of tens or hundreds proteins. This approach can be used to give a name to the genes and a function to the transcripts.\n\n\nPerform Blast searches from the command line on Uppmax:\n\n\nTo run Blast on your data, use the Ncbi Blast+ package against a Drosophila-specific database (included in the folder we have provided for you, under \nannotation_course/data/blastdb/uniprot_dmel/uniprot_dmel.fa\n) - of course, any other NCBI database would also work:\n\n\nblastp -db ~/annotation_course/data/blastdb/uniprot_dmel/uniprot_dmel.fa -query AA.fa -outfmt 6 -out blast.out -num_threads 8\n\n\n\n\nAgainst the Drosophila-specific database, the blast search takes about 2 secs per protein request - depending on how many sequences you have submitted, you can make a fairly deducted guess regarding the running time.\n\n\nProcess the blast outout with Annie\n\n\nThe Blast outputs must be processed to retrieve the information of the closest protein (best e-value) found by Blast. This work will be done using \nannie\n.  \n\n\nFirst download annie:  \n\n\ngit clone https://github.com/genomeannotation/Annie.git\n\n\n\n\nNow launch annie:\n\n\nAnnie/annie.py -b blast.out -db ~/annotation_course/data/blastdb/uniprot_dmel/uniprot_dmel.fa -g maker_with_abinitio.gff -o annotation_blast.annie\n\n\n\n\nAnnie writes in a 3-column table format file, providing gene name and mRNA product information. The purpose of annie is relatively simple. It recovers the information in the sequence header of the uniprot fasta file, from the best sequence found by Blast (the lowest e-value).\n\n\nload the retrieved information in your annotation file:\n\n\nNow you should be able to use the following script:\n\n\nmaker_gff3manager_JD_v8.pl -f maker_with_abinitio_with_interpro.gff -b annotation_blast.annie --ID FLY -o finalOutputDir  \n\n\n\n\nThat will add the name attribute to the \"gene\" feature and the description attribute (corresponding to the product information) to the \"mRNA\" feature into you annotation file. This script may be used for other purpose like to modify the ID value by something more convenient (i.e FLYG00000001 instead of maker-4-exonerate_protein2genome-gene-8.41).\n\nThe improved annotation is a file named \"codingGeneFeatures.gff\" inside the finalOutputDir.\n\n\nFor displaying the product attribute in Webapollo you can change this attribute by description using this script:\n\n\n/home/student/.local/GAAS/annotation/WebApollo/gff3_webApollo_compliant.pl --gff finalOutputDir/codingGeneFeatures.gff -o final_annotation.gff\n\n\n\n\nVisualise the final annotation\n\n\nTransfer the final_annotation.gff file to your computer using scp in a new terminal:\n\n\nscp -i ~/.ssh/azure_rsa student@\nIP\n:/home/student/annotation_course/practical4/final_annotation.gff .\n\n\nLoad the file in into the genome portal called drosophila_melanogaster_chr4 in the Webapollo genome browser available at the address \nhttp://annotation-prod.scilifelab.se:8080/NBIS_course/\n. \nHere find the WebApollo instruction\n\n\nWondeful ! insn't it ?\n\n\nWhat's next?\n\n\nBecause of Makers' compatibility with GMOD standards, an annotation augmented in one or both of this way can be loaded into e.g. WebApollo and will save annotators a lot of work when e.g. adding meta data to transcript models.\n\n\nSubmission to public repository (creation of an EMBL file)\n\n\nOnce your are satisfied by the wonderful annotation you have done, it would useful important to submit it to a public repostiroy. Fisrt you will be applaused by the community because you share your nice work, secondly this is often mandatory if you wish to publish some work related to this annotation.\n\n\nCurrent state-of-the-art genome annotation tools use the GFF3 format as output, while this format is not accepted as submission format by the International Nucleotide Sequence Database Collaboration (INSDC) databases. Converting the GFF3 format to a format accepted by one of the three INSDC databases is a key step in the achievement of genome annotation projects. However, the flexibility existing in the GFF3 format makes this conversion task difficult to perform.\n\n\nIn order to submit to \nNCBI\n, the use of a tool like \nGAG\n will save you lot time.\n\nIn order to submit to \nEBI\n, the use of a tool like \nEMBLmyGFF3\n will be your best choice.\n\n\nLet's prepare your annotation to submit to ENA (EBI)\n\n\nIn real life, prior to a submission to ENA, you need to create an account and create a project asking a locus_tag for your annotation. You have also to fill lot of metada information related to the assembly and so on. We will skip those tasks using fake information.\nFirst you need to download and install EMBLmyGFF3:\n\n\npip install --user git+https://github.com/NBISweden/EMBLmyGFF3.git\nEMBLmyGFF3 finalOutputDir/codingGeneFeatures.gff 4.fa -o my_annotation_ready_to_submit.embl\n\n\n\n\nYou now have a EMBL flat file ready to submit. In theory to finsish the submission, you will have to send this archived file to their ftp server and finish the submission process in the website side too.\nBut we will not go further. We are done. CONGRATULATION you know most of the secrets needed to understand the annotations on and perform your own !",
            "title": "Practical 4"
        },
        {
            "location": "/nbis_annotation/practical_session/practical4_funcAnnotInterp/#functional-annotation",
            "text": "Functional annotation is the process during which we try to put names to faces - what do genes that we have annotated and curated? Basically all existing approaches accomplish this by means of similarity. If a translation product has strong similarity to a protein that has previously been assigned a function, the function in this newly annotated transcript is probably the same. Of course, this thinking is a bit problematic (where do other functional annotations come from...?) and the method will break down the more distant a newly annotated genome is to existing reference data. A complementary strategy is to scan for more limited similarity - specifically to look for the motifs of functionally characterized protein domains. It doesn't directly tell you what the protein is doing exactly, but it can provide some first indication.  In this exercise we will use an approach that combines the search for full-sequence simliarity by means of 'Blast' against large public databases with more targeted characterization of functional elements through the InterproScan pipeline. Interproscan is a meta-search engine that can compare protein queries against numerous databases. The output from Blast and Interproscan can then be used to add some information to our annotation.",
            "title": "Functional annotation"
        },
        {
            "location": "/nbis_annotation/practical_session/practical4_funcAnnotInterp/#prepare-the-input-data",
            "text": "Since we do not wish to spend too much time on this, we will again limit our analysis to chromosome 4. It is also probably best to choose the analysis with ab-initio predictions enabled (unless you found the other build to be more convincing). Maker produces a protein fasta file (called \"annotations.proteins.fa\") together with the annotation and this file should be located in your maker directory.  Move in the proper folder:    cd ~/annotation_course/practical4  Now link the annotation you choose to work with. The command will looks like:  ln -s ~/annotation_course/practical2/maker/maker_with_abinitio/annotationByType/maker.gff maker_with_abinitio.gff    Extract the corresponding proteins from the gff file:  ln -s ~/annotation_course/data/genome/4.fa\ngff3_sp_extract_sequences.pl --gff maker_with_abinitio.gff -f 4.fa -p --cfs -o AA.fa",
            "title": "Prepare the input data"
        },
        {
            "location": "/nbis_annotation/practical_session/practical4_funcAnnotInterp/#interproscan-approach",
            "text": "Interproscan combines a number of searches for conserved motifs and curated data sets of protein clusters etc. This step may take fairly long time. It is recommended to paralellize it for huge amount of data by doing analysis of chunks of tens or hundreds proteins.",
            "title": "Interproscan approach"
        },
        {
            "location": "/nbis_annotation/practical_session/practical4_funcAnnotInterp/#perform-interproscan-analysis",
            "text": "InterproScan can be run through a website or from the command line on a linux server. Here we are interested in the command line approach. Interproscan allows to look up pathways, families, domains, sites, repeats, structural domains and other sequence features.     Launch Interproscan with the option -h if you want have a look about all the parameters.   The '-app' option allows defining the database used. Here we will use the PfamA,ProDom and SuperFamily databases.    Interproscan uses an internal database that related entries in public databases to established GO terms. By running the '-goterms' option, we can add this information to our data set.  If you enable the InterPro lookup ('-iprlookup'), you can also get the InterPro identifier corresponding to each motif retrieved: for example, the same motif is known as PF01623 in Pfam and as IPR002568 in InterPro.  The option '-pa' provides mappings from matches to pathway information (MetaCyc,UniPathway,KEGG,Reactome).   interproscan.sh -i AA.fa -t p -dp -pa -appl Pfam,ProDom-2006.1,SuperFamily-1.75 --goterms --iprlookup  The analysis shoud take 2-3 secs per protein request - depending on how many sequences you have submitted, you can make a fairly deducted guess regarding the running time. \nYou will obtain 3 result files with the following extension '.gff3', '.tsv' and '.xml'. Explanation of these output are available  >>here<< .",
            "title": "Perform InterproScan analysis"
        },
        {
            "location": "/nbis_annotation/practical_session/practical4_funcAnnotInterp/#load-the-retrieved-functional-information-in-your-annotation-file",
            "text": "Next, you could write scripts of your own to merge interproscan output into your annotation. Incidentially, Maker comes with utility scripts that can take InterProscan output and add it to a Maker annotation file (you need to load maker).     ipr_update_gff: adds searchable tags to the gene and mRNA features in the GFF3 files.    iprscan2gff3: adds physical viewable features for domains that can be displayed in JBrowse, Gbrowse, and Web Apollo.   ipr_update_gff maker_with_abinitio.gff AA.fa.tsv >  maker_with_abinitio_with_interpro.gff  Where a match is found, the new file will now include features called Dbxref and/or Ontology_term in the gene and transcript feature field (9th column).",
            "title": "load the retrieved functional information in your annotation file:"
        },
        {
            "location": "/nbis_annotation/practical_session/practical4_funcAnnotInterp/#blast-approach",
            "text": "Blast searches provide an indication about potential homology to known proteins.\nA 'full' Blast analysis can run for several days and consume several GB of Ram. Consequently, for a huge amount of data it is recommended to parallelize this step doing analysis of chunks of tens or hundreds proteins. This approach can be used to give a name to the genes and a function to the transcripts.",
            "title": "BLAST approach"
        },
        {
            "location": "/nbis_annotation/practical_session/practical4_funcAnnotInterp/#perform-blast-searches-from-the-command-line-on-uppmax",
            "text": "To run Blast on your data, use the Ncbi Blast+ package against a Drosophila-specific database (included in the folder we have provided for you, under  annotation_course/data/blastdb/uniprot_dmel/uniprot_dmel.fa ) - of course, any other NCBI database would also work:  blastp -db ~/annotation_course/data/blastdb/uniprot_dmel/uniprot_dmel.fa -query AA.fa -outfmt 6 -out blast.out -num_threads 8  Against the Drosophila-specific database, the blast search takes about 2 secs per protein request - depending on how many sequences you have submitted, you can make a fairly deducted guess regarding the running time.",
            "title": "Perform Blast searches from the command line on Uppmax:"
        },
        {
            "location": "/nbis_annotation/practical_session/practical4_funcAnnotInterp/#process-the-blast-outout-with-annie",
            "text": "The Blast outputs must be processed to retrieve the information of the closest protein (best e-value) found by Blast. This work will be done using  annie .    First download annie:    git clone https://github.com/genomeannotation/Annie.git  Now launch annie:  Annie/annie.py -b blast.out -db ~/annotation_course/data/blastdb/uniprot_dmel/uniprot_dmel.fa -g maker_with_abinitio.gff -o annotation_blast.annie  Annie writes in a 3-column table format file, providing gene name and mRNA product information. The purpose of annie is relatively simple. It recovers the information in the sequence header of the uniprot fasta file, from the best sequence found by Blast (the lowest e-value).",
            "title": "Process the blast outout with Annie"
        },
        {
            "location": "/nbis_annotation/practical_session/practical4_funcAnnotInterp/#load-the-retrieved-information-in-your-annotation-file",
            "text": "Now you should be able to use the following script:  maker_gff3manager_JD_v8.pl -f maker_with_abinitio_with_interpro.gff -b annotation_blast.annie --ID FLY -o finalOutputDir    That will add the name attribute to the \"gene\" feature and the description attribute (corresponding to the product information) to the \"mRNA\" feature into you annotation file. This script may be used for other purpose like to modify the ID value by something more convenient (i.e FLYG00000001 instead of maker-4-exonerate_protein2genome-gene-8.41). \nThe improved annotation is a file named \"codingGeneFeatures.gff\" inside the finalOutputDir.  For displaying the product attribute in Webapollo you can change this attribute by description using this script:  /home/student/.local/GAAS/annotation/WebApollo/gff3_webApollo_compliant.pl --gff finalOutputDir/codingGeneFeatures.gff -o final_annotation.gff",
            "title": "load the retrieved information in your annotation file:"
        },
        {
            "location": "/nbis_annotation/practical_session/practical4_funcAnnotInterp/#visualise-the-final-annotation",
            "text": "Transfer the final_annotation.gff file to your computer using scp in a new terminal:  scp -i ~/.ssh/azure_rsa student@ IP :/home/student/annotation_course/practical4/final_annotation.gff .  Load the file in into the genome portal called drosophila_melanogaster_chr4 in the Webapollo genome browser available at the address  http://annotation-prod.scilifelab.se:8080/NBIS_course/ .  Here find the WebApollo instruction  Wondeful ! insn't it ?",
            "title": "Visualise the final annotation"
        },
        {
            "location": "/nbis_annotation/practical_session/practical4_funcAnnotInterp/#whats-next",
            "text": "Because of Makers' compatibility with GMOD standards, an annotation augmented in one or both of this way can be loaded into e.g. WebApollo and will save annotators a lot of work when e.g. adding meta data to transcript models.",
            "title": "What's next?"
        },
        {
            "location": "/nbis_annotation/practical_session/practical4_funcAnnotInterp/#submission-to-public-repository-creation-of-an-embl-file",
            "text": "Once your are satisfied by the wonderful annotation you have done, it would useful important to submit it to a public repostiroy. Fisrt you will be applaused by the community because you share your nice work, secondly this is often mandatory if you wish to publish some work related to this annotation.  Current state-of-the-art genome annotation tools use the GFF3 format as output, while this format is not accepted as submission format by the International Nucleotide Sequence Database Collaboration (INSDC) databases. Converting the GFF3 format to a format accepted by one of the three INSDC databases is a key step in the achievement of genome annotation projects. However, the flexibility existing in the GFF3 format makes this conversion task difficult to perform.  In order to submit to  NCBI , the use of a tool like  GAG  will save you lot time. \nIn order to submit to  EBI , the use of a tool like  EMBLmyGFF3  will be your best choice.  Let's prepare your annotation to submit to ENA (EBI)  In real life, prior to a submission to ENA, you need to create an account and create a project asking a locus_tag for your annotation. You have also to fill lot of metada information related to the assembly and so on. We will skip those tasks using fake information.\nFirst you need to download and install EMBLmyGFF3:  pip install --user git+https://github.com/NBISweden/EMBLmyGFF3.git\nEMBLmyGFF3 finalOutputDir/codingGeneFeatures.gff 4.fa -o my_annotation_ready_to_submit.embl  You now have a EMBL flat file ready to submit. In theory to finsish the submission, you will have to send this archived file to their ftp server and finish the submission process in the website side too.\nBut we will not go further. We are done. CONGRATULATION you know most of the secrets needed to understand the annotations on and perform your own !",
            "title": "Submission to public repository (creation of an EMBL file)"
        },
        {
            "location": "/nbis_annotation/practical_session/UsingWebApollo/",
            "text": "Using WebApollo to view annotations\n\n\nFor this course, we have set up a WebApollo installation - as a reminder, the url is \nhttp://annotation-prod.scilifelab.se:8080/NBIS_course\n.\n\n\nLogin information are the following:\n \n\nusername: userX\n  (Where X is the number you have been assigned)\n \n\npassword: demo\n \n \n\n\nWhen logged in to the page, select the proper project corresponding to the exercice (drosophila_melanogaster_chr4 or drosophila_melanogaster_chr4_jamboree).\n\n\nIn the right side of the browser you will see different tabs available. The most useful one for you will be the \"Tracks\" one. In this tab will be display the different tracks available for displaying.\n\n\nIf you wish uploading a(n) track/annotation you have created to the web portal, follow these instructions:\n\n\n\n\nClick on 'File' in the top left corner of the page  \n\n\nSelect 'Open'  \n\n\nClick in 'Select Files' from the 'Local Files' section  \n\n\nSelect the file you wish to upload, and leave all settings at their defaults (you may wish to specify a more informative name for the new track though).\n\n\n\n\nThis should upload your annotation track to the page. However, remember that tracks added in this way are only temporary and will disappear if you log out or lose connection to the server.",
            "title": "WebApollo"
        },
        {
            "location": "/nbis_annotation/practical_session/UsingWebApollo/#using-webapollo-to-view-annotations",
            "text": "For this course, we have set up a WebApollo installation - as a reminder, the url is  http://annotation-prod.scilifelab.se:8080/NBIS_course .  Login information are the following:   username: userX   (Where X is the number you have been assigned)   password: demo      When logged in to the page, select the proper project corresponding to the exercice (drosophila_melanogaster_chr4 or drosophila_melanogaster_chr4_jamboree).  In the right side of the browser you will see different tabs available. The most useful one for you will be the \"Tracks\" one. In this tab will be display the different tracks available for displaying.  If you wish uploading a(n) track/annotation you have created to the web portal, follow these instructions:   Click on 'File' in the top left corner of the page    Select 'Open'    Click in 'Select Files' from the 'Local Files' section    Select the file you wish to upload, and leave all settings at their defaults (you may wish to specify a more informative name for the new track though).   This should upload your annotation track to the page. However, remember that tracks added in this way are only temporary and will disappear if you log out or lose connection to the server.",
            "title": "Using WebApollo to view annotations"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_gatherEvidence/",
            "text": "Obtaining Protein\n\n\nSwissprot:\n Uniprot is an excellent source for high quality protein sequences. The main site can be found at \nhttp://www.uniprot.org\n. This is also the place to find Swissprot, a collection of manually curated non-redundant proteins that cover a wide range of organisms while still being manageable in size.\n\n\nExercise 1\n - Swissprot:\n\nNavigate the Uniprot site to find the download location for Swissprot in fasta-format. You do not need to download the file, just find it. In what way does Swissprot differ from Uniref (another excellent source of proteins, also available at the same site)?\n\n\nUniprot:\n Even with Swissprot available, you also often want to include protein sequences from organisms closely related to your study organism. An approach we often use is to concatenate Swissprot with a few protein fasta-files from closely related organisms and use this in our annotation pipeline.\n\n\nExercise 2\n - Uniprot:\n\nUse Uniprot to find (not download) all protein sequences for all the complete genomes in the family Drosophilidae. How many complete genomes in Drosophilidae do you find?\n\n\nRefseq:\n Refseq is another good place to find non-redundant protein sequences to use in your project. The sequences are to some extent sorted by organismal group, but only to very large and inclusive groups. The best way to download large datasets from refseq is using their ftp-server at \nftp://ftp.ncbi.nlm.nih.gov/refseq/\n.\n\n\nExercise 3\n - Refseq:\n\nNavigate the Refseq ftp site to find the invertebrate collection of protein sequences. You do not need to download the sequences, just find them. The files are mixed with other types of data, which files include the protein sequences?\n\n\nEnsembl:\n The European Ensembl project makes data available for a number of genome projects, in particular vertebrate animals, through their excellent webinterface. This is a good place to find annotations for model organisms as well as download protein sequences and other types of data. They also supply the Biomart interface, which is excellent if you want to download data for a specific region, a specific gene, or create easily parsable file with gene names etc.\n\n\nExercise 4\n - Ensembl Biomart:\n\nGo to Biomart at \nhttp://www.ensembl.org/biomart/martview\n and use it to download all protein sequences for chromosome 4 in Drosophila melanogaster. Once you have downloaded the file, use some command line magic to figure out how many sequences are included in the file. Please ask the teachers if you are having problems here.\n\n\nObtaining EST\n\n\nEST data is not commonly generated anymore, but may become useful for some projects where such data is still available. Examples may include older genomes targeted for re-annotation or genomes with available EST data for closely related species.\n\n\nThe NCBI or EBI websites are the most appropriate places to retrieve such kind of data.\n\n\nExercise 5\n - NCBI:\n\nGo to the NCBI website and find how many ESTs are available for the drosophila melanogaster species.\n\n\nObtaining RNA-seq\n\n\nCommonly, such data are produced within the project you are working on. Otherwise the most appropriate data could be retrieved on the Sequence Read Archive (SRA) website from the NCBI or the European Nucleotide Archive (ENA) from the EBI.",
            "title": "Protein, EST and RNA-seq data"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_gatherEvidence/#obtaining-protein",
            "text": "Swissprot:  Uniprot is an excellent source for high quality protein sequences. The main site can be found at  http://www.uniprot.org . This is also the place to find Swissprot, a collection of manually curated non-redundant proteins that cover a wide range of organisms while still being manageable in size.  Exercise 1  - Swissprot: \nNavigate the Uniprot site to find the download location for Swissprot in fasta-format. You do not need to download the file, just find it. In what way does Swissprot differ from Uniref (another excellent source of proteins, also available at the same site)?  Uniprot:  Even with Swissprot available, you also often want to include protein sequences from organisms closely related to your study organism. An approach we often use is to concatenate Swissprot with a few protein fasta-files from closely related organisms and use this in our annotation pipeline.  Exercise 2  - Uniprot: \nUse Uniprot to find (not download) all protein sequences for all the complete genomes in the family Drosophilidae. How many complete genomes in Drosophilidae do you find?  Refseq:  Refseq is another good place to find non-redundant protein sequences to use in your project. The sequences are to some extent sorted by organismal group, but only to very large and inclusive groups. The best way to download large datasets from refseq is using their ftp-server at  ftp://ftp.ncbi.nlm.nih.gov/refseq/ .  Exercise 3  - Refseq: \nNavigate the Refseq ftp site to find the invertebrate collection of protein sequences. You do not need to download the sequences, just find them. The files are mixed with other types of data, which files include the protein sequences?  Ensembl:  The European Ensembl project makes data available for a number of genome projects, in particular vertebrate animals, through their excellent webinterface. This is a good place to find annotations for model organisms as well as download protein sequences and other types of data. They also supply the Biomart interface, which is excellent if you want to download data for a specific region, a specific gene, or create easily parsable file with gene names etc.  Exercise 4  - Ensembl Biomart: \nGo to Biomart at  http://www.ensembl.org/biomart/martview  and use it to download all protein sequences for chromosome 4 in Drosophila melanogaster. Once you have downloaded the file, use some command line magic to figure out how many sequences are included in the file. Please ask the teachers if you are having problems here.",
            "title": "Obtaining Protein"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_gatherEvidence/#obtaining-est",
            "text": "EST data is not commonly generated anymore, but may become useful for some projects where such data is still available. Examples may include older genomes targeted for re-annotation or genomes with available EST data for closely related species.  The NCBI or EBI websites are the most appropriate places to retrieve such kind of data.  Exercise 5  - NCBI: \nGo to the NCBI website and find how many ESTs are available for the drosophila melanogaster species.",
            "title": "Obtaining EST"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_gatherEvidence/#obtaining-rna-seq",
            "text": "Commonly, such data are produced within the project you are working on. Otherwise the most appropriate data could be retrieved on the Sequence Read Archive (SRA) website from the NCBI or the European Nucleotide Archive (ENA) from the EBI.",
            "title": "Obtaining RNA-seq"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_transcriptome/",
            "text": "Assembling transcripts based on RNA-seq data\n\n\nRna-seq data is in general very useful in annotation projects as the data usually comes from the actual organism you are studying and thus avoids the danger of introducing errors caused by differences in gene structure between your study organism and other species.\n\n\nImportant remarks to remember before starting working with RNA-seq:\n- Check if RNAseq are paired or not. Last generation of sequenced short reads (since 2013) are almost all paired. Anyway, it is important to check that information, which will be useful for the tools used in the next steps.\n- Check if RNAseq are stranded. Indeed this information will be useful for the tools used in the next steps. (In general way we recommend to use stranded RNAseq to avoid transcript fusion during the transcript assembly process. That gives more reliable results. )\n- Left / L / forward / 1 are identical meaning. It is the same for Right / R /Reverse / 2\n\n\nFirst create a dedicated folder to work in:\n\n\ncd ~/annotation_course/practical2\nmkdir RNAseq\ncd RNAseq\n\n\n\n\n1. Genome guided transcriptome assembly:\n\n\nChecking encoding version and fastq quality score format\n\n\nTo check the technology used to sequences the RNAseq and get some extra information we have to use fastqc tool.\n\n\nmkdir fastqc\ncd fastqc\nmkdir fastqc_reports\nfastqc ~/annotation_course/data/RNAseq/fastq/ERR305399.left.fastq.gz -o fastqc_reports/\n\n\n\n\nTransfer the html file resulting of fastqc to your computer using scp in another terminal:   \n\n\nscp -i ~/.ssh/azure_rsa student@__IP__:/home/student/annotation_course/practical1/augustus/augustus_drosophila.gff .\n\n\n\n\nOpen it. What kind of result do you have?\n\n\nChecking the fastq quality score format\n\n\nfastq_guessMyFormat.pl -i ~/annotation_course/data/RNAseq/fastq/ERR305399.left.fastq.gz\n\n\n\n\nIn the normal mode, it differentiates between Sanger/Illumina1.8+ and Solexa/Illumina1.3+/Illumina1.5+.\nIn the advanced mode, it will try to pinpoint exactly which scoring system is used.\n\n\nMore test can be made and should be made on RNA-seq data before doing the assembly, we have not time to do all of them during this course. have a look \nhere\n\n\nTrimmomatic (trimming reads)\n\n\nTrimmomatic\n performs a variety of useful trimming tasks for illumina paired-end and single ended data.The selection of trimming steps and their associated parameters are supplied on the command line.\n\n\nThe following command line will perform the following:\n\n    \u2022 Remove adapters (ILLUMINACLIP:TruSeq3-PE.fa:2:30:10)\n\n    \u2022 Remove leading low quality or N bases (below quality 3) (LEADING:3)\n\n    \u2022 Remove trailing low quality or N bases (below quality 3) (TRAILING:3)\n\n    \u2022 Scan the read with a 4-base wide sliding window, cutting when the average quality per base drops below 15 (SLIDINGWINDOW:4:15)\n\n    \u2022 Drop reads below the 36 bases long (MINLEN:36)  \n\n\ncd ~/annotation_course/practical2/RNAseq\nmkdir trimmomatic\ncd trimmomatic\n\njava -jar trimmomatic-0.32.jar PE -threads 8 ~/annotation_course/data/RNAseq/fastq/ERR305399.left.fastq.gz ~/annotation_course/data/RNAseq/fastq/ERR305399.right.fastq.gz ERR305399.left_paired.fastq.gz ERR305399.left_unpaired.fastq.gz ERR305399.right_paired.fastq.gz ERR305399.right_unpaired.fastq.gz ILLUMINACLIP:trimmomatic/0.32/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\n\n\n\n\nTophat (splice-aware mapping reads to genome)\n\n\nOnce the reads have been trimmed, we use \ntophat\n to align the RNA-seq reads to a genome in order to identify exon-exon splice junctions. It is built on the ultrafast short read mapping program \nBowtie\n.\n\n\ncd ~/annotation_course/practical2/RNAseq\nmkdir tophat\ncd tophat\n\ntophat --library-type=fr-firststrand ~/annotation_course/data/genome/4.fa ../trimmomatic/ERR305399.left_paired.fastq.gz ../trimmomatic/ERR305399.right_paired.fastq.gz -p 8\n\n\n\n\nThis step will take a really long time so you can use the bam file located here ~/annotation_course/data/RNAseq/tophat/accepted_hits.bam\n\n\nStringtie (Assembling reads into transcripts)\n\n\nStringTie is a fast and highly efficient assembler of RNA-Seq alignments into potential transcripts. It uses a novel network flow algorithm as well as an optional de novo assembly step to assemble and quantitate full-length transcripts representing multiple splice variants for each gene locus. Its input can include not only the alignments of raw reads used by other transcript assemblers, but also alignments longer sequences that have been assembled from those reads.\n\n\ncd ~/annotation_course/practical2/RNAseq\nmkdir stringtie\ncd stringtie\n\nstringtie ../tophat/tophat_out/accepted_hits.bam -o outdir/transcripts.gtf\n\n\n\n\nWhen done you can find your results in the directory \u2018outdir\u2019. The file transcripts.gtf includes your assembled transcripts.\nAs Webapollo doesn't like the gtf format file you should convert it in gff3 format.\n\n\ngxf_to_gff3.pl --gff transcripts.gtf -o transcripts.gff3\n\n\n\n\nThen, transfer the gff3 file to your computer and load it into \nWebapollo\n. How well does it compare with your Augustus results? Looking at your results, are you happy with the default values of Stringtie (which we used in this exercise) or is there something you would like to change?\n\n\n2. De-novo transcriptome assembly:\n\n\nTrinity\n\n\nTrinity assemblies can be used as complementary evidence, particularly when trying to polish a gene build with Pasa. Before you start, check how big the raw read data is that you wish to assemble to avoid unreasonably long run times.\n\n\ncd ~/annotation_course/practical2/RNAseq\nmkdir trinity\ncd trinity\n\nTrinity --seqType fq --max_memory 64G --left ~/annotation_course/data/RNAseq/ERR305399.left.fastq.gz --right ~/annotation_course/data/RNAseq/ERR305399.right.fastq.gz --CPU 8 --output trinity_result --SS_lib_type RF \n\n\n\n\nTrinity takes a long time to run if you want to have a look at the results, look in ~/annotation_course/course_material/data/dmel/chromosome_4/RNAseq/ the output that will be used later on for the annotation will be Trinity.fasta\n\n\nClosing remarks\n\n\nYou have now successfully perform transcript assemblies. You have seen how to perform a genome-guided assembly as well as de-no assembly.",
            "title": "Assembling transcripts"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_transcriptome/#assembling-transcripts-based-on-rna-seq-data",
            "text": "Rna-seq data is in general very useful in annotation projects as the data usually comes from the actual organism you are studying and thus avoids the danger of introducing errors caused by differences in gene structure between your study organism and other species.  Important remarks to remember before starting working with RNA-seq:\n- Check if RNAseq are paired or not. Last generation of sequenced short reads (since 2013) are almost all paired. Anyway, it is important to check that information, which will be useful for the tools used in the next steps.\n- Check if RNAseq are stranded. Indeed this information will be useful for the tools used in the next steps. (In general way we recommend to use stranded RNAseq to avoid transcript fusion during the transcript assembly process. That gives more reliable results. )\n- Left / L / forward / 1 are identical meaning. It is the same for Right / R /Reverse / 2  First create a dedicated folder to work in:  cd ~/annotation_course/practical2\nmkdir RNAseq\ncd RNAseq",
            "title": "Assembling transcripts based on RNA-seq data"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_transcriptome/#1-genome-guided-transcriptome-assembly",
            "text": "",
            "title": "1. Genome guided transcriptome assembly:"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_transcriptome/#checking-encoding-version-and-fastq-quality-score-format",
            "text": "To check the technology used to sequences the RNAseq and get some extra information we have to use fastqc tool.  mkdir fastqc\ncd fastqc\nmkdir fastqc_reports\nfastqc ~/annotation_course/data/RNAseq/fastq/ERR305399.left.fastq.gz -o fastqc_reports/  Transfer the html file resulting of fastqc to your computer using scp in another terminal:     scp -i ~/.ssh/azure_rsa student@__IP__:/home/student/annotation_course/practical1/augustus/augustus_drosophila.gff .  Open it. What kind of result do you have?  Checking the fastq quality score format  fastq_guessMyFormat.pl -i ~/annotation_course/data/RNAseq/fastq/ERR305399.left.fastq.gz  In the normal mode, it differentiates between Sanger/Illumina1.8+ and Solexa/Illumina1.3+/Illumina1.5+.\nIn the advanced mode, it will try to pinpoint exactly which scoring system is used.  More test can be made and should be made on RNA-seq data before doing the assembly, we have not time to do all of them during this course. have a look  here",
            "title": "Checking encoding version and fastq quality score format"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_transcriptome/#trimmomatic-trimming-reads",
            "text": "Trimmomatic  performs a variety of useful trimming tasks for illumina paired-end and single ended data.The selection of trimming steps and their associated parameters are supplied on the command line.  The following command line will perform the following: \n    \u2022 Remove adapters (ILLUMINACLIP:TruSeq3-PE.fa:2:30:10) \n    \u2022 Remove leading low quality or N bases (below quality 3) (LEADING:3) \n    \u2022 Remove trailing low quality or N bases (below quality 3) (TRAILING:3) \n    \u2022 Scan the read with a 4-base wide sliding window, cutting when the average quality per base drops below 15 (SLIDINGWINDOW:4:15) \n    \u2022 Drop reads below the 36 bases long (MINLEN:36)    cd ~/annotation_course/practical2/RNAseq\nmkdir trimmomatic\ncd trimmomatic\n\njava -jar trimmomatic-0.32.jar PE -threads 8 ~/annotation_course/data/RNAseq/fastq/ERR305399.left.fastq.gz ~/annotation_course/data/RNAseq/fastq/ERR305399.right.fastq.gz ERR305399.left_paired.fastq.gz ERR305399.left_unpaired.fastq.gz ERR305399.right_paired.fastq.gz ERR305399.right_unpaired.fastq.gz ILLUMINACLIP:trimmomatic/0.32/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36",
            "title": "Trimmomatic (trimming reads)"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_transcriptome/#tophat-splice-aware-mapping-reads-to-genome",
            "text": "Once the reads have been trimmed, we use  tophat  to align the RNA-seq reads to a genome in order to identify exon-exon splice junctions. It is built on the ultrafast short read mapping program  Bowtie .  cd ~/annotation_course/practical2/RNAseq\nmkdir tophat\ncd tophat\n\ntophat --library-type=fr-firststrand ~/annotation_course/data/genome/4.fa ../trimmomatic/ERR305399.left_paired.fastq.gz ../trimmomatic/ERR305399.right_paired.fastq.gz -p 8  This step will take a really long time so you can use the bam file located here ~/annotation_course/data/RNAseq/tophat/accepted_hits.bam",
            "title": "Tophat (splice-aware mapping reads to genome)"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_transcriptome/#stringtie-assembling-reads-into-transcripts",
            "text": "StringTie is a fast and highly efficient assembler of RNA-Seq alignments into potential transcripts. It uses a novel network flow algorithm as well as an optional de novo assembly step to assemble and quantitate full-length transcripts representing multiple splice variants for each gene locus. Its input can include not only the alignments of raw reads used by other transcript assemblers, but also alignments longer sequences that have been assembled from those reads.  cd ~/annotation_course/practical2/RNAseq\nmkdir stringtie\ncd stringtie\n\nstringtie ../tophat/tophat_out/accepted_hits.bam -o outdir/transcripts.gtf  When done you can find your results in the directory \u2018outdir\u2019. The file transcripts.gtf includes your assembled transcripts.\nAs Webapollo doesn't like the gtf format file you should convert it in gff3 format.  gxf_to_gff3.pl --gff transcripts.gtf -o transcripts.gff3  Then, transfer the gff3 file to your computer and load it into  Webapollo . How well does it compare with your Augustus results? Looking at your results, are you happy with the default values of Stringtie (which we used in this exercise) or is there something you would like to change?",
            "title": "Stringtie (Assembling reads into transcripts)"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_transcriptome/#2-de-novo-transcriptome-assembly",
            "text": "",
            "title": "2. De-novo transcriptome assembly:"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_transcriptome/#trinity",
            "text": "Trinity assemblies can be used as complementary evidence, particularly when trying to polish a gene build with Pasa. Before you start, check how big the raw read data is that you wish to assemble to avoid unreasonably long run times.  cd ~/annotation_course/practical2/RNAseq\nmkdir trinity\ncd trinity\n\nTrinity --seqType fq --max_memory 64G --left ~/annotation_course/data/RNAseq/ERR305399.left.fastq.gz --right ~/annotation_course/data/RNAseq/ERR305399.right.fastq.gz --CPU 8 --output trinity_result --SS_lib_type RF   Trinity takes a long time to run if you want to have a look at the results, look in ~/annotation_course/course_material/data/dmel/chromosome_4/RNAseq/ the output that will be used later on for the annotation will be Trinity.fasta",
            "title": "Trinity"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_transcriptome/#closing-remarks",
            "text": "You have now successfully perform transcript assemblies. You have seen how to perform a genome-guided assembly as well as de-no assembly.",
            "title": "Closing remarks"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerNoAbinit/",
            "text": "Making an evidence based annotation with MAKER\n\n\nOverview\n\n\nThe first run of Maker will be done without ab-initio predictions. What are your expectations for the resulting gene build? In essence, we are attempting a purely evidence-based annotation, where the best protein- and EST-alignments are chosen to build the most likely gene models. The purpose of an evidence-based annotation is simple. Basically, you may try to annotate an organism where no usable ab-initio model is available. The evidence-based annotation can then be used to create a set of genes on which a new model could be trained on (using e.g. Snap or Augustus). Selection of genes for training can be based on the annotation edit distance (AED score), which says something about how great the distance between a gene model and the evidence alignments is. A score of 0.0 would essentially say that the final model is in perfect agreement with the evidence.\n\n\nLet's do this step-by-step:\n\n\nPrepare the input data\n\n\nLink the raw computes you want to use into your folder. The files you will need are:\n\n\n\n\nthe gff file of the pre-computed repeats (coordinates of repeatmasked regions)\n\n\n\n\nln -s ~/annotation_course/data/raw_computes/repeatmasker.chr4.gff\nln -s ~/annotation_course/data/raw_computes/repeatrunner.chr4.gff\n\n\n\n\nIn addition, you will also need the genome sequence.\n\n\nln -s ~/annotation_course/data/genome/4.fa\n\n\n\n\nThen you will also need EST and protein fasta file:  \n\n\nln -s ~/annotation_course/data/evidence/est.chr4.fa \nln -s ~/annotation_course/data/evidence/proteins.chr4.fa\n\n\n\n\nTo finish you will could use a transcriptome assembly (This one has been made using Stringtie):\n\n\nln -s ~/annotation_course/data/RNAseq/stringtie/stringtie2genome.chr4.gff\n\n\n\n\n/!\\ Always check that the gff files you provides as protein or EST contains   match / match_part (gff alignment type ) feature types rather than genes/transcripts (gff annotation type) otherwise MAKER will not use the contained data properly. Here we have to fix the stringtie gff file.\n\n\ngff3_sp_alignment_output_style.pl --gff stringtie2genome.chr4.gff -o stringtie2genome.chr4.ok.gff\n\n\n\n\nYou should now have 1 repeat file, 1 EST file, 1 protein file, 1 transcript file, and the genome sequence in the working directory. \n\n\nFor Maker to use this information, we need create the three config files, typing this command:\n\n\nmaker -CTL\n\n\n\n\nYou can leave the two files controlling external software behaviors untouched. In the actual maker options file called \nmaker_opts.ctl\n, we need to provide:\n\n\n\n\nname of the genome sequence (genome=)\n\n\nname of the 'EST' file in fasta format  (est=)\n\n\nname of the 'Transcript' file in gff format (est_gff=)\n\n\nname of the 'Protein' set file(s) (protein=)\n\n\nname of the repeatmasker and repeatrunner files (rm_gff=) \n\n\n\n\nYou can list multiple files in one field by separating their names by a \ncomma\n ','.\n\n\nThis time, we do not specify a reference species to be used by augustus, which will disable ab-initio gene finding. Instead we set:\n\n\nprotein2genome=1\n\n  \nest2genome=1\n\n\nThis will enable gene building directly from the evidence alignments.\n\n\nTo edit the \nmaker_opts.ctl\n file you can use the nano text editor:\n\n\nnano maker_opts.ctl\n\n\n\n\nBefore running MAKER you can check you have modified the maker_opts.ctl file properly \nhere\n.\n\n/!\\ Be sure to have deactivated the parameters \nmodel_org= #\n and \nrepeat_protein= #\n to avoid the heavy work of repeatmasker.\n\n\nRun Maker\n\n\nIf your maker_opts.ctl is configured correctly, you should be able to run maker:\n\n\nmpiexec -n 8 maker\n\n\n\n\nThis will start Maker on 8 cores, if everything is configured correctly.\nThis will take a little while and process a lot of output to the screen. Luckily, much of the heavy work - such as repeat masking - are already done, so the total running time is quite manageable, even on a small number of cores.\n\n\nInspect the output (optional)\n\n\nHere you can find details about the MAKER output.\n\n\nCompile the output\n\n\nOnce Maker is finished, compile the annotation:\n\n\nmaker_merge_outputs_from_datastore.pl --output maker_no_abinitio\n\n\n\n\nWe have specified a name for the output directory since we will be creating more than one annotation and need to be able to tell them apart.  \n\n\nThis should create a \"maker_no_abinitio\" directory containing a maker annotation file together with the matching protein predictions file and a sub-directory containing different annotation files including the \nmaker.gff\n which is the result to keep from this analysis. \n\n\n=> You could sym-link the maker.gff file to another folder called e.g. dmel_results, so everything is in the same place in the end. Just make sure to call the link something other than maker.gff, since any maker output will be called that.\n\n\nInspect the gene models\n\n\nTo get some statistics of your annotation you could launch :\n\n\ngff3_sp_statistics.pl --gff maker_no_abinitio/annotationByType/maker.gff\n\n\n\n\nWe could now also visualise the annotation in the Webapollo genome browser.",
            "title": "Evidence Based Annotation"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerNoAbinit/#making-an-evidence-based-annotation-with-maker",
            "text": "",
            "title": "Making an evidence based annotation with MAKER"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerNoAbinit/#overview",
            "text": "The first run of Maker will be done without ab-initio predictions. What are your expectations for the resulting gene build? In essence, we are attempting a purely evidence-based annotation, where the best protein- and EST-alignments are chosen to build the most likely gene models. The purpose of an evidence-based annotation is simple. Basically, you may try to annotate an organism where no usable ab-initio model is available. The evidence-based annotation can then be used to create a set of genes on which a new model could be trained on (using e.g. Snap or Augustus). Selection of genes for training can be based on the annotation edit distance (AED score), which says something about how great the distance between a gene model and the evidence alignments is. A score of 0.0 would essentially say that the final model is in perfect agreement with the evidence.  Let's do this step-by-step:",
            "title": "Overview"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerNoAbinit/#prepare-the-input-data",
            "text": "Link the raw computes you want to use into your folder. The files you will need are:   the gff file of the pre-computed repeats (coordinates of repeatmasked regions)   ln -s ~/annotation_course/data/raw_computes/repeatmasker.chr4.gff\nln -s ~/annotation_course/data/raw_computes/repeatrunner.chr4.gff  In addition, you will also need the genome sequence.  ln -s ~/annotation_course/data/genome/4.fa  Then you will also need EST and protein fasta file:    ln -s ~/annotation_course/data/evidence/est.chr4.fa \nln -s ~/annotation_course/data/evidence/proteins.chr4.fa  To finish you will could use a transcriptome assembly (This one has been made using Stringtie):  ln -s ~/annotation_course/data/RNAseq/stringtie/stringtie2genome.chr4.gff  /!\\ Always check that the gff files you provides as protein or EST contains   match / match_part (gff alignment type ) feature types rather than genes/transcripts (gff annotation type) otherwise MAKER will not use the contained data properly. Here we have to fix the stringtie gff file.  gff3_sp_alignment_output_style.pl --gff stringtie2genome.chr4.gff -o stringtie2genome.chr4.ok.gff  You should now have 1 repeat file, 1 EST file, 1 protein file, 1 transcript file, and the genome sequence in the working directory.   For Maker to use this information, we need create the three config files, typing this command:  maker -CTL  You can leave the two files controlling external software behaviors untouched. In the actual maker options file called  maker_opts.ctl , we need to provide:   name of the genome sequence (genome=)  name of the 'EST' file in fasta format  (est=)  name of the 'Transcript' file in gff format (est_gff=)  name of the 'Protein' set file(s) (protein=)  name of the repeatmasker and repeatrunner files (rm_gff=)    You can list multiple files in one field by separating their names by a  comma  ','.  This time, we do not specify a reference species to be used by augustus, which will disable ab-initio gene finding. Instead we set:  protein2genome=1 \n   est2genome=1  This will enable gene building directly from the evidence alignments.  To edit the  maker_opts.ctl  file you can use the nano text editor:  nano maker_opts.ctl  Before running MAKER you can check you have modified the maker_opts.ctl file properly  here . \n/!\\ Be sure to have deactivated the parameters  model_org= #  and  repeat_protein= #  to avoid the heavy work of repeatmasker.",
            "title": "Prepare the input data"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerNoAbinit/#run-maker",
            "text": "If your maker_opts.ctl is configured correctly, you should be able to run maker:  mpiexec -n 8 maker  This will start Maker on 8 cores, if everything is configured correctly.\nThis will take a little while and process a lot of output to the screen. Luckily, much of the heavy work - such as repeat masking - are already done, so the total running time is quite manageable, even on a small number of cores.",
            "title": "Run Maker"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerNoAbinit/#inspect-the-output-optional",
            "text": "Here you can find details about the MAKER output.",
            "title": "Inspect the output (optional)"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerNoAbinit/#compile-the-output",
            "text": "Once Maker is finished, compile the annotation:  maker_merge_outputs_from_datastore.pl --output maker_no_abinitio  We have specified a name for the output directory since we will be creating more than one annotation and need to be able to tell them apart.    This should create a \"maker_no_abinitio\" directory containing a maker annotation file together with the matching protein predictions file and a sub-directory containing different annotation files including the  maker.gff  which is the result to keep from this analysis.   => You could sym-link the maker.gff file to another folder called e.g. dmel_results, so everything is in the same place in the end. Just make sure to call the link something other than maker.gff, since any maker output will be called that.",
            "title": "Compile the output"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerNoAbinit/#inspect-the-gene-models",
            "text": "To get some statistics of your annotation you could launch :  gff3_sp_statistics.pl --gff maker_no_abinitio/annotationByType/maker.gff  We could now also visualise the annotation in the Webapollo genome browser.",
            "title": "Inspect the gene models"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerAbinit/",
            "text": "Making an abinitio evidence-driven annotation with MAKER\n\n\nThe recommended way of running Maker is in combination with one or more ab-initio profile models. Maker natively supports input from several tools, including augustus, snap and genemark. The choice of tool depends a bit on the organism that you are annotating - for example, GeneMark-ES is mostly recommended for fungi, whereas augustus and snap have a more general use.\n\n\nThe biggest problem with ab-initio models is the process of training them. It is usually recommended to have somewhere around 500-1000 curated gene models for this purpose. Naturally, this is a bit of a contradiction for a not-yet annotated genome.\n\n\nHowever, if one or more good ab-initio profiles are available, they can potentially greatly enhance the quality of an annotation by filling in the blanks left by missing evidence. Interestingly, Maker even works with ab-initio profiles from somewhat distantly related species since it can create so-called hints from the evidence alignments, which the gene predictor can take into account to fine-tune the predictions.\n\n\nUsually when no close ab-initio profile exists for the investigated species, we use the first round of annotation (evidence based) to create one. We first filter the best gene models from this annotation, which are used then to train the abinitio tools of our choice.\n\n\nIn order to compare the performance of Maker with and without ab-initio predictions in a real-world scenario, we have first run a gene build without ab-initio predictions. Now, we run a similar analysis but enable ab-initio predictions through augustus.\n\n\nPrepare the input data\n\n\nNo need to re-compute the mapping/alignment of the different lines of evidence. Indeed, this time consuming task has already been performed during the previous round of annotation (evidence based). So, we will use the corresponding gff files previously produced by MAKER.\n\n\nLink the gff files you want to use into your folder:\n\n\n\n\nrepeatmasker.chr4.gff (already present)\n\n\nrepeatrunner.chr4.gff (already present)\n\n\n4.fa (already present)\n\n\nest_gff_stringtie.gff (transcript that have been mapped by MAKER during the evidence based round of annotation) \n\n\nest2genome.gff \n\n\nprotein2genome.gff \n\n\n\n\nln -s maker_no_abinitio/annotationByType/est_gff:stringtie.gff est_gff_stringtie.gff\nln -s maker_no_abinitio/annotationByType/est2genome.gff \nln -s maker_no_abinitio/annotationByType/protein2genome.gff\n\n\n\n\nThis time, we do specify a reference species to be used by augustus, which will enable ab-initio gene finding and keep_preds=1 will also show abinitio prediction not supported by any evidences :\n\n\naugustus_species=fly\n #Augustus gene prediction species model  (this is where you can call the database you trained for augustus)\n...\n\n\nprotein2genome=0\n\n\nest2genome=0\n\n\nkeep_preds=1\n\n\nWith these settings, Maker will run augustus to predict gene loci, but inform these predictions with information from the protein and est alignments.\n\n\nBefore running MAKER you can check you have modified the maker_opts.ctl file properly \nhere\n.\n\n\nRun Maker with ab-initio predictions\n\n\nWith everything configured, run Maker as you did for the previous analysis:\n\n\nmpiexec -n 8 maker\n\n\n\n\nWe probably expect this to take a little bit longer than before, since we have added another step to our analysis.\n\n\nCompile the output\n\n\nWhen Maker has finished, compile the output:\n\n\nmaker_merge_outputs_from_datastore.pl --output maker_with_abinitio \n\n\n\n\nAnd again, it is probably best to link the resulting output (maker.gff) to a result folder (the same as defined in the previous exercise e.g. dmel_results), under a descriptive name.\n\n\nInspect the gene models\n\n\nTo get some statistics of your annotation you could launch :\n\n\ngff3_sp_statistics.pl --gff maker_with_abinitio/annotationByType/maker.gff\n\n\n\n\nWe could now also visualise the annotation in the Webapollo genome browser.",
            "title": "Abinitio Annotation"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerAbinit/#making-an-abinitio-evidence-driven-annotation-with-maker",
            "text": "The recommended way of running Maker is in combination with one or more ab-initio profile models. Maker natively supports input from several tools, including augustus, snap and genemark. The choice of tool depends a bit on the organism that you are annotating - for example, GeneMark-ES is mostly recommended for fungi, whereas augustus and snap have a more general use.  The biggest problem with ab-initio models is the process of training them. It is usually recommended to have somewhere around 500-1000 curated gene models for this purpose. Naturally, this is a bit of a contradiction for a not-yet annotated genome.  However, if one or more good ab-initio profiles are available, they can potentially greatly enhance the quality of an annotation by filling in the blanks left by missing evidence. Interestingly, Maker even works with ab-initio profiles from somewhat distantly related species since it can create so-called hints from the evidence alignments, which the gene predictor can take into account to fine-tune the predictions.  Usually when no close ab-initio profile exists for the investigated species, we use the first round of annotation (evidence based) to create one. We first filter the best gene models from this annotation, which are used then to train the abinitio tools of our choice.  In order to compare the performance of Maker with and without ab-initio predictions in a real-world scenario, we have first run a gene build without ab-initio predictions. Now, we run a similar analysis but enable ab-initio predictions through augustus.",
            "title": "Making an abinitio evidence-driven annotation with MAKER"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerAbinit/#prepare-the-input-data",
            "text": "No need to re-compute the mapping/alignment of the different lines of evidence. Indeed, this time consuming task has already been performed during the previous round of annotation (evidence based). So, we will use the corresponding gff files previously produced by MAKER.  Link the gff files you want to use into your folder:   repeatmasker.chr4.gff (already present)  repeatrunner.chr4.gff (already present)  4.fa (already present)  est_gff_stringtie.gff (transcript that have been mapped by MAKER during the evidence based round of annotation)   est2genome.gff   protein2genome.gff    ln -s maker_no_abinitio/annotationByType/est_gff:stringtie.gff est_gff_stringtie.gff\nln -s maker_no_abinitio/annotationByType/est2genome.gff \nln -s maker_no_abinitio/annotationByType/protein2genome.gff  This time, we do specify a reference species to be used by augustus, which will enable ab-initio gene finding and keep_preds=1 will also show abinitio prediction not supported by any evidences :  augustus_species=fly  #Augustus gene prediction species model  (this is where you can call the database you trained for augustus)\n...  protein2genome=0  est2genome=0  keep_preds=1  With these settings, Maker will run augustus to predict gene loci, but inform these predictions with information from the protein and est alignments.  Before running MAKER you can check you have modified the maker_opts.ctl file properly  here .",
            "title": "Prepare the input data"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerAbinit/#run-maker-with-ab-initio-predictions",
            "text": "With everything configured, run Maker as you did for the previous analysis:  mpiexec -n 8 maker  We probably expect this to take a little bit longer than before, since we have added another step to our analysis.",
            "title": "Run Maker with ab-initio predictions"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerAbinit/#compile-the-output",
            "text": "When Maker has finished, compile the output:  maker_merge_outputs_from_datastore.pl --output maker_with_abinitio   And again, it is probably best to link the resulting output (maker.gff) to a result folder (the same as defined in the previous exercise e.g. dmel_results), under a descriptive name.",
            "title": "Compile the output"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerAbinit/#inspect-the-gene-models",
            "text": "To get some statistics of your annotation you could launch :  gff3_sp_statistics.pl --gff maker_with_abinitio/annotationByType/maker.gff  We could now also visualise the annotation in the Webapollo genome browser.",
            "title": "Inspect the gene models"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/",
            "text": "Comparing and evaluating annotations\n\n\nIn this exercise you will take the three annotations you have created for Drosophila - the pure abinitio one done with augustus, the evidence-based done with MAKER and the abinitio evidence drived one done with MAKER. First, we will count the features annotated in each of them and compare that number against the existing reference annotation. Next, we will perform a proper feature-level comparison to obtain a proper estimate of congruency.\n\n\nEvaluating annotation quality\n\n\nEvaluating an annotation can be done in three ways - running busco but with the proteins obtained from the annotation, in comparison with another annotation or in reference to the evidence alignments. The former isn't so much a quality check as a measure of congruency - i.e. the resulting numbers don't tell you which of the two gene builds is more correct. On the other hand, a comparison with evidence alignments is what Maker uses internally to select gene models. After synthesizing and annotating loci, the resulting model will be ranked against the filtered evidence alignments. The more congruent these two points of information are, the lower the 'annotation edit distance' (AED) will be. The AED score can be used to e.g. check an annotation for problematic models that may then be subjected to manual curation.\n\n\nBUSCO\n\n\nBUSCO is run before annotating to check if the assembly is good and therefore if the annotation will be good. It is also run after the structural annotation to then compare if we indeed find a number of genes corresponding of the first run of busco.\n\n\nYou will need to link the protein file created by maker on the run with the ab-initio\n\n\ncd ~/annotation_course/practical2\nmkdir busco\ncd busco\n\nln -s ../maker/maker_with_abinitio/annotations.proteins.fa   \nln -s ~/annotation_course/practical1/busco/metazoa_odb9\n\nBUSCO.py -i annotations.proteins.fa -o dmel_maker_abinitio -m prot -c 8 -l metazoa_odb9\n\n\n\n\n\n\nif you compare with you first busco results what do you see?\n\n\n\n\nComparing annotations\n\n\nAs with many tasks within bioinformatics, it is always a great idea to first look around for existing solutions. In the case of comparing annotations, there are in fact options already out there. One such example is genometools, which we have briefly used before.\n\n\nPreparing the input files\n\n\nFirst you have to be situated in a folder containing the two maker annotations (with and without ab initio) and the augustus annotation. \n\n\ncd ~/annotation_course/practical2\nmkdir compare\ncd compare\nln -s ../maker/maker_no_abinitio/annotationByType/maker.gff maker_no_abinitio.gff \nln -s ../maker/maker_with_abinitio/annotationByType/maker.gff maker_abinitio.gff \n\n\n\n\nThen, copy or sym-link the EnsEMBL reference annotation.\n\n\nln -s ~/annotation_course/data/annotation/ensembl.chr4.gff\n\n\n\n\nNow we have to sort any GFF3-formatted annotation in a way that genometools accepts.\n\n\nsed -i '1i##gff-version 3' maker_no_abinitio.gff\nsed -i '1i##gff-version 3' maker_abinitio.gff\n\ngt gff3 -sort maker_no_abinitio.gff > maker_no_abinitio.sorted.gff \ngt gff3 -sort maker_abinitio.gff > maker_abinitio.sorted.gff \ngt gff3 -sort ensembl.chr4.gff > ensembl.sorted.gff \n\n\n\n\nCounting features\n\n\nNext, we get the feature counts for the three annotations and the reference from EnsEMBL:\n\n\ngt stat maker_no_abinitio.gff\n\n\n\n\nor\n\n\ngff3_sp_statistics.pl --gff maker_no_abinitio.gff\n\n\n\n\n(or whatever you decided to name the file(s). The use of the sorted file or the original one changes nothing here)\n\n\nAs you will note, there are some differences - and of course, this is expected, since we used quite different approaches to generate the two gene builds. EnsEMBL on the other hand is originally imported from FlyBase. Obviously, a lot of manual labor and much more data has been put into the FlyBase annotation - and this highlights a common limitation of any computational pipeline. You will simply never reach the same level of quality and detail as seen in a manually curated reference annotation.\n\n\nPairwise comparison of features\n\n\nBut feature counts alone can't really give you a clear measure of overlap/differences between any two annotations. In order to properly compare them, we can use another function included in genometools.\n\n\nWith the sorted files, we can now perform a comparison:\n\n\ngt eval ensembl.sorted.gff maker_no_abinitio.sorted.gff  \n\n\n\n\nThis will create a long list of measures for all relevant sequence features with respect to both the 'sensitivity' and 'specificity' - as a measure of how close the annotation comes to a reference. As a reminder, 'specificity' measures the fraction of a reference overlapping a prediction whereas 'sensitivity' measures the fraction of a prediction overlapping a reference.\n\n\nNote that the measures employed by genometools function in a all-or-nothing fashion. If the overlap is not 100%, it doesn't count (which is why you are unlikely to find gene-level congruencies between your gene builds and the reference annotation).  \n\n\nFrom the comparison of your annotations to the Ensembl annotation, which one \nseems\n to be the most comprehensive to you ?\n\n\nVisualising annotations\n\n\nNote:\n The following section overlaps with some of the exercises you have done earlier (comparing augustus predictions against the reference annotation).\n\n\nIn the previous tasks, we have looked at the overlap between different gene builds. While this gives us an indication of how similar two annotations are, \nit doesn't really allow us to judge the overall quality and similarity of annotations\n. Remember, sensitivity and specificity are 'all-or-nothing' - two annotations may be considered very different, but provide similar information, biologically. By that, we mean that two gene models don't need to be 100% identical in their coordinates to tell the scientist that a gene indeed exists in a given location and what it's product looks like.\n\n\nWe therefore need to visually inspect and compare the gene builds. This is a crucial step in any annotation project - gene build pipelines use a set of defined rules, but human pattern recognition is needed to spot potential systematic errors. For example, a pipeline like Maker will simply take all your input and try to synthesize it into an annotation, but it doesn't do too much checks on the data itself. What if you RNA-seq data is messier than you thought? What if your protein data set includes to many 'predicted' proteins that are in clear conflict with the other data?\n\n\nThere exist a number of 'annotation viewers' - IGV, Argo and Apollo, to name a few. A common choice for annotators is the web-based version of Apollo, WebApollo, mostly for its curation capabilities.\n\n\nUsing WebApollo to view annotations\n\n\nTransfer your maker annotation files to your computer using the scp command.\n\nThen, jump to \nWebApollo\n and upload your annotation track into the genome portal called \ndrosophila_melanogaster_chr4\n. \nHere find the WebApollo instruction\n\nYou can now compare your gene builds against this reference. Some questions to ask yourself:\n\n\n\n\nDo my gene builds recover all the genes found in the reference?  \n\n\nWhat sort of differences are most common?",
            "title": "Comparing Annotations"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#comparing-and-evaluating-annotations",
            "text": "In this exercise you will take the three annotations you have created for Drosophila - the pure abinitio one done with augustus, the evidence-based done with MAKER and the abinitio evidence drived one done with MAKER. First, we will count the features annotated in each of them and compare that number against the existing reference annotation. Next, we will perform a proper feature-level comparison to obtain a proper estimate of congruency.",
            "title": "Comparing and evaluating annotations"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#evaluating-annotation-quality",
            "text": "Evaluating an annotation can be done in three ways - running busco but with the proteins obtained from the annotation, in comparison with another annotation or in reference to the evidence alignments. The former isn't so much a quality check as a measure of congruency - i.e. the resulting numbers don't tell you which of the two gene builds is more correct. On the other hand, a comparison with evidence alignments is what Maker uses internally to select gene models. After synthesizing and annotating loci, the resulting model will be ranked against the filtered evidence alignments. The more congruent these two points of information are, the lower the 'annotation edit distance' (AED) will be. The AED score can be used to e.g. check an annotation for problematic models that may then be subjected to manual curation.",
            "title": "Evaluating annotation quality"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#busco",
            "text": "BUSCO is run before annotating to check if the assembly is good and therefore if the annotation will be good. It is also run after the structural annotation to then compare if we indeed find a number of genes corresponding of the first run of busco.  You will need to link the protein file created by maker on the run with the ab-initio  cd ~/annotation_course/practical2\nmkdir busco\ncd busco\n\nln -s ../maker/maker_with_abinitio/annotations.proteins.fa   \nln -s ~/annotation_course/practical1/busco/metazoa_odb9\n\nBUSCO.py -i annotations.proteins.fa -o dmel_maker_abinitio -m prot -c 8 -l metazoa_odb9   if you compare with you first busco results what do you see?",
            "title": "BUSCO"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#comparing-annotations",
            "text": "As with many tasks within bioinformatics, it is always a great idea to first look around for existing solutions. In the case of comparing annotations, there are in fact options already out there. One such example is genometools, which we have briefly used before.",
            "title": "Comparing annotations"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#preparing-the-input-files",
            "text": "First you have to be situated in a folder containing the two maker annotations (with and without ab initio) and the augustus annotation.   cd ~/annotation_course/practical2\nmkdir compare\ncd compare\nln -s ../maker/maker_no_abinitio/annotationByType/maker.gff maker_no_abinitio.gff \nln -s ../maker/maker_with_abinitio/annotationByType/maker.gff maker_abinitio.gff   Then, copy or sym-link the EnsEMBL reference annotation.  ln -s ~/annotation_course/data/annotation/ensembl.chr4.gff  Now we have to sort any GFF3-formatted annotation in a way that genometools accepts.  sed -i '1i##gff-version 3' maker_no_abinitio.gff\nsed -i '1i##gff-version 3' maker_abinitio.gff\n\ngt gff3 -sort maker_no_abinitio.gff > maker_no_abinitio.sorted.gff \ngt gff3 -sort maker_abinitio.gff > maker_abinitio.sorted.gff \ngt gff3 -sort ensembl.chr4.gff > ensembl.sorted.gff",
            "title": "Preparing the input files"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#counting-features",
            "text": "Next, we get the feature counts for the three annotations and the reference from EnsEMBL:  gt stat maker_no_abinitio.gff  or  gff3_sp_statistics.pl --gff maker_no_abinitio.gff  (or whatever you decided to name the file(s). The use of the sorted file or the original one changes nothing here)  As you will note, there are some differences - and of course, this is expected, since we used quite different approaches to generate the two gene builds. EnsEMBL on the other hand is originally imported from FlyBase. Obviously, a lot of manual labor and much more data has been put into the FlyBase annotation - and this highlights a common limitation of any computational pipeline. You will simply never reach the same level of quality and detail as seen in a manually curated reference annotation.",
            "title": "Counting features"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#pairwise-comparison-of-features",
            "text": "But feature counts alone can't really give you a clear measure of overlap/differences between any two annotations. In order to properly compare them, we can use another function included in genometools.  With the sorted files, we can now perform a comparison:  gt eval ensembl.sorted.gff maker_no_abinitio.sorted.gff    This will create a long list of measures for all relevant sequence features with respect to both the 'sensitivity' and 'specificity' - as a measure of how close the annotation comes to a reference. As a reminder, 'specificity' measures the fraction of a reference overlapping a prediction whereas 'sensitivity' measures the fraction of a prediction overlapping a reference.  Note that the measures employed by genometools function in a all-or-nothing fashion. If the overlap is not 100%, it doesn't count (which is why you are unlikely to find gene-level congruencies between your gene builds and the reference annotation).    From the comparison of your annotations to the Ensembl annotation, which one  seems  to be the most comprehensive to you ?",
            "title": "Pairwise comparison of features"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#visualising-annotations",
            "text": "Note:  The following section overlaps with some of the exercises you have done earlier (comparing augustus predictions against the reference annotation).  In the previous tasks, we have looked at the overlap between different gene builds. While this gives us an indication of how similar two annotations are,  it doesn't really allow us to judge the overall quality and similarity of annotations . Remember, sensitivity and specificity are 'all-or-nothing' - two annotations may be considered very different, but provide similar information, biologically. By that, we mean that two gene models don't need to be 100% identical in their coordinates to tell the scientist that a gene indeed exists in a given location and what it's product looks like.  We therefore need to visually inspect and compare the gene builds. This is a crucial step in any annotation project - gene build pipelines use a set of defined rules, but human pattern recognition is needed to spot potential systematic errors. For example, a pipeline like Maker will simply take all your input and try to synthesize it into an annotation, but it doesn't do too much checks on the data itself. What if you RNA-seq data is messier than you thought? What if your protein data set includes to many 'predicted' proteins that are in clear conflict with the other data?  There exist a number of 'annotation viewers' - IGV, Argo and Apollo, to name a few. A common choice for annotators is the web-based version of Apollo, WebApollo, mostly for its curation capabilities.",
            "title": "Visualising annotations"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#using-webapollo-to-view-annotations",
            "text": "Transfer your maker annotation files to your computer using the scp command. \nThen, jump to  WebApollo  and upload your annotation track into the genome portal called  drosophila_melanogaster_chr4 .  Here find the WebApollo instruction \nYou can now compare your gene builds against this reference. Some questions to ask yourself:   Do my gene builds recover all the genes found in the reference?    What sort of differences are most common?",
            "title": "Using WebApollo to view annotations"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_supl_maker/",
            "text": "Configure your maker project : The maker_opts.ctl file in detail:\n\n\nWhen executing the command \"maker -CTL\" MAKER creates 3 control files.\nOf these, only \nmaker_opts.ctl\n is of concern to us. Have a look at the following sections and fill in the information as shown:\n\n\n#-----Genome (these are always required)\n\ngenome=\n4.fa\n #genome sequence (fasta file or fasta embeded in GFF3 file)\n\norganism_type=eukaryotic #eukaryotic or prokaryotic. Default is eukaryotic\n\n\n...\n\n\n#-----EST Evidence (for best results provide a file for at least one)\n\n\nest=est.chr4.fa\n #set of ESTs or assembled mRNA-seq in fasta format\n\naltest= #EST/cDNA sequence file in fasta format from an alternate organism\n\n\nest_gff=stringtie2genome.chr4.ok.gff\n #aligned ESTs or mRNA-seq from an external GFF3 file\n\naltest_gff= #aligned ESTs from a closly relate species in GFF3 format\n\n\n...\n\n\n#-----Protein Homology Evidence (for best results provide a file for at least one)\n\n\nprotein=proteins.chr4.fa\n #protein sequence file in fasta format (i.e. from mutiple oransisms)\n\nprotein_gff= #aligned protein homology evidence from an external GFF3 file\n\n\n...\n\n\n#-----Repeat Masking (leave values blank to skip repeat masking)\n\n\nmodel_org=\n #select a model organism for RepBase masking in RepeatMasker\n\nrmlib= #provide an organism specific repeat library in fasta format for RepeatMasker \n\n\nrepeat_protein=\n #provide a fasta file of transposable element proteins for RepeatRunner\n\nrm_gff=\nrepeatmasker.chr4.gff,repeatrunner.chr4.gff\n #pre-identified repeat elements from an external GFF3 file\n\nprok_rm=0 #forces MAKER to repeatmask prokaryotes (no reason to change this), 1 = yes, 0 = no\n\nsoftmask=1 #use soft-masking rather than hard-masking in BLAST (i.e. seg and dust filtering)\n\n\n...\n\n\n#-----Gene Prediction\n\nsnaphmm= #SNAP HMM file\n\ngmhmm= #GeneMark HMM file\n\naugustus_species= #Augustus gene prediction species model\n\nfgenesh_par_file= #FGENESH parameter file\n\npred_gff= #ab-initio predictions from an external GFF3 file\n\nmodel_gff= #annotated gene models from an external GFF3 file (annotation pass-through)\n\n\nest2genome=1\n #infer gene predictions directly from ESTs, 1 = yes, 0 = no\n\n\nprotein2genome=1\n #infer predictions from protein homology, 1 = yes, 0 = no\n\ntrna=0 #find tRNAs with tRNAscan, 1 = yes, 0 = no\n\nsnoscan_rrna= #rRNA file to have Snoscan find snoRNAs\n\nunmask=0 #also run ab-initio prediction programs on unmasked sequence, 1 = yes, 0 = no\n\n\nTo better understand the different parameters you can have a look \nhere",
            "title": "Configuring Maker (I)"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_supl_maker/#configure-your-maker-project-the-maker_optsctl-file-in-detail",
            "text": "When executing the command \"maker -CTL\" MAKER creates 3 control files.\nOf these, only  maker_opts.ctl  is of concern to us. Have a look at the following sections and fill in the information as shown:  #-----Genome (these are always required) \ngenome= 4.fa  #genome sequence (fasta file or fasta embeded in GFF3 file) \norganism_type=eukaryotic #eukaryotic or prokaryotic. Default is eukaryotic  ...  #-----EST Evidence (for best results provide a file for at least one)  est=est.chr4.fa  #set of ESTs or assembled mRNA-seq in fasta format \naltest= #EST/cDNA sequence file in fasta format from an alternate organism  est_gff=stringtie2genome.chr4.ok.gff  #aligned ESTs or mRNA-seq from an external GFF3 file \naltest_gff= #aligned ESTs from a closly relate species in GFF3 format  ...  #-----Protein Homology Evidence (for best results provide a file for at least one)  protein=proteins.chr4.fa  #protein sequence file in fasta format (i.e. from mutiple oransisms) \nprotein_gff= #aligned protein homology evidence from an external GFF3 file  ...  #-----Repeat Masking (leave values blank to skip repeat masking)  model_org=  #select a model organism for RepBase masking in RepeatMasker \nrmlib= #provide an organism specific repeat library in fasta format for RepeatMasker   repeat_protein=  #provide a fasta file of transposable element proteins for RepeatRunner \nrm_gff= repeatmasker.chr4.gff,repeatrunner.chr4.gff  #pre-identified repeat elements from an external GFF3 file \nprok_rm=0 #forces MAKER to repeatmask prokaryotes (no reason to change this), 1 = yes, 0 = no \nsoftmask=1 #use soft-masking rather than hard-masking in BLAST (i.e. seg and dust filtering)  ...  #-----Gene Prediction \nsnaphmm= #SNAP HMM file \ngmhmm= #GeneMark HMM file \naugustus_species= #Augustus gene prediction species model \nfgenesh_par_file= #FGENESH parameter file \npred_gff= #ab-initio predictions from an external GFF3 file \nmodel_gff= #annotated gene models from an external GFF3 file (annotation pass-through)  est2genome=1  #infer gene predictions directly from ESTs, 1 = yes, 0 = no  protein2genome=1  #infer predictions from protein homology, 1 = yes, 0 = no \ntrna=0 #find tRNAs with tRNAscan, 1 = yes, 0 = no \nsnoscan_rrna= #rRNA file to have Snoscan find snoRNAs \nunmask=0 #also run ab-initio prediction programs on unmasked sequence, 1 = yes, 0 = no  To better understand the different parameters you can have a look  here",
            "title": "Configure your maker project : The maker_opts.ctl file in detail:"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_supl3_maker/",
            "text": "Configure the maker_opts.ctl properly for the abinitio evidence-driven annotation:\n\n\n#-----Genome (these are always required)\n\n\ngenome=4.fa\n #genome sequence (fasta file or fasta embeded in GFF3 file)\n\norganism_type=eukaryotic #eukaryotic or prokaryotic. Default is eukaryotic\n\n\n...\n\n\n#-----EST Evidence (for best results provide a file for at least one) \n\nest=\n #set of ESTs or assembled mRNA-seq in fasta format\n\naltest= #EST/cDNA sequence file in fasta format from an alternate organism\n\n\nest_gff=est2genome.gff, est_gff_stringtie.gff\n #aligned ESTs or mRNA-seq from an external GFF3 file\n\naltest_gff= #aligned ESTs from a closly relate species in GFF3 format\n\n\n...\n\n\n#-----Protein Homology Evidence (for best results provide a file for at least one)\n\n\nprotein=\n #protein sequence file in fasta format (i.e. from mutiple oransisms)\n\n\nprotein_gff=protein2genome.gff\n #aligned protein homology evidence from an external GFF3 file\n\n\n...\n\n\n#-----Repeat Masking (leave values blank to skip repeat masking)\n\n\nmodel_org=\n #select a model organism for RepBase masking in RepeatMasker\n\nrmlib= #provide an organism specific repeat library in fasta format for RepeatMasker \n\n\nrepeat_protein=\n #provide a fasta file of transposable element proteins for RepeatRunner\n\n\nrm_gff=repeatmasker.chr4.gff,repeatrunner.chr4.gff\n #pre-identified repeat elements from an external GFF3 file\n\nprok_rm=0 #forces MAKER to repeatmask prokaryotes (no reason to change this), 1 = yes, 0 = no\n\nsoftmask=1 #use soft-masking rather than hard-masking in BLAST (i.e. seg and dust filtering)\n\n\n...\n\n\n#-----Gene Prediction\n\nsnaphmm= #SNAP HMM file\n\ngmhmm= #GeneMark HMM file\n\n\naugustus_species=fly\n #Augustus gene prediction species model\n\nfgenesh_par_file= #FGENESH parameter file\n\npred_gff= #ab-initio predictions from an external GFF3 file\n\nmodel_gff= #annotated gene models from an external GFF3 file (annotation pass-through)\n\n\nest2genome=0\n #infer gene predictions directly from ESTs, 1 = yes, 0 = no\n\n\nprotein2genome=0\n #infer predictions from protein homology, 1 = yes, 0 = no\n\ntrna=0 #find tRNAs with tRNAscan, 1 = yes, 0 = no\n\nsnoscan_rrna= #rRNA file to have Snoscan find snoRNAs\n\nunmask=0 #also run ab-initio prediction programs on unmasked sequence, 1 = yes, 0 = no\n\n\n...\n\nkeep_preds=1\n\n...\n\n\nTo better understand the different parameters you can have a look \nhere",
            "title": "Configuring Maker (I)"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_supl3_maker/#configure-the-maker_optsctl-properly-for-the-abinitio-evidence-driven-annotation",
            "text": "#-----Genome (these are always required)  genome=4.fa  #genome sequence (fasta file or fasta embeded in GFF3 file) \norganism_type=eukaryotic #eukaryotic or prokaryotic. Default is eukaryotic  ...  #-----EST Evidence (for best results provide a file for at least one)  est=  #set of ESTs or assembled mRNA-seq in fasta format \naltest= #EST/cDNA sequence file in fasta format from an alternate organism  est_gff=est2genome.gff, est_gff_stringtie.gff  #aligned ESTs or mRNA-seq from an external GFF3 file \naltest_gff= #aligned ESTs from a closly relate species in GFF3 format  ...  #-----Protein Homology Evidence (for best results provide a file for at least one)  protein=  #protein sequence file in fasta format (i.e. from mutiple oransisms)  protein_gff=protein2genome.gff  #aligned protein homology evidence from an external GFF3 file  ...  #-----Repeat Masking (leave values blank to skip repeat masking)  model_org=  #select a model organism for RepBase masking in RepeatMasker \nrmlib= #provide an organism specific repeat library in fasta format for RepeatMasker   repeat_protein=  #provide a fasta file of transposable element proteins for RepeatRunner  rm_gff=repeatmasker.chr4.gff,repeatrunner.chr4.gff  #pre-identified repeat elements from an external GFF3 file \nprok_rm=0 #forces MAKER to repeatmask prokaryotes (no reason to change this), 1 = yes, 0 = no \nsoftmask=1 #use soft-masking rather than hard-masking in BLAST (i.e. seg and dust filtering)  ...  #-----Gene Prediction \nsnaphmm= #SNAP HMM file \ngmhmm= #GeneMark HMM file  augustus_species=fly  #Augustus gene prediction species model \nfgenesh_par_file= #FGENESH parameter file \npred_gff= #ab-initio predictions from an external GFF3 file \nmodel_gff= #annotated gene models from an external GFF3 file (annotation pass-through)  est2genome=0  #infer gene predictions directly from ESTs, 1 = yes, 0 = no  protein2genome=0  #infer predictions from protein homology, 1 = yes, 0 = no \ntrna=0 #find tRNAs with tRNAscan, 1 = yes, 0 = no \nsnoscan_rrna= #rRNA file to have Snoscan find snoRNAs \nunmask=0 #also run ab-initio prediction programs on unmasked sequence, 1 = yes, 0 = no  ... keep_preds=1 \n...  To better understand the different parameters you can have a look  here",
            "title": "Configure the maker_opts.ctl properly for the abinitio evidence-driven annotation:"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_supl2_maker/",
            "text": "Inspect the output\n\n\nFinding your way around\n\n\nBy default, Maker will write the output of its different analyses into a folder named:\n\n\n<name_of_genome_fasta>.maker.output\n\n\nIn our case:\n\n\n4.maker.output\n\n\nWithin the main output directory, Maker keeps a copy of the config files, a database (here: 4.db), directories for the blast databases created from your evidence data and a file called 4_master_datastore_index.log.\n\n\nOut of these files, only the 4_master_datastore_index is really interesting to us. It includes a log of all the contigs included in the genome fasta file - together with their processing status (ideally: FINISHED) and the location of the output files. Since Maker can technically run in parallel on a large number of contigs, it creates separate folders for each of these input data. For larger genomes, this can generate a very deep and confusing folder tree. The 4_master_datastore_index helps you make sense of it:\n\n\n4       4\\_datastore/A8/7F/4/ STARTED  \n4       4\\_datastore/A8/7F/4/ FINISHED\n\n\n\n\nThis meens the sequence \n4\n was started - and finished, with all data (annotation, protein predictions etc) written to the subfolder 4_datastore/A8/7F/4/.\n\n\nIf you look into that folder, you will find the finished Maker annotation for this contig.\n\n\nrw-rw-r- 1 student student 472193 Mar 24 10:16 4.gff <br/>\n\\*rw-rw-r- 1 student student 3599 Mar 24 10:16 4.maker.augustus\\_masked.proteins.fasta <br/>\n\\*rw-rw-r- 1 student student 10388 Mar 24 10:16 4.maker.augustus\\_masked.transcripts.fasta  <br/>\n\\*rw-rw-r- 1 student student 176 Mar 24 10:16 4.maker.non\\_overlapping\\_ab\\_initio.proteins.fasta <br/>\n\\*rw-rw-r- 1 student student 328 Mar 24 10:16 4.maker.non\\_overlapping\\_ab\\_initio.transcripts.fasta  <br/>\nrw-rw-r- 1 student student 3931 Mar 24 10:16 4.maker.proteins.fasta  <br/>\nrw-rw-r- 1 student student 20865 Mar 24 10:16 4.maker.transcripts.fasta  <br/>\nrw-rw-r- 1 student student 4248 Mar 24 10:15 run.log  <br/>\ndrwxrwsr-x 3 student student 4096 Mar 24 10:16 theVoid.4\n\n\n\n\n* only if an abinitio tool has been activated\n\n\nThe main annotation file is '4.gff' - including both the finished gene models and all the raw compute data. The other files include fasta files for the different sequence features that have been annotated - based on ab-initio predictions through augustus as well as on the finished gene models. The folder 'theVoid' include all the raw computations that Maker has performed to synthesize the evidence into gene models.\n\n\nUnderstanding a Maker annotation\n\n\nYou have two options now for gathering the output in some usable form - copy select files by hand to wherever you want them. Or you can use a script that does the job for you (we have included an example in the script folder).\n\n\nFrom the folder you have run Maker, run the script called 'maker_merge_outputs_from_datastore' to create an output file for all annotations and protein files:\n\n\nmaker_merge_outputs_from_datastore.pl \n\n\n\n\nThis will create a directory called \"\nannotations\n\" containing:\n\n\n-annotations.gff\n\n-annotations.proteins.fa\n\n-annotationByType/  \n\n\n\n\nannotations.gff\n file\n  \n\n\n\n\nIt's a mix of all the gff tracks produced/handled by maker. It contains the annotation done by maker mixed up with other gff lines like the protein alignments, repeats, etc..\nIf you use 'less' to read the annotation file \nannotations.gff\n (\nGFF3 format\n), you will see a range of different features:\n\n\n##gff-version 3  \n4       .       contig  1       1351857 .       .       .       ID=4;Name=4\n4       maker   gene    24134   25665   .       +       .       ID=maker-4-exonerate_protein2genome-gene-0.0;Name=maker-4-exonerate_protein2genome-gene-0.0\n4       maker   mRNA    24134   25665   917     +       .       ID=maker-4-exonerate_protein2genome-gene-0.0-mRNA-1;Parent=maker-4-exonerate_protein2genome-gene-0.0;Name=maker-4-exonerate_protein2genome-gene-0.0-mRNA-1;_AED=0.09;_eAED=0.09;_QI=0|0.33|0.25|1|0|0|4|44|290\n\n\n\n\n...\n\n\nFor example, the above lines read:\n\n\nA new contig is being shown, with the id '4' and a length of 1351857 nucleotides\n\nOn this contig, a gene feature is located from position 24134 to 25665, on the plus strand and with the id 'maker-4-exonerate_protein2genome-gene-0.0'. \nOn this contig, belonging to the gene, is located a transcript from position 24134 to 25665, on the plus strand and with the id 'maker-4-exonerate_protein2genome-gene-0.0-mRNA-1'. It's quality, or AED score, is 0.09 - which means that the evidence alignments are close to be in perfect agreement with the transcript model.\n\n\nAnd so on.\n\n\n\n\n\n\nannotations.proteins.fa\n file\n\nThis file contains the proteins translated from the CDS of gene models predicted.\n\n\n\n\n\n\nannotationByType\n directory\n\nThe different types of information present in the annotation file (annotations.gff) are separated into independent file into the \"annotationByType\" directory.This is useful for a number of applications, like visualizing it as separate tracks in a genome browser. Or to compute some intersting numbers from the gene models.\n\n\n\n\n\n\nThis should contains a bunch of files, including '\nmaker.gff\n' - which contains the actual gene models.",
            "title": "Inspecting Maker output"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_supl2_maker/#inspect-the-output",
            "text": "",
            "title": "Inspect the output"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_supl2_maker/#finding-your-way-around",
            "text": "By default, Maker will write the output of its different analyses into a folder named:  <name_of_genome_fasta>.maker.output  In our case:  4.maker.output  Within the main output directory, Maker keeps a copy of the config files, a database (here: 4.db), directories for the blast databases created from your evidence data and a file called 4_master_datastore_index.log.  Out of these files, only the 4_master_datastore_index is really interesting to us. It includes a log of all the contigs included in the genome fasta file - together with their processing status (ideally: FINISHED) and the location of the output files. Since Maker can technically run in parallel on a large number of contigs, it creates separate folders for each of these input data. For larger genomes, this can generate a very deep and confusing folder tree. The 4_master_datastore_index helps you make sense of it:  4       4\\_datastore/A8/7F/4/ STARTED  \n4       4\\_datastore/A8/7F/4/ FINISHED  This meens the sequence  4  was started - and finished, with all data (annotation, protein predictions etc) written to the subfolder 4_datastore/A8/7F/4/.  If you look into that folder, you will find the finished Maker annotation for this contig.  rw-rw-r- 1 student student 472193 Mar 24 10:16 4.gff <br/>\n\\*rw-rw-r- 1 student student 3599 Mar 24 10:16 4.maker.augustus\\_masked.proteins.fasta <br/>\n\\*rw-rw-r- 1 student student 10388 Mar 24 10:16 4.maker.augustus\\_masked.transcripts.fasta  <br/>\n\\*rw-rw-r- 1 student student 176 Mar 24 10:16 4.maker.non\\_overlapping\\_ab\\_initio.proteins.fasta <br/>\n\\*rw-rw-r- 1 student student 328 Mar 24 10:16 4.maker.non\\_overlapping\\_ab\\_initio.transcripts.fasta  <br/>\nrw-rw-r- 1 student student 3931 Mar 24 10:16 4.maker.proteins.fasta  <br/>\nrw-rw-r- 1 student student 20865 Mar 24 10:16 4.maker.transcripts.fasta  <br/>\nrw-rw-r- 1 student student 4248 Mar 24 10:15 run.log  <br/>\ndrwxrwsr-x 3 student student 4096 Mar 24 10:16 theVoid.4  * only if an abinitio tool has been activated  The main annotation file is '4.gff' - including both the finished gene models and all the raw compute data. The other files include fasta files for the different sequence features that have been annotated - based on ab-initio predictions through augustus as well as on the finished gene models. The folder 'theVoid' include all the raw computations that Maker has performed to synthesize the evidence into gene models.",
            "title": "Finding your way around"
        },
        {
            "location": "/nbis_annotation/practical_session/practical2_supl2_maker/#understanding-a-maker-annotation",
            "text": "You have two options now for gathering the output in some usable form - copy select files by hand to wherever you want them. Or you can use a script that does the job for you (we have included an example in the script folder).  From the folder you have run Maker, run the script called 'maker_merge_outputs_from_datastore' to create an output file for all annotations and protein files:  maker_merge_outputs_from_datastore.pl   This will create a directory called \" annotations \" containing:  -annotations.gff \n-annotations.proteins.fa \n-annotationByType/     annotations.gff  file      It's a mix of all the gff tracks produced/handled by maker. It contains the annotation done by maker mixed up with other gff lines like the protein alignments, repeats, etc..\nIf you use 'less' to read the annotation file  annotations.gff  ( GFF3 format ), you will see a range of different features:  ##gff-version 3  \n4       .       contig  1       1351857 .       .       .       ID=4;Name=4\n4       maker   gene    24134   25665   .       +       .       ID=maker-4-exonerate_protein2genome-gene-0.0;Name=maker-4-exonerate_protein2genome-gene-0.0\n4       maker   mRNA    24134   25665   917     +       .       ID=maker-4-exonerate_protein2genome-gene-0.0-mRNA-1;Parent=maker-4-exonerate_protein2genome-gene-0.0;Name=maker-4-exonerate_protein2genome-gene-0.0-mRNA-1;_AED=0.09;_eAED=0.09;_QI=0|0.33|0.25|1|0|0|4|44|290  ...  For example, the above lines read:  A new contig is being shown, with the id '4' and a length of 1351857 nucleotides \nOn this contig, a gene feature is located from position 24134 to 25665, on the plus strand and with the id 'maker-4-exonerate_protein2genome-gene-0.0'. \nOn this contig, belonging to the gene, is located a transcript from position 24134 to 25665, on the plus strand and with the id 'maker-4-exonerate_protein2genome-gene-0.0-mRNA-1'. It's quality, or AED score, is 0.09 - which means that the evidence alignments are close to be in perfect agreement with the transcript model.  And so on.    annotations.proteins.fa  file \nThis file contains the proteins translated from the CDS of gene models predicted.    annotationByType  directory \nThe different types of information present in the annotation file (annotations.gff) are separated into independent file into the \"annotationByType\" directory.This is useful for a number of applications, like visualizing it as separate tracks in a genome browser. Or to compute some intersting numbers from the gene models.    This should contains a bunch of files, including ' maker.gff ' - which contains the actual gene models.",
            "title": "Understanding a Maker annotation"
        }
    ]
}