{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SGBC Bioinformatics Course This website is a collection of lectures and tutorials given during the annual Bioinformatics course given at SLU by the SGBC . Course information What: Bioinformatics, 15 ECTS Where: SLU, Campus Ultuna, Uppsala, Sweden When: 21 Jan - 25 March 2019 Course page: link Course schedule: link (Google docs) Welcome to the course! Table of Content First Week Biological Databases Online Blast Introduction to the command-line Cloud Computing Installing Software Command-line Blast Second week Sequencing Technologies Quality Control De-novo Genome Assembly Assembly Challenge Genome Annotation Pan-genome Analysis Nanopore Sequencing Third week Genome Browsers R Fourth week RNA Sequencing Genome Annotation (NBIS) Fifth week Phylogeny EMBOSS UGENE Sixth week Introduction to proteins analysis Metabarcoding Metagenome assembly Comparative metagenomics License Unless stated otherwise, all the lessons are licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA. Contributors The following people have contributed to these course material, in no particular order: Hadrien Gourl\u00e9 Juliette Hayer Oskar Karlsson Jacques Dainat","title":"Home"},{"location":"#sgbc-bioinformatics-course","text":"This website is a collection of lectures and tutorials given during the annual Bioinformatics course given at SLU by the SGBC .","title":"SGBC Bioinformatics Course"},{"location":"#course-information","text":"What: Bioinformatics, 15 ECTS Where: SLU, Campus Ultuna, Uppsala, Sweden When: 21 Jan - 25 March 2019 Course page: link Course schedule: link (Google docs) Welcome to the course!","title":"Course information"},{"location":"#table-of-content","text":"","title":"Table of Content"},{"location":"#first-week","text":"Biological Databases Online Blast Introduction to the command-line Cloud Computing Installing Software Command-line Blast","title":"First Week"},{"location":"#second-week","text":"Sequencing Technologies Quality Control De-novo Genome Assembly Assembly Challenge Genome Annotation Pan-genome Analysis Nanopore Sequencing","title":"Second week"},{"location":"#third-week","text":"Genome Browsers R","title":"Third week"},{"location":"#fourth-week","text":"RNA Sequencing Genome Annotation (NBIS)","title":"Fourth week"},{"location":"#fifth-week","text":"Phylogeny EMBOSS UGENE","title":"Fifth week"},{"location":"#sixth-week","text":"Introduction to proteins analysis Metabarcoding Metagenome assembly Comparative metagenomics","title":"Sixth week"},{"location":"#license","text":"Unless stated otherwise, all the lessons are licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.","title":"License"},{"location":"#contributors","text":"The following people have contributed to these course material, in no particular order: Hadrien Gourl\u00e9 Juliette Hayer Oskar Karlsson Jacques Dainat","title":"Contributors"},{"location":"assembly_challenge/","text":"Assembly Challenge During this session, you will be asked to produce the best assembly possible of *Mycoplasma * JCVI-syn1.0 Delta1-6 8-Deletion Strain. The data comes from a synthetic genome developped by the JCVI There are various assemblers already installed on your virtual machines but feel free to try and install others. Below you will find the commands needed to download the data, as well as links to the websites of some well-known assemblers and quality assessment tools. Good luck! Download the Data fastq-dump -split-files SRR1530976 Assemblers available megahit SPAdes sga Abyss Unicycler Quality assessment quast busco bowtie2 multiqc","title":"Assembly Challenge"},{"location":"assembly_challenge/#assembly-challenge","text":"During this session, you will be asked to produce the best assembly possible of *Mycoplasma * JCVI-syn1.0 Delta1-6 8-Deletion Strain. The data comes from a synthetic genome developped by the JCVI There are various assemblers already installed on your virtual machines but feel free to try and install others. Below you will find the commands needed to download the data, as well as links to the websites of some well-known assemblers and quality assessment tools. Good luck!","title":"Assembly Challenge"},{"location":"assembly_challenge/#download-the-data","text":"fastq-dump -split-files SRR1530976","title":"Download the Data"},{"location":"assembly_challenge/#assemblers-available","text":"megahit SPAdes sga Abyss Unicycler","title":"Assemblers available"},{"location":"assembly_challenge/#quality-assessment","text":"quast busco bowtie2 multiqc","title":"Quality assessment"},{"location":"cloud/","text":"Introduction to Cloud Computing In this lesson you'll learn how to connect and use a linux server. Most bioinformaticians worldwide connect daily to cloud computing services to perform their analyses. There are several reasons for this. Firstly biology - like most other areas of science - is dealing with a deluge of data due to the rapid advancement of data collection methods. It is now common that data collected for an experiment doesn't fit on a researcher's laptop and that the resources needed for running an analysis far exceed a desktop computer's computing power. Secondly the vast majority of research software are developed and released for linux. Most people run MacOS or Windows on their desktop computers and laptop, which makes the installation of some software difficult or at the very least inconvenient. What is the cloud anyway? The cloud is basically lots of servers (thing big big computers) stacked together in a giant, powerful infrastructure. You can lend part of this infrastructure for your computing needs. While it is not cheap, it is generally scalable and guarantees a stable environment. In research there are two approaches to lend computing time and power: either (a) you lend computing time and resources from a commercial provider or you obtain access to a research computing infrastructure. Some countries have built national infrastructures where you can apply for computing time for your research projects. Most academic institutions or departments also have their own computing resources. Popular cloud/HPC services Academic: \ud83c\uddf8\ud83c\uddea UPPMAX \ud83c\uddfa\ud83c\uddf8 Jetstream Commercial: amazon web services google cloud microsoft azure For this tutorial, we'll use Microsoft azure. Connecting to another computer Connecting to another computer is usually done using the SSH protocol, which is an encrypted way to connect over the network. Before connecting to our cloud computers, we need to create them. Note While the cloud is effectively \"someone else's computer\" the way we use commercial cloud infrastructures is by create a virtual computer with the computing resources that we pay for. Note For this course, your instructor will create a virtual instance on the Azure cloud for you Authentication There are two main ways to authenticate to a remote server via SSH: using a password or using a cryptographic key. Using a key prevent people to try to guess your password and since brute-force attacks are very common on machines that have public IPs, we'll use keys. Note How do keys work? The keys we will use to connect to our machine work by pairs: a public key and a private key. Any machine you want to connect to using keys has to contain your public key, while the private key should always stay on your computer. When you try to connect to a machine and the two keys match, you successfully connect! Since your instructor will create a virtual machine for you, he will also provide you with a private key for this machine. Put the private key your instructor gave you in the ~/.ssh folder: mkdir -p ~/.ssh chmod 700 ~/ssh mv ~/Downloads/azure_rsa ~/.ssh chmod 600 ~/.ssh/azure_rsa First connection Now you can connect to the virtual machine that was assigned to you ssh -i .ssh/azure_rsa student@IP_ADDRESS do not forget to replace IP_ADDRESS by your virtual machine ip in the above command! Getting around Now that you are connected to the cloud, there is a few things you should know. For all intents and purposes it is almost like being in the terminal of your own linux machine. All commands we've seen during the unix shell lesson will work you are administrator on your cloud machine. You have the power to break things... ... but do not freak out! the machine is not actually real, so anything you break can be rebuilt in a matter of minutes to exit the virtual machine, press ^D or type exit Transferring files One thing that will happen sooner while working in the cloud is that you will want to transfer files to or from your machine. The command to transfer files over ssh is very similar to cp and is called scp , for secure copy. Copying a file from your computer to the server On your computer, firstly create a file: echo \"I will put that file on my cloud machine!\" > my_file.txt then use scp to transfer the file scp my_file.txt student@IP_ADDRESS:/home/student/ Copying a file from the server to your computer First, remove my_file.txt from your local computer rm my_file.txt then copy it back from the server scp student@IP_ADDRESS:/home/student/my_file.txt .","title":"Cloud Computing"},{"location":"cloud/#introduction-to-cloud-computing","text":"In this lesson you'll learn how to connect and use a linux server. Most bioinformaticians worldwide connect daily to cloud computing services to perform their analyses. There are several reasons for this. Firstly biology - like most other areas of science - is dealing with a deluge of data due to the rapid advancement of data collection methods. It is now common that data collected for an experiment doesn't fit on a researcher's laptop and that the resources needed for running an analysis far exceed a desktop computer's computing power. Secondly the vast majority of research software are developed and released for linux. Most people run MacOS or Windows on their desktop computers and laptop, which makes the installation of some software difficult or at the very least inconvenient.","title":"Introduction to Cloud Computing"},{"location":"cloud/#what-is-the-cloud-anyway","text":"The cloud is basically lots of servers (thing big big computers) stacked together in a giant, powerful infrastructure. You can lend part of this infrastructure for your computing needs. While it is not cheap, it is generally scalable and guarantees a stable environment. In research there are two approaches to lend computing time and power: either (a) you lend computing time and resources from a commercial provider or you obtain access to a research computing infrastructure. Some countries have built national infrastructures where you can apply for computing time for your research projects. Most academic institutions or departments also have their own computing resources.","title":"What is the cloud anyway?"},{"location":"cloud/#popular-cloudhpc-services","text":"Academic: \ud83c\uddf8\ud83c\uddea UPPMAX \ud83c\uddfa\ud83c\uddf8 Jetstream Commercial: amazon web services google cloud microsoft azure For this tutorial, we'll use Microsoft azure.","title":"Popular cloud/HPC services"},{"location":"cloud/#connecting-to-another-computer","text":"Connecting to another computer is usually done using the SSH protocol, which is an encrypted way to connect over the network. Before connecting to our cloud computers, we need to create them. Note While the cloud is effectively \"someone else's computer\" the way we use commercial cloud infrastructures is by create a virtual computer with the computing resources that we pay for. Note For this course, your instructor will create a virtual instance on the Azure cloud for you","title":"Connecting to another computer"},{"location":"cloud/#authentication","text":"There are two main ways to authenticate to a remote server via SSH: using a password or using a cryptographic key. Using a key prevent people to try to guess your password and since brute-force attacks are very common on machines that have public IPs, we'll use keys. Note How do keys work? The keys we will use to connect to our machine work by pairs: a public key and a private key. Any machine you want to connect to using keys has to contain your public key, while the private key should always stay on your computer. When you try to connect to a machine and the two keys match, you successfully connect! Since your instructor will create a virtual machine for you, he will also provide you with a private key for this machine. Put the private key your instructor gave you in the ~/.ssh folder: mkdir -p ~/.ssh chmod 700 ~/ssh mv ~/Downloads/azure_rsa ~/.ssh chmod 600 ~/.ssh/azure_rsa","title":"Authentication"},{"location":"cloud/#first-connection","text":"Now you can connect to the virtual machine that was assigned to you ssh -i .ssh/azure_rsa student@IP_ADDRESS do not forget to replace IP_ADDRESS by your virtual machine ip in the above command!","title":"First connection"},{"location":"cloud/#getting-around","text":"Now that you are connected to the cloud, there is a few things you should know. For all intents and purposes it is almost like being in the terminal of your own linux machine. All commands we've seen during the unix shell lesson will work you are administrator on your cloud machine. You have the power to break things... ... but do not freak out! the machine is not actually real, so anything you break can be rebuilt in a matter of minutes to exit the virtual machine, press ^D or type exit","title":"Getting around"},{"location":"cloud/#transferring-files","text":"One thing that will happen sooner while working in the cloud is that you will want to transfer files to or from your machine. The command to transfer files over ssh is very similar to cp and is called scp , for secure copy.","title":"Transferring files"},{"location":"cloud/#copying-a-file-from-your-computer-to-the-server","text":"On your computer, firstly create a file: echo \"I will put that file on my cloud machine!\" > my_file.txt then use scp to transfer the file scp my_file.txt student@IP_ADDRESS:/home/student/","title":"Copying a file from your computer to the server"},{"location":"cloud/#copying-a-file-from-the-server-to-your-computer","text":"First, remove my_file.txt from your local computer rm my_file.txt then copy it back from the server scp student@IP_ADDRESS:/home/student/my_file.txt .","title":"Copying a file from the server to your computer"},{"location":"git/","text":"Introduction to Git Most of the introduction to Git material can be found at https://software-carpentry.org Many thanks to them for existing! Useful resources Link to the course material from software carpentry reference of concepts and commands seen during the lesson The official git website Comparison of popular git hosting services - Medium article https://choosealicense.com","title":"Introduction to Git"},{"location":"git/#introduction-to-git","text":"Most of the introduction to Git material can be found at https://software-carpentry.org Many thanks to them for existing!","title":"Introduction to Git"},{"location":"git/#useful-resources","text":"Link to the course material from software carpentry reference of concepts and commands seen during the lesson The official git website Comparison of popular git hosting services - Medium article https://choosealicense.com","title":"Useful resources"},{"location":"project_organisation/","text":"Project organization and management Most of the the project organization material can be found at https://software-carpentry.org and http://www.datacarpentry.org Many thanks to them for existing! Structure or architecture of a data science project Some good practice when you will organise your project directory on the server, on the cloud or any other machine where you will compute: Create 3 or 4 different directories within you project directory (use mkdir ): data/ for keeping the raw data results/ for all the outputs from the multiple analyses that you will perform docs/ for all the notes written about the analyses carried out (ex: history > 20180125.logs for the commands executed today) scripts/ for all the scripts that you will use to produce the results Note You should always have the raw data in (at least) one place and not modify them More about data structure and metadata direct link to the tutorial used fo the lesson: Shell genomics: project organisation good practice for the structure of data and metadata of a genomics project: Organisation of genomics project Some extra material: Spreadsheet ecology lesson Exercise This exercise combines the knowledge you have acquired during the unix , git and project organisation lessons. You have designed an experiment where you are studying the species and weight of animals caught in plots in a study area. Data was collected by a third party a deposited in figshare , a public database. Our goals are to download and exploring the data, while keeping an organised project directory that we will version control using git! Set up First we go to our Desktop and create a project directory cd ~/Desktop mkdir 2018_animals cd 2018_animals and initialize 2018_animals as a git repository git init As we saw during the project organization tutorial, it is good practice to separate data, results and scripts. Let us create those three directories mkdir data results scripts Downloading the data First we go to our data directory cd data then we download our data file and give it a more appropriate name wget https://ndownloader.figshare.com/files/2292169 mv 2292169 survey_data.csv Since we'll never modify our raw data file (or at least we do not want to! ) it is safer to remove the writing permissions chmod -w survey_data.csv Additionally since we are now unable to modify it, we do not want to track it in our git repository. We add a .gitignore and tell git to not track the data/ directory nano .gitignore Note what if my data is really big? Usually when you download data that is several gigabytes large, they will usually be compressed. You learnt about compression during the installing software lesson. Let us look at the first few lines of our file: head data/data_joined.csv Our data file is a .csv file, that is a file where fields are separated by commas , . Each row represent an animal that was caught in a plot, and each column contains information about that animal. Question How many animals do we have? wc -l data/data_joined.csv # 34787 data/data_joined.csv It seems that our dataset contains 34787 lines. Since each line is an animals, we caught a grand total of 34787 animals over the course of our study. Our first analysis script we saw when we did the head command that all 10 first plots captured rodents. Question Is rodent the only taxon that we have captured? In our csv file, we can see that \"taxa\" is the 12th column. We can print only that column using the cut command cut -d ',' -f 12 data/data_joined.csv | head We still pipe in in head because we do not want to print 34787 line to our screen. Additionally head makes us notice that we still have the column header printed out cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | uniq -c But while uniq is supposed to count all occurrence of a word, it only count similar adjacent occurrences. Before counting, we need to sort our input: cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | sort | uniq -c We see that although we caught a vast majority of rodents, we also caught reptiles, birds and rabbits! Now that we have a working one-liner, let us put it into a script nano scripts/taxa_count.sh and write # script that prints the count of species for csv files cut -d ',' -f 12 \"$1\" | tail -n +2 | sort | uniq -c Keeping track of things Now keep track of your script in git git add scripts/taxa_count.sh git commit -m 'added taxa_count' as well as your gitignore git add .gitignore git commit -m 'added gitignore' Saving the result bash scripts/taxa_count.sh data/data_joined.csv > results/taxa_count.txt cat results/taxa_count.txt git add results/taxa_count.txt git commit -m 'added results of taxa_count.sh' Improving our script We would also like to know the distribution of the numbers of animals caught in plots each year. The year is the 4th column in our dataset and our script, in its current state, always selects the 12th columns of a file. We can change our script to make it flexible so that the user can chose which columns they wishes to work on. nano scripts/taxa_count.sh # script that prints the count of occurrence in one column for csv files cut -d ',' -f \"$2\" \"$1\" | tail -n +2 | sort | uniq -c Now it doesn't make much sense to have it named taxa_count.sh mv scripts/taxa_count.sh scripts/column_count.sh and let us not forget to keep track of our changes in git! git add -A git commit -m 'made script more flexible about which column to cut on' Question which year did we catch the most animals? try to answer programmatically. Question save the sorted output to a file in the results directory and keep track of it in git. Investigating further We'd like to refine our animal count and knowing how many animals of each taxon were captured every year we can use cut on several columns like this: cut -d ',' -f 4,12 \"data/data_joined.csv\" | tail -n +2 | sort | uniq -c Now that we are ahhpy with our one-liner, let us save it in a script: nano scripts/taxa_per_year.sh then save the output to results/taxa_per_year.txt bash scripts/taxa_per_year.sh > results/taxa_per_year.txt Question Which year was the first reptile captured? The next step would be to refine our analysis by year. We will save one individual output for each year count The seq command To perform what we want to do, we need to be able to loop over the years. The seq command can help us with that. First we try seq 1 10 then seq 1997 2002 and what about the span of years we are interested in? seq 1977 2002 Great! So now does it work with a for loop? for year in $(seq 1977 2002) do echo $year done It does! Before doing our analysis on each year, we still have to figure out how to do it on one year. grep 1998 results/taxa_per_year.txt \"Grepping\" the year seems to work. Now we need to save it into a file containing the year First let's create a directory where to store our results mkdir results/years and we try to redirect our yearly count into a file grep 1998 results/taxa_per_year.txt > results/years/1998-count.txt bash cat results/years/1998-count.txt It seems to have worked. Now with the loop for year in $(seq 1977 2002) do grep $year results/taxa_per_year.txt > results/years/$year-count.txt done ls results/years Question Put your loop in a script, and commit everything with git","title":"Project organisation"},{"location":"project_organisation/#project-organization-and-management","text":"Most of the the project organization material can be found at https://software-carpentry.org and http://www.datacarpentry.org Many thanks to them for existing!","title":"Project organization and management"},{"location":"project_organisation/#structure-or-architecture-of-a-data-science-project","text":"Some good practice when you will organise your project directory on the server, on the cloud or any other machine where you will compute: Create 3 or 4 different directories within you project directory (use mkdir ): data/ for keeping the raw data results/ for all the outputs from the multiple analyses that you will perform docs/ for all the notes written about the analyses carried out (ex: history > 20180125.logs for the commands executed today) scripts/ for all the scripts that you will use to produce the results Note You should always have the raw data in (at least) one place and not modify them","title":"Structure or architecture of a data science project"},{"location":"project_organisation/#more-about-data-structure-and-metadata","text":"direct link to the tutorial used fo the lesson: Shell genomics: project organisation good practice for the structure of data and metadata of a genomics project: Organisation of genomics project Some extra material: Spreadsheet ecology lesson","title":"More about data structure and metadata"},{"location":"project_organisation/#exercise","text":"This exercise combines the knowledge you have acquired during the unix , git and project organisation lessons. You have designed an experiment where you are studying the species and weight of animals caught in plots in a study area. Data was collected by a third party a deposited in figshare , a public database. Our goals are to download and exploring the data, while keeping an organised project directory that we will version control using git!","title":"Exercise"},{"location":"project_organisation/#set-up","text":"First we go to our Desktop and create a project directory cd ~/Desktop mkdir 2018_animals cd 2018_animals and initialize 2018_animals as a git repository git init As we saw during the project organization tutorial, it is good practice to separate data, results and scripts. Let us create those three directories mkdir data results scripts","title":"Set up"},{"location":"project_organisation/#downloading-the-data","text":"First we go to our data directory cd data then we download our data file and give it a more appropriate name wget https://ndownloader.figshare.com/files/2292169 mv 2292169 survey_data.csv Since we'll never modify our raw data file (or at least we do not want to! ) it is safer to remove the writing permissions chmod -w survey_data.csv Additionally since we are now unable to modify it, we do not want to track it in our git repository. We add a .gitignore and tell git to not track the data/ directory nano .gitignore Note what if my data is really big? Usually when you download data that is several gigabytes large, they will usually be compressed. You learnt about compression during the installing software lesson. Let us look at the first few lines of our file: head data/data_joined.csv Our data file is a .csv file, that is a file where fields are separated by commas , . Each row represent an animal that was caught in a plot, and each column contains information about that animal. Question How many animals do we have? wc -l data/data_joined.csv # 34787 data/data_joined.csv It seems that our dataset contains 34787 lines. Since each line is an animals, we caught a grand total of 34787 animals over the course of our study.","title":"Downloading the data"},{"location":"project_organisation/#our-first-analysis-script","text":"we saw when we did the head command that all 10 first plots captured rodents. Question Is rodent the only taxon that we have captured? In our csv file, we can see that \"taxa\" is the 12th column. We can print only that column using the cut command cut -d ',' -f 12 data/data_joined.csv | head We still pipe in in head because we do not want to print 34787 line to our screen. Additionally head makes us notice that we still have the column header printed out cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | uniq -c But while uniq is supposed to count all occurrence of a word, it only count similar adjacent occurrences. Before counting, we need to sort our input: cut -d ',' -f 12 data/data_joined.csv | tail -n +2 | sort | uniq -c We see that although we caught a vast majority of rodents, we also caught reptiles, birds and rabbits! Now that we have a working one-liner, let us put it into a script nano scripts/taxa_count.sh and write # script that prints the count of species for csv files cut -d ',' -f 12 \"$1\" | tail -n +2 | sort | uniq -c","title":"Our first analysis script"},{"location":"project_organisation/#keeping-track-of-things","text":"Now keep track of your script in git git add scripts/taxa_count.sh git commit -m 'added taxa_count' as well as your gitignore git add .gitignore git commit -m 'added gitignore'","title":"Keeping track of things"},{"location":"project_organisation/#saving-the-result","text":"bash scripts/taxa_count.sh data/data_joined.csv > results/taxa_count.txt cat results/taxa_count.txt git add results/taxa_count.txt git commit -m 'added results of taxa_count.sh'","title":"Saving the result"},{"location":"project_organisation/#improving-our-script","text":"We would also like to know the distribution of the numbers of animals caught in plots each year. The year is the 4th column in our dataset and our script, in its current state, always selects the 12th columns of a file. We can change our script to make it flexible so that the user can chose which columns they wishes to work on. nano scripts/taxa_count.sh # script that prints the count of occurrence in one column for csv files cut -d ',' -f \"$2\" \"$1\" | tail -n +2 | sort | uniq -c Now it doesn't make much sense to have it named taxa_count.sh mv scripts/taxa_count.sh scripts/column_count.sh and let us not forget to keep track of our changes in git! git add -A git commit -m 'made script more flexible about which column to cut on' Question which year did we catch the most animals? try to answer programmatically. Question save the sorted output to a file in the results directory and keep track of it in git.","title":"Improving our script"},{"location":"project_organisation/#investigating-further","text":"We'd like to refine our animal count and knowing how many animals of each taxon were captured every year we can use cut on several columns like this: cut -d ',' -f 4,12 \"data/data_joined.csv\" | tail -n +2 | sort | uniq -c Now that we are ahhpy with our one-liner, let us save it in a script: nano scripts/taxa_per_year.sh then save the output to results/taxa_per_year.txt bash scripts/taxa_per_year.sh > results/taxa_per_year.txt Question Which year was the first reptile captured? The next step would be to refine our analysis by year. We will save one individual output for each year count","title":"Investigating further"},{"location":"project_organisation/#the-seq-command","text":"To perform what we want to do, we need to be able to loop over the years. The seq command can help us with that. First we try seq 1 10 then seq 1997 2002 and what about the span of years we are interested in? seq 1977 2002 Great! So now does it work with a for loop? for year in $(seq 1977 2002) do echo $year done It does! Before doing our analysis on each year, we still have to figure out how to do it on one year. grep 1998 results/taxa_per_year.txt \"Grepping\" the year seems to work. Now we need to save it into a file containing the year First let's create a directory where to store our results mkdir results/years and we try to redirect our yearly count into a file grep 1998 results/taxa_per_year.txt > results/years/1998-count.txt bash cat results/years/1998-count.txt It seems to have worked. Now with the loop for year in $(seq 1977 2002) do grep $year results/taxa_per_year.txt > results/years/$year-count.txt done ls results/years Question Put your loop in a script, and commit everything with git","title":"The seq command"},{"location":"proteins/","text":"Introduction to protein sequences and structures analysis ToolBox that could be useful for protein sequences analysis: http://expasy.org/ http://www.uniprot.org/ http://www.ebiokit.eu/ http://npsa-pbil.ibcp.fr http://blast.ncbi.nlm.nih.gov/Blast.cgi https://www.ebi.ac.uk/interpro http://www.rcsb.org/pdb After cloning and sequencing of coding DNA, the sequence of the X protein had been determined. The sequence of X is given here: LAAVSVDCSEYPKPACTLEYRPLCGSDNKTYGNKCNFCNAVVESNGTLTLSHFGKC In normal conditions, this X protein is expressed but we have no idea about it function. The goal of this practical work is to collect the maximum of information about structure and function of the X protein. I - Search Patterns, Profiles A way to identify the function of X is to look if it contains signatures (pattern) of a function or a protein family. 2 options: http://prosite.expasy.org/scanprosite/ NPS@ and follow the link \"ProScan: scan a sequence for sites/signatures against PROSITE database\" (activate: Include documentation in result file). Question Which signature(s) could you identify? Which specific features in this protein? Try to change the parameters and comment the results. Note InterPro gives a summary of several methods. You can find it at the EBI . Keep the signatures that could attest the function in your notepad. What do you think about the function of X? II - Search homolog proteins with BLAST Go to the NCBI BLAST page Choose the Protein Blast (blastp) Paste your sequence Select the Swissprot database Question Did you identify homologs? What are their function(s)? III - Multiple sequences alignment Select several homolog sequences from the Blast results. Perform a multiple sequence alignment (MSA) of these sequence using Clustal Omega for example Try other MSA tools (for example Tcoffee and Muscle) Question Do you observe differences between the results obtained from different algorithms? What can you observe in these MSAs? Info : You could also retrieve the selected sequences in Fasta format and perform MSAs elsewhere Clustal Omega and Muscle: available in Seaview alignment viewer Tcoffee: http://tcoffee.vital-it.ch/apps/tcoffee/index.html Other tools: http://expasy.org/genomics/sequence_alignment IV - The Y protein Another experiment had shown that the X protein was interacting specifically with another protein: Y. After purification of the active Y protein, from the complex, a partial sequence of Y was obtained (by protein extremity sequencing). The corresponding peptide could be: ISGGD or ISGGN 1. Identification of the Y sequence using PROSITE patterns Design the pattern (regular expression) corresponding to these peptides. Search the sequences containing this pattern in SwissProt using PATTERN SEARCH at SIB or PATTINPROT at NPS@. If needed, use the help to design your pattern. Question How many results do you get? How can you identify the right one? Once the Y protein sequence identified, copy the FASTA sequence in your notepad. 2. Composition analysis After purification of the Y active protein, the amino-acid composition has been determined (% of each aa in the protein) and is given in the following table: A 8.11 F 2.70 L 3.78 R 4.32 X 0 B 0 G 17.30 M 1.08 S 11.89 Y 5.41 C 2.16 H 1.08 N 5.41 T 15.14 Z 0 D 3.78 I 3.78 P 2.70 V 7.57 E 1.08 K 0.54 Q 1.08 W 1.08 Compute the composition of the sequence that you retrieve. Use PROTPARAM or the tool 'Amino-acid composition' at NPS@ Compare this computed composition with the composition of Y experimentally determined. Question Do you observe differences? Explain. 3. Search pattern in Y Once the correct sequence of Y obtained, keep it in your notepad, you will need it for the following analyses. Question Identify the signatures (motifs, Pfam profiles) of Y using PROSCAN and/or Interpro. 4. Identification of homologs to Y Use NCBI BLASTP or NPS@ BLASTP against SwissProt database to search sequences similar to Y. Use PSI-BLAST (with SwissProt) to see if you can detect more distant sequences. Select sequences from BLAST and/or PSI-BLAST results to perform a multiple sequence alignment. Question Did you observe difference in the results of BLAST and PSI-BLAST? Comment. Propose a strategy to retrieve all the proteins having the same catalytic activity as Y protein. V - Secondary structure prediction for X and Y Go to the consensus secondary structure prediction page at NPS@. Analyze the secondary structure of the protein Y. Include secondary structure predictions by methods (DPM, GOR1, PREDATOR, SIMPA96). Question Conclude on the organization of secondary structures. Perform the same analysis for X protein. VI - Comparison with solved structures 1. The Z protein The structure of a protein Z has just been published. The sequence of protein Z is shown below: IAGGEAITTGGSRCSLGFNVSVNGVAHALTAGHCTNISASWSIGTRTGTSFPNNDYGIIRHSNPAAANGRVYLYNGSYQD ITTAGNAFVGQAVQRSGSTTGLRSGSVTGLNATVNYGSSGIVYGMIQTNVCAQPGDSGGSLFAGSTALGLTSGGSGNCRT GGTTFYQPVTEALSAYGATVL Question Could you use this information for the study of protein Y? Make your own analysis. 2. Find the correct structures Download and install Deep-View - SwissPDBViewer . You can find the tutorial and user guide of DeepView here . Download to the archive PDB_files_part6.zip and unzip it. You might find 8 PDB files in the directory. Open them with DeepView. Display the secondary structure representation mode (see part VII-A-5 and/or the user guide). Question Try to identify the structures corresponding to X and Y proteins. VII - Tridimensional protein structure: Play with 3D structures using SwissPDBViewer (DeepView) Go to the Protein Data Bank Search and download the following PDB files: 1CRN, 1LDM. You will visualize these protein structures using DeepView A - Analyze protein structures with DeepView 1. Load a 3D structure File => Open Choose the 1CRN.pdb file that you have downloaded from the PDB. 2. Visualize the number of chains Is it only the protein or can we find ligands? Is it a monomer or a polymer? 3. Visualize the general shape Try to get the actual space taken by the molecule. You need to use the control panel and use the ':v' column to activate the space-filling spheres representation (+ menu Display > Render in solid 3D). Test also the Slab mode to visualize the space within the molecule: Display > Slab 4. Display a distance between 2 atoms, angle between 3 atoms Use the graphical panel. You can now measure the real dimensions of your protein 5. Visualize secondary structure elements In the control panel, activate \"ribbon\" (rbn). You can also color the molecule by secondary structures. 6. Visualize ligands (if there is any) Select and color them. You could also remove the rest, or better, have a look at the residues that are around those ligands (radius function in the graphical panel). 7. Analysis of other protein structures The teacher will give PDB codes of other structures to analyze. Choose DeepView or Rasmol/Jmol to do so, that is up to you! B - Optional: if you want to use RasMol/Jmol 1. Load a 3D structure File => Open Choose the 1CRN.pdb file that you have downloaded from the PDB. HELP SECTION FOR RASMOL Molecule main moves with the mouse: Left button: XY rotation Left button + Shift: Zoom Right button: Translation Right button + Shift: Z rotation Keep the graphical window and the command (text) window on your screen (> \u200b\u200bis a command to type in the text window). For each selection (SELECT command), the number of selected atoms appears in the text window. After you can apply an action to be able to visualize the elements that you have selected (e.g. COLOR GREEN). Ctrl+Z does not exist in Rasmol. You can type the command RESET. If you want to come back in a standard representation of your molecule, type: SELECT ALL CPK => This will reset previous actions on representation modes (but keep colors). CPK: space-filling spheres representation COLOR CPK: colors \\'atom\\' objects by the atom (element) type Help for Jmol: A lot of \"actions\" (color, selection...) are available by right clicking on the main screen To get the terminal window: menu File > Console 2. Example: visualize the disulfide bonds Type in the text window SELECT CYS The text window \\\"answers\\\" 36 atoms selected (selected cysteine's atoms) COLOR GREEN Observe the graphics window. RESTRICT CYS Compare with the SELECT command Highlight the disulfide bonds: SSBONDS COLOR YELLOW SSBONDS 75 COLOR CPK 3. Visualize secondary structure elements SSBONDS OFF (remove SS bonds) SELECT ALL CARTOONS COLOR STRUCTURE 4. Display a distance between 2 atoms Activate the compute distance mode typing: SET PICKING DISTANCE Then, you can click the 2 atoms. You can display angle values typing: SET PICKING ANGLE Then pick the 3 atoms 5. Other useful commands SHOW SEQUENCE SHOW INFO SELECT ALL CPK ON RESTRICT NOT HOH (remove water molecules) CPK OFF HBONDS SELECT CYCLIC AND NOT PRO STEREO ON Try them to better understand the Rasmol command language. 6. Store a command script and reload it Repeat the actions described in paragraph 2 WRITE SCRIPT MY_SCRIPT.SC Exit the software (File => Quit) Restart the software SOURCE MY_SCRIPT.SC 7. Select the atoms in a sphere File => Close Load the file 1LDM.pdb Discover and analyze the molecule (number of channels, ligands, etc .) To select all the atoms in a 3\u00c5 radius sphere centered on a ligand ( e.g. NAD) SELECT ALL COLOR CHAIN SELECT WITHIN (3.0, NAD) CPK Option => Slab Mode (comment).","title":"Proteins"},{"location":"proteins/#introduction-to-protein-sequences-and-structures-analysis","text":"ToolBox that could be useful for protein sequences analysis: http://expasy.org/ http://www.uniprot.org/ http://www.ebiokit.eu/ http://npsa-pbil.ibcp.fr http://blast.ncbi.nlm.nih.gov/Blast.cgi https://www.ebi.ac.uk/interpro http://www.rcsb.org/pdb After cloning and sequencing of coding DNA, the sequence of the X protein had been determined. The sequence of X is given here: LAAVSVDCSEYPKPACTLEYRPLCGSDNKTYGNKCNFCNAVVESNGTLTLSHFGKC In normal conditions, this X protein is expressed but we have no idea about it function. The goal of this practical work is to collect the maximum of information about structure and function of the X protein.","title":"Introduction to protein sequences and structures analysis"},{"location":"proteins/#i-search-patterns-profiles","text":"A way to identify the function of X is to look if it contains signatures (pattern) of a function or a protein family. 2 options: http://prosite.expasy.org/scanprosite/ NPS@ and follow the link \"ProScan: scan a sequence for sites/signatures against PROSITE database\" (activate: Include documentation in result file). Question Which signature(s) could you identify? Which specific features in this protein? Try to change the parameters and comment the results. Note InterPro gives a summary of several methods. You can find it at the EBI . Keep the signatures that could attest the function in your notepad. What do you think about the function of X?","title":"I - Search Patterns, Profiles"},{"location":"proteins/#ii-search-homolog-proteins-with-blast","text":"Go to the NCBI BLAST page Choose the Protein Blast (blastp) Paste your sequence Select the Swissprot database Question Did you identify homologs? What are their function(s)?","title":"II - Search homolog proteins with BLAST"},{"location":"proteins/#iii-multiple-sequences-alignment","text":"Select several homolog sequences from the Blast results. Perform a multiple sequence alignment (MSA) of these sequence using Clustal Omega for example Try other MSA tools (for example Tcoffee and Muscle) Question Do you observe differences between the results obtained from different algorithms? What can you observe in these MSAs? Info : You could also retrieve the selected sequences in Fasta format and perform MSAs elsewhere Clustal Omega and Muscle: available in Seaview alignment viewer Tcoffee: http://tcoffee.vital-it.ch/apps/tcoffee/index.html Other tools: http://expasy.org/genomics/sequence_alignment","title":"III - Multiple sequences alignment"},{"location":"proteins/#iv-the-y-protein","text":"Another experiment had shown that the X protein was interacting specifically with another protein: Y. After purification of the active Y protein, from the complex, a partial sequence of Y was obtained (by protein extremity sequencing). The corresponding peptide could be: ISGGD or ISGGN","title":"IV - The Y protein"},{"location":"proteins/#1-identification-of-the-y-sequence-using-prosite-patterns","text":"Design the pattern (regular expression) corresponding to these peptides. Search the sequences containing this pattern in SwissProt using PATTERN SEARCH at SIB or PATTINPROT at NPS@. If needed, use the help to design your pattern. Question How many results do you get? How can you identify the right one? Once the Y protein sequence identified, copy the FASTA sequence in your notepad.","title":"1. Identification of the Y sequence using PROSITE patterns"},{"location":"proteins/#2-composition-analysis","text":"After purification of the Y active protein, the amino-acid composition has been determined (% of each aa in the protein) and is given in the following table: A 8.11 F 2.70 L 3.78 R 4.32 X 0 B 0 G 17.30 M 1.08 S 11.89 Y 5.41 C 2.16 H 1.08 N 5.41 T 15.14 Z 0 D 3.78 I 3.78 P 2.70 V 7.57 E 1.08 K 0.54 Q 1.08 W 1.08 Compute the composition of the sequence that you retrieve. Use PROTPARAM or the tool 'Amino-acid composition' at NPS@ Compare this computed composition with the composition of Y experimentally determined. Question Do you observe differences? Explain.","title":"2. Composition analysis"},{"location":"proteins/#3-search-pattern-in-y","text":"Once the correct sequence of Y obtained, keep it in your notepad, you will need it for the following analyses. Question Identify the signatures (motifs, Pfam profiles) of Y using PROSCAN and/or Interpro.","title":"3. Search pattern in Y"},{"location":"proteins/#4-identification-of-homologs-to-y","text":"Use NCBI BLASTP or NPS@ BLASTP against SwissProt database to search sequences similar to Y. Use PSI-BLAST (with SwissProt) to see if you can detect more distant sequences. Select sequences from BLAST and/or PSI-BLAST results to perform a multiple sequence alignment. Question Did you observe difference in the results of BLAST and PSI-BLAST? Comment. Propose a strategy to retrieve all the proteins having the same catalytic activity as Y protein.","title":"4. Identification of homologs to Y"},{"location":"proteins/#v-secondary-structure-prediction-for-x-and-y","text":"Go to the consensus secondary structure prediction page at NPS@. Analyze the secondary structure of the protein Y. Include secondary structure predictions by methods (DPM, GOR1, PREDATOR, SIMPA96). Question Conclude on the organization of secondary structures. Perform the same analysis for X protein.","title":"V - Secondary structure prediction for X and Y"},{"location":"proteins/#vi-comparison-with-solved-structures","text":"","title":"VI - Comparison with solved structures"},{"location":"proteins/#1-the-z-protein","text":"The structure of a protein Z has just been published. The sequence of protein Z is shown below: IAGGEAITTGGSRCSLGFNVSVNGVAHALTAGHCTNISASWSIGTRTGTSFPNNDYGIIRHSNPAAANGRVYLYNGSYQD ITTAGNAFVGQAVQRSGSTTGLRSGSVTGLNATVNYGSSGIVYGMIQTNVCAQPGDSGGSLFAGSTALGLTSGGSGNCRT GGTTFYQPVTEALSAYGATVL Question Could you use this information for the study of protein Y? Make your own analysis.","title":"1. The Z protein"},{"location":"proteins/#2-find-the-correct-structures","text":"Download and install Deep-View - SwissPDBViewer . You can find the tutorial and user guide of DeepView here . Download to the archive PDB_files_part6.zip and unzip it. You might find 8 PDB files in the directory. Open them with DeepView. Display the secondary structure representation mode (see part VII-A-5 and/or the user guide). Question Try to identify the structures corresponding to X and Y proteins.","title":"2. Find the correct structures"},{"location":"proteins/#vii-tridimensional-protein-structure-play-with-3d-structures-using-swisspdbviewer-deepview","text":"Go to the Protein Data Bank Search and download the following PDB files: 1CRN, 1LDM. You will visualize these protein structures using DeepView","title":"VII - Tridimensional protein structure: Play with 3D structures using SwissPDBViewer (DeepView)"},{"location":"proteins/#a-analyze-protein-structures-with-deepview","text":"","title":"A - Analyze protein structures with DeepView"},{"location":"proteins/#1-load-a-3d-structure","text":"File => Open Choose the 1CRN.pdb file that you have downloaded from the PDB.","title":"1. Load a 3D structure"},{"location":"proteins/#2-visualize-the-number-of-chains","text":"Is it only the protein or can we find ligands? Is it a monomer or a polymer?","title":"2. Visualize the number of chains"},{"location":"proteins/#3-visualize-the-general-shape","text":"Try to get the actual space taken by the molecule. You need to use the control panel and use the ':v' column to activate the space-filling spheres representation (+ menu Display > Render in solid 3D). Test also the Slab mode to visualize the space within the molecule: Display > Slab","title":"3. Visualize the general shape"},{"location":"proteins/#4-display-a-distance-between-2-atoms-angle-between-3-atoms","text":"Use the graphical panel. You can now measure the real dimensions of your protein","title":"4. Display a distance between 2 atoms, angle between 3 atoms"},{"location":"proteins/#5-visualize-secondary-structure-elements","text":"In the control panel, activate \"ribbon\" (rbn). You can also color the molecule by secondary structures.","title":"5. Visualize secondary structure elements"},{"location":"proteins/#6-visualize-ligands-if-there-is-any","text":"Select and color them. You could also remove the rest, or better, have a look at the residues that are around those ligands (radius function in the graphical panel).","title":"6. Visualize ligands (if there is any)"},{"location":"proteins/#7-analysis-of-other-protein-structures","text":"The teacher will give PDB codes of other structures to analyze. Choose DeepView or Rasmol/Jmol to do so, that is up to you!","title":"7. Analysis of other protein structures"},{"location":"proteins/#b-optional-if-you-want-to-use-rasmoljmol","text":"","title":"B - Optional: if you want to use RasMol/Jmol"},{"location":"proteins/#1-load-a-3d-structure_1","text":"File => Open Choose the 1CRN.pdb file that you have downloaded from the PDB. HELP SECTION FOR RASMOL Molecule main moves with the mouse: Left button: XY rotation Left button + Shift: Zoom Right button: Translation Right button + Shift: Z rotation Keep the graphical window and the command (text) window on your screen (> \u200b\u200bis a command to type in the text window). For each selection (SELECT command), the number of selected atoms appears in the text window. After you can apply an action to be able to visualize the elements that you have selected (e.g. COLOR GREEN). Ctrl+Z does not exist in Rasmol. You can type the command RESET. If you want to come back in a standard representation of your molecule, type: SELECT ALL CPK => This will reset previous actions on representation modes (but keep colors). CPK: space-filling spheres representation COLOR CPK: colors \\'atom\\' objects by the atom (element) type Help for Jmol: A lot of \"actions\" (color, selection...) are available by right clicking on the main screen To get the terminal window: menu File > Console","title":"1. Load a 3D structure"},{"location":"proteins/#2-example-visualize-the-disulfide-bonds","text":"Type in the text window SELECT CYS The text window \\\"answers\\\" 36 atoms selected (selected cysteine's atoms) COLOR GREEN Observe the graphics window. RESTRICT CYS Compare with the SELECT command Highlight the disulfide bonds: SSBONDS COLOR YELLOW SSBONDS 75 COLOR CPK","title":"2. Example: visualize the disulfide bonds"},{"location":"proteins/#3-visualize-secondary-structure-elements","text":"SSBONDS OFF (remove SS bonds) SELECT ALL CARTOONS COLOR STRUCTURE","title":"3. Visualize secondary structure elements"},{"location":"proteins/#4-display-a-distance-between-2-atoms","text":"Activate the compute distance mode typing: SET PICKING DISTANCE Then, you can click the 2 atoms. You can display angle values typing: SET PICKING ANGLE Then pick the 3 atoms","title":"4. Display a distance between 2 atoms"},{"location":"proteins/#5-other-useful-commands","text":"SHOW SEQUENCE SHOW INFO SELECT ALL CPK ON RESTRICT NOT HOH (remove water molecules) CPK OFF HBONDS SELECT CYCLIC AND NOT PRO STEREO ON Try them to better understand the Rasmol command language.","title":"5. Other useful commands"},{"location":"proteins/#6-store-a-command-script-and-reload-it","text":"Repeat the actions described in paragraph 2 WRITE SCRIPT MY_SCRIPT.SC Exit the software (File => Quit) Restart the software SOURCE MY_SCRIPT.SC","title":"6. Store a command script and reload it"},{"location":"proteins/#7-select-the-atoms-in-a-sphere","text":"File => Close Load the file 1LDM.pdb Discover and analyze the molecule (number of channels, ligands, etc .) To select all the atoms in a 3\u00c5 radius sphere centered on a ligand ( e.g. NAD) SELECT ALL COLOR CHAIN SELECT WITHIN (3.0, NAD) CPK Option => Slab Mode (comment).","title":"7. Select the atoms in a sphere"},{"location":"seq_tech/","text":"Sequencing Technologies Exercise You will be divided into small groups. Each group will be given a small research project. Each group will present a short presentation (5-7 minutes) answering the following question: Which sequencing technology(-ies) would you use for your experiment and why? Litterature Coming of age: ten years of next-generation sequencing technologies","title":"Sequencing Technologies"},{"location":"seq_tech/#sequencing-technologies","text":"","title":"Sequencing Technologies"},{"location":"seq_tech/#exercise","text":"You will be divided into small groups. Each group will be given a small research project. Each group will present a short presentation (5-7 minutes) answering the following question: Which sequencing technology(-ies) would you use for your experiment and why?","title":"Exercise"},{"location":"seq_tech/#litterature","text":"Coming of age: ten years of next-generation sequencing technologies","title":"Litterature"},{"location":"software/","text":"Installing software Bioinformatics is a relatively new (It's younger that Erik!) and fast-progressing field. Therefore new software as well as new versions of existing software are released on a regular basis. During this course as well as during your future career as a bioinformatician ( ;-) ) you will be confronted quite often to the installation of new software on UNIX platforms (i.e. the server you are using at the moment) Compiled and Interpreted languages Programming languages in the bioinformatics world - and in general - can be separated in two categories: intepreted languages, and compiled languages. While with interpreted languages you write scripts, and execute them (as we saw with the bash scripts during the UNIX lesson) it is different for compiled languages: an extra step is required Compilation As from Wikipedia , compilation is the translation of source code into object code by a compiler. That's right. The extra step required by compiled languages is translating the source code, that is the lines of code the programmer(s) wrote into a language that your computer understand better, usually binary (1s and 0s). The big advantage of compiled languages is that they are much faster than interpreted languages. However, programming in them is usually slower and more difficult than in interpreted languages. Using them or not for a software project is a trade-off between development-time, and how much faster your software could run if it was programmed using a compiled language. The most popular compiled language is the C programming language, which Linux is mainly written in. Package Managers All modern linux distributions come with a package manager , i.e. a tool that automates installation of software. In most cases the software manager download already compiled binaries and installs them in your system. We'll see how it works in a moment Let us install our first package! The package manager for Ubuntu is called APT . Like most package managers, the syntax will look like this: [package_manager] [action] [package_name] We'll use apt to install a local version of ncbi-blast that you've use previously. First we search if the package is available apt search ncbi-blast There seems to be two versions of it. The legacy version is probably outdated, so let us investigate the other one apt show ncbi-blast+ It seems to be what we are looking for, we install it with: apt install ncbi-blast+ Question Did it work? What could have been wrong? You should have gotten an error message asking if you are root . The user root is the most powerful user in a linux system and usually has extra rights that a regular user does not have. To install software in the default system location with apt, you have to have special permissions. We can \"borrow\" those permissions from root by prefixing our command with sudo . sudo apt install ncbi-blast+ Now if you execute blastn -help it should print the (rather long) error message of the blastn command. Question Why does blast has different executable? What is the difference between blastn and blastp? Downloading and unpacking Although most popular software can be installed with your distribution's package manager, sometimes (especially in some fast-growing areas of bioinformatics) the software you want isn't available through a package manager. We'll install spades , a popular genome assembly tool. Let's imagine it is not available in the apt sources. We'd have to: download the source code compile the software move it at the right place on our system Which is quite cumbersome, especially the compilation. Luckily, it is fairly common for developers to make linux binaries - that is compiled version of the software - already available for download. First let us create a directory for all our future installs: mkdir -p ~/install cd ~/install The spades binaries are available on their website, http://cab.spbu.ru/software/spades/ Download them with wget http://cab.spbu.ru/files/release3.11.1/SPAdes-3.11.1-Linux.tar.gz and uncompress tar xvf SPAdes-3.11.1-Linux.tar.gz cd SPAdes-3.11.1-Linux/bin/ and now if we execute spades.py ./spades.py we get the help of the spades assembler! A minor inconvenience is that right now pwd # /home/hadrien/install/SPAdes-3.11.1-Linux/bin we have to always go to this directory to run spades.py , or call the software with the full path. We'd like to be able to execute spades from anywhere, like we do with ls and cd . In most linux distributions, which directory can contain software that are executed from anywhere is defined by an environment variable: $PATH Let us take a look: echo $PATH # /home/hadrien/bin:/home/hadrien/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin To make spades.py available from anywhere we have to put it in one of the above locations. Note When apt installs software it usually places it in /usr/bin , which requires administration privileges. This is why we needed sudo for installing packages earlier. mkdir -p ~/.local/bin mv * ~/.local/bin/ Et voil\u00e0! Now you can execute spades.py from anywhere! Installing from source For some bioinformatics software, binaries are not available. In that case you have to download the source code, and compile it yourself for your system. This is the case of samtools per example. samtools is one of the most popular bioinformatics software and allows you to deal with bam and sam files (more about that later) We'll need a few things to be able to compile samtools, notably make and a C compiler, gcc sudo apt install make gcc samtools also need some libraries that are not installed by default on an ubuntu system. sudo apt install libncurses5-dev libbz2-dev liblzma-dev libcurl4-gnutls-dev Now we can download and unpack the source code: cd ~/install wget https://github.com/samtools/samtools/releases/download/1.6/samtools-1.6.tar.bz2 tar xvf samtools-1.6.tar.bz2 cd samtools-1.6 Compiling software written in C usually follows the same 3 steps. ./configure to configure the compilation options to our machine architecture we run make to compile the software we run make install to move the compiled binaries into a location in the $PATH ./configure make make install Warning Did make install succeed? Why not? As we saw before, we need sudo to install packages to system locations with apt . make install follows the same principle and tries by default to install software in /usr/bin We can change that default behavior by passing options to configure , but first we have to clean our installation: make clean than we can run configure, make and make install again ./configure --prefix=/home/$(whoami)/.local/ make make install samtools Question The bwa source code is available on github, a popular code sharing platform (more on this in the git lesson!). Navigate to https://github.com/lh3/bwa then in release copy the link behind bwa-0.7.17.tar.bz2 - Install bwa! Installing python packages While compiled languages are faster than interpreted languages, they are usually harder to learn, code in and debug. For theses reasons you'll often find many bioinformatics packages written in interpreted languages such as python or ruby . While historically it has been a pain to install software written in interpreted languages, most modern languages now come with their own package managers! For example: Python has pip Ruby has gem Javascript has npm ... Most of theses package managers have similar syntaxes. We will focus on python here since it's one of the most popular languages in bioinformatics. Note You will notice the absence of R here. R is mostly used interactively and installing packages in R will be part of the R part of the course. Your ubuntu comes with an old version of python. We start with installing a newer one cd ~/install wget https://www.python.org/ftp/python/3.6.4/Python-3.6.4.tar.xz tar xvf Python-3.6.4.tar.xz cd Python-3.6.4 ./configure --prefix=/home/$(whoami)/.local/ make -j2 make install Question What does the make option -j2 do? which python3 which pip3 We now have the newest python installed. Let us install our first python package pip3 install multiqc it should take a while and install multiqc as well as all the necessary dependencies. to see if multiqc was properly installed: multiqc -h Exercises During the following weeks we'll use a lot of different bioinformatics software to perform a variety of tasks. Tip Most software come with a file named INSTALL or README . Such file usually contains instructions on how to install! Note unless indicated otherwise, try with apt first Note do not hesitate to ask your teacher for help! Let's install a few: fastqc scythe sickle bowtie2 megahit quast prokka","title":"Installing software"},{"location":"software/#installing-software","text":"Bioinformatics is a relatively new (It's younger that Erik!) and fast-progressing field. Therefore new software as well as new versions of existing software are released on a regular basis. During this course as well as during your future career as a bioinformatician ( ;-) ) you will be confronted quite often to the installation of new software on UNIX platforms (i.e. the server you are using at the moment)","title":"Installing software"},{"location":"software/#compiled-and-interpreted-languages","text":"Programming languages in the bioinformatics world - and in general - can be separated in two categories: intepreted languages, and compiled languages. While with interpreted languages you write scripts, and execute them (as we saw with the bash scripts during the UNIX lesson) it is different for compiled languages: an extra step is required","title":"Compiled and Interpreted languages"},{"location":"software/#compilation","text":"As from Wikipedia , compilation is the translation of source code into object code by a compiler. That's right. The extra step required by compiled languages is translating the source code, that is the lines of code the programmer(s) wrote into a language that your computer understand better, usually binary (1s and 0s). The big advantage of compiled languages is that they are much faster than interpreted languages. However, programming in them is usually slower and more difficult than in interpreted languages. Using them or not for a software project is a trade-off between development-time, and how much faster your software could run if it was programmed using a compiled language. The most popular compiled language is the C programming language, which Linux is mainly written in.","title":"Compilation"},{"location":"software/#package-managers","text":"All modern linux distributions come with a package manager , i.e. a tool that automates installation of software. In most cases the software manager download already compiled binaries and installs them in your system. We'll see how it works in a moment Let us install our first package! The package manager for Ubuntu is called APT . Like most package managers, the syntax will look like this: [package_manager] [action] [package_name] We'll use apt to install a local version of ncbi-blast that you've use previously. First we search if the package is available apt search ncbi-blast There seems to be two versions of it. The legacy version is probably outdated, so let us investigate the other one apt show ncbi-blast+ It seems to be what we are looking for, we install it with: apt install ncbi-blast+ Question Did it work? What could have been wrong? You should have gotten an error message asking if you are root . The user root is the most powerful user in a linux system and usually has extra rights that a regular user does not have. To install software in the default system location with apt, you have to have special permissions. We can \"borrow\" those permissions from root by prefixing our command with sudo . sudo apt install ncbi-blast+ Now if you execute blastn -help it should print the (rather long) error message of the blastn command. Question Why does blast has different executable? What is the difference between blastn and blastp?","title":"Package Managers"},{"location":"software/#downloading-and-unpacking","text":"Although most popular software can be installed with your distribution's package manager, sometimes (especially in some fast-growing areas of bioinformatics) the software you want isn't available through a package manager. We'll install spades , a popular genome assembly tool. Let's imagine it is not available in the apt sources. We'd have to: download the source code compile the software move it at the right place on our system Which is quite cumbersome, especially the compilation. Luckily, it is fairly common for developers to make linux binaries - that is compiled version of the software - already available for download. First let us create a directory for all our future installs: mkdir -p ~/install cd ~/install The spades binaries are available on their website, http://cab.spbu.ru/software/spades/ Download them with wget http://cab.spbu.ru/files/release3.11.1/SPAdes-3.11.1-Linux.tar.gz and uncompress tar xvf SPAdes-3.11.1-Linux.tar.gz cd SPAdes-3.11.1-Linux/bin/ and now if we execute spades.py ./spades.py we get the help of the spades assembler! A minor inconvenience is that right now pwd # /home/hadrien/install/SPAdes-3.11.1-Linux/bin we have to always go to this directory to run spades.py , or call the software with the full path. We'd like to be able to execute spades from anywhere, like we do with ls and cd . In most linux distributions, which directory can contain software that are executed from anywhere is defined by an environment variable: $PATH Let us take a look: echo $PATH # /home/hadrien/bin:/home/hadrien/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin To make spades.py available from anywhere we have to put it in one of the above locations. Note When apt installs software it usually places it in /usr/bin , which requires administration privileges. This is why we needed sudo for installing packages earlier. mkdir -p ~/.local/bin mv * ~/.local/bin/ Et voil\u00e0! Now you can execute spades.py from anywhere!","title":"Downloading and unpacking"},{"location":"software/#installing-from-source","text":"For some bioinformatics software, binaries are not available. In that case you have to download the source code, and compile it yourself for your system. This is the case of samtools per example. samtools is one of the most popular bioinformatics software and allows you to deal with bam and sam files (more about that later) We'll need a few things to be able to compile samtools, notably make and a C compiler, gcc sudo apt install make gcc samtools also need some libraries that are not installed by default on an ubuntu system. sudo apt install libncurses5-dev libbz2-dev liblzma-dev libcurl4-gnutls-dev Now we can download and unpack the source code: cd ~/install wget https://github.com/samtools/samtools/releases/download/1.6/samtools-1.6.tar.bz2 tar xvf samtools-1.6.tar.bz2 cd samtools-1.6 Compiling software written in C usually follows the same 3 steps. ./configure to configure the compilation options to our machine architecture we run make to compile the software we run make install to move the compiled binaries into a location in the $PATH ./configure make make install Warning Did make install succeed? Why not? As we saw before, we need sudo to install packages to system locations with apt . make install follows the same principle and tries by default to install software in /usr/bin We can change that default behavior by passing options to configure , but first we have to clean our installation: make clean than we can run configure, make and make install again ./configure --prefix=/home/$(whoami)/.local/ make make install samtools Question The bwa source code is available on github, a popular code sharing platform (more on this in the git lesson!). Navigate to https://github.com/lh3/bwa then in release copy the link behind bwa-0.7.17.tar.bz2 - Install bwa!","title":"Installing from source"},{"location":"software/#installing-python-packages","text":"While compiled languages are faster than interpreted languages, they are usually harder to learn, code in and debug. For theses reasons you'll often find many bioinformatics packages written in interpreted languages such as python or ruby . While historically it has been a pain to install software written in interpreted languages, most modern languages now come with their own package managers! For example: Python has pip Ruby has gem Javascript has npm ... Most of theses package managers have similar syntaxes. We will focus on python here since it's one of the most popular languages in bioinformatics. Note You will notice the absence of R here. R is mostly used interactively and installing packages in R will be part of the R part of the course. Your ubuntu comes with an old version of python. We start with installing a newer one cd ~/install wget https://www.python.org/ftp/python/3.6.4/Python-3.6.4.tar.xz tar xvf Python-3.6.4.tar.xz cd Python-3.6.4 ./configure --prefix=/home/$(whoami)/.local/ make -j2 make install Question What does the make option -j2 do? which python3 which pip3 We now have the newest python installed. Let us install our first python package pip3 install multiqc it should take a while and install multiqc as well as all the necessary dependencies. to see if multiqc was properly installed: multiqc -h","title":"Installing python packages"},{"location":"software/#exercises","text":"During the following weeks we'll use a lot of different bioinformatics software to perform a variety of tasks. Tip Most software come with a file named INSTALL or README . Such file usually contains instructions on how to install! Note unless indicated otherwise, try with apt first Note do not hesitate to ask your teacher for help! Let's install a few: fastqc scythe sickle bowtie2 megahit quast prokka","title":"Exercises"},{"location":"unix/","text":"Introduction to Unix Most of the introduction to Unix material can be found at https://software-carpentry.org Many thanks to them for existing! Useful resources Below you will find links to various useful resources for learning or using the UNIX shell. Link to the course material from software carpentry reference of concepts and commands seen during the lesson shell commands explained - a website that shows the help text of any command awesome bash - an awesome list of resources about the bash shell tldp - the linux documentation project (the books can be hard to digest but are very thorough)","title":"Unix"},{"location":"unix/#introduction-to-unix","text":"Most of the introduction to Unix material can be found at https://software-carpentry.org Many thanks to them for existing!","title":"Introduction to Unix"},{"location":"unix/#useful-resources","text":"Below you will find links to various useful resources for learning or using the UNIX shell. Link to the course material from software carpentry reference of concepts and commands seen during the lesson shell commands explained - a website that shows the help text of any command awesome bash - an awesome list of resources about the bash shell tldp - the linux documentation project (the books can be hard to digest but are very thorough)","title":"Useful resources"},{"location":"blast/blast_cli/","text":"Command-line Blast Installing blast While you should have installed blast during the installing software tutorial, you can copy/paste the code block below to reinstall it if needed sudo apt install ncbi-blast+ Getting data We will download some cows and human proteins from RefSeq wget ftp://ftp.ncbi.nih.gov/refseq/B_taurus/mRNA_Prot/cow.1.protein.faa.gz wget ftp://ftp.ncbi.nih.gov/refseq/H_sapiens/mRNA_Prot/human.1.protein.faa.gz Both these files are compressed. They are not tar archives, like we encountered earlier, but gzip files. To uncompress: gzip -d *.gz Let us take a look at the human file head human.1.protein.faa Both files contain protein sequences in the FASTA format Question How many sequences do I have in each file? The files are slightly too big for our first time blasting things at the command-line. Let's downsize the cow file head -6 cow.1.protein.faa > cow.small.faa Our first blast Now we can blast these two cow sequences against the set of human sequences. First we need to build a blast database with our human sequences makeblastdb -in human.1.protein.faa -dbtype prot ls The makeblastdb produced a lot of extra files. Those files are indexes and necessary for blast to function. Now we can run blast blastp -query cow.small.faa -db human.1.protein.faa -out cow_vs_human_blast_results.txt We can look at the results using less less cow_vs_human_blast_results.txt To know about the various options that we can use with blastp: blastp -help and for easier reading blastp -help | less Question How could I modify the previous blast command to filter the hits with an e-value of 1e-5 Bigger dataset Now that we succeeded using a small dataset of two proteins, let's try with a slightly bigger one. head -199 cow.1.protein.faa > cow.medium.faa Question How many protein sequences does cow.medium.faa contain? We run blast again blastp -query cow.medium.faa -db human.1.protein.faa \\ -out cow_vs_human_blast_results.tab -evalue 1e-5 \\ -outfmt 6 -max_target_seqs 1 Question What do -outfmt and -max_target_seqs do?","title":"Command-line Blast"},{"location":"blast/blast_cli/#command-line-blast","text":"","title":"Command-line Blast"},{"location":"blast/blast_cli/#installing-blast","text":"While you should have installed blast during the installing software tutorial, you can copy/paste the code block below to reinstall it if needed sudo apt install ncbi-blast+","title":"Installing blast"},{"location":"blast/blast_cli/#getting-data","text":"We will download some cows and human proteins from RefSeq wget ftp://ftp.ncbi.nih.gov/refseq/B_taurus/mRNA_Prot/cow.1.protein.faa.gz wget ftp://ftp.ncbi.nih.gov/refseq/H_sapiens/mRNA_Prot/human.1.protein.faa.gz Both these files are compressed. They are not tar archives, like we encountered earlier, but gzip files. To uncompress: gzip -d *.gz Let us take a look at the human file head human.1.protein.faa Both files contain protein sequences in the FASTA format Question How many sequences do I have in each file? The files are slightly too big for our first time blasting things at the command-line. Let's downsize the cow file head -6 cow.1.protein.faa > cow.small.faa","title":"Getting data"},{"location":"blast/blast_cli/#our-first-blast","text":"Now we can blast these two cow sequences against the set of human sequences. First we need to build a blast database with our human sequences makeblastdb -in human.1.protein.faa -dbtype prot ls The makeblastdb produced a lot of extra files. Those files are indexes and necessary for blast to function. Now we can run blast blastp -query cow.small.faa -db human.1.protein.faa -out cow_vs_human_blast_results.txt We can look at the results using less less cow_vs_human_blast_results.txt To know about the various options that we can use with blastp: blastp -help and for easier reading blastp -help | less Question How could I modify the previous blast command to filter the hits with an e-value of 1e-5","title":"Our first blast"},{"location":"blast/blast_cli/#bigger-dataset","text":"Now that we succeeded using a small dataset of two proteins, let's try with a slightly bigger one. head -199 cow.1.protein.faa > cow.medium.faa Question How many protein sequences does cow.medium.faa contain? We run blast again blastp -query cow.medium.faa -db human.1.protein.faa \\ -out cow_vs_human_blast_results.tab -evalue 1e-5 \\ -outfmt 6 -max_target_seqs 1 Question What do -outfmt and -max_target_seqs do?","title":"Bigger dataset"},{"location":"blast/blast_online/","text":"Using Blast online Blast is one of the most used bioinformatics tools ever written. It allows to find similaroties between sequences of proteins or nucleotides, at a reasonable speed.! Its ancestor, the fasta suite (like the format) used an algorithm clled smith-waterman. While very accurate, was very slow. Blast is available opnline on the ncbi website at the following address https://blast.ncbi.nlm.nih.gov/Blast.cgi Let us try it out! Your first blast go on the blast ncbi website and select protein blast Copy and paste the following protein sequence >a_protein MKWVTLISFIFLFSSATSRNLQRFARDAEHKSEIAHRYNDLKEETFKAVAMITFAQYLQR CSYEGLSKLVKDVVDLAQKCVANEDAPECSKPLPSIILDEICQVEKLRDSYGAMADCCSK ADPERNECFLSFKVSQPDFVQPYQRPASDVICQEYQDNRVSFLGHFIYSVARRHPFLYAP AILSFAVDFEHALQSCCKESDVGACLDTKEIVMREKAKGVSVKQQYFCGILKQFGDRVFQ ARQLIYLSQKYPKAPFSEVSKFVHDSIGVHKECCEGDMVECMDDMARMMSNLCSQQDVFS GKIKDCCEKPIVERSQCIMEAEFDEKPADLPSLVEKYIEDKEVCKSFEAGHDAFMAEFVY EYSRRHPEFSIQLIMRIAKGYESLLEKCCKTDNPAECYANAQEQLNQHIKETQDVVKTNC DLLHDHGEADFLKSILIRYTKKMPQVPTDLLLETGKKMTTIGTKCCQLGEDRRMACSEGY LSIVIHDTCRKQETTPINDNVSQCCSQLYANRRPCFTAMGVDTKYVPPPFNPDMFSFDEK LCSAPAEEREVGQMKLLINLIKRKPQMTEEQIKTIADGFTAMVDKCCKQSDINTCFGEEG ANLIVQSRATLGIGA into the \"enter query sequence\" form and click \"Blast\" (you may have to scroll down a bit) then wait for your results. Once the results have loaded, scroll down to the first hit. Question From which organism is our protein? What is it? Note Sometyimes, two hits have an identical score. In that case, be careful with the interpretation. Restrict the search parameters Let us imagine we'd like to compare our query to only human proteins. We can select an organism to restrict our search. Question What is the percentage identidy between the chicken and human albumin? Question Here is an unknown protein. From which organism and which function does it have? MVLSAADKGNVKAAWGKVGGHAAEYGAEALERMFLSFPTTKTYFPHFDLSHGSAQVKGHG AKVAAALTKAVEHLDDLPGALSELSDLHAHKLRVDPVNFKLLSHSLLVTLASHLPSDFTP AVHASLDKFLANVSTVLTSKYR What's next? The Command-line An Introduction to Cloud computing","title":"Blast Online"},{"location":"blast/blast_online/#using-blast-online","text":"Blast is one of the most used bioinformatics tools ever written. It allows to find similaroties between sequences of proteins or nucleotides, at a reasonable speed.! Its ancestor, the fasta suite (like the format) used an algorithm clled smith-waterman. While very accurate, was very slow. Blast is available opnline on the ncbi website at the following address https://blast.ncbi.nlm.nih.gov/Blast.cgi Let us try it out!","title":"Using Blast online"},{"location":"blast/blast_online/#your-first-blast","text":"go on the blast ncbi website and select protein blast Copy and paste the following protein sequence >a_protein MKWVTLISFIFLFSSATSRNLQRFARDAEHKSEIAHRYNDLKEETFKAVAMITFAQYLQR CSYEGLSKLVKDVVDLAQKCVANEDAPECSKPLPSIILDEICQVEKLRDSYGAMADCCSK ADPERNECFLSFKVSQPDFVQPYQRPASDVICQEYQDNRVSFLGHFIYSVARRHPFLYAP AILSFAVDFEHALQSCCKESDVGACLDTKEIVMREKAKGVSVKQQYFCGILKQFGDRVFQ ARQLIYLSQKYPKAPFSEVSKFVHDSIGVHKECCEGDMVECMDDMARMMSNLCSQQDVFS GKIKDCCEKPIVERSQCIMEAEFDEKPADLPSLVEKYIEDKEVCKSFEAGHDAFMAEFVY EYSRRHPEFSIQLIMRIAKGYESLLEKCCKTDNPAECYANAQEQLNQHIKETQDVVKTNC DLLHDHGEADFLKSILIRYTKKMPQVPTDLLLETGKKMTTIGTKCCQLGEDRRMACSEGY LSIVIHDTCRKQETTPINDNVSQCCSQLYANRRPCFTAMGVDTKYVPPPFNPDMFSFDEK LCSAPAEEREVGQMKLLINLIKRKPQMTEEQIKTIADGFTAMVDKCCKQSDINTCFGEEG ANLIVQSRATLGIGA into the \"enter query sequence\" form and click \"Blast\" (you may have to scroll down a bit) then wait for your results. Once the results have loaded, scroll down to the first hit. Question From which organism is our protein? What is it? Note Sometyimes, two hits have an identical score. In that case, be careful with the interpretation.","title":"Your first blast"},{"location":"blast/blast_online/#restrict-the-search-parameters","text":"Let us imagine we'd like to compare our query to only human proteins. We can select an organism to restrict our search. Question What is the percentage identidy between the chicken and human albumin? Question Here is an unknown protein. From which organism and which function does it have? MVLSAADKGNVKAAWGKVGGHAAEYGAEALERMFLSFPTTKTYFPHFDLSHGSAQVKGHG AKVAAALTKAVEHLDDLPGALSELSDLHAHKLRVDPVNFKLLSHSLLVTLASHLPSDFTP AVHASLDKFLANVSTVLTSKYR","title":"Restrict the search parameters"},{"location":"blast/blast_online/#whats-next","text":"The Command-line An Introduction to Cloud computing","title":"What's next?"},{"location":"nbis_annotation/schedule/","text":"Introduction to genome annotation Teacher: Jacques Dainat, Ph.D. NBIS (National Bioinformatics Infrastructure Sweden) Genome Annotation Service http://nbis.se/about/staff/jacques-dainat/ http://nbis.se Schedule Click the heading of a topic to see the lecture slides or lab instructions. Thursday 1st 09.00-09.30: Lecture: Structural annotation programs and pipelines 10.00-10.15: Coffee break 10.15-11.30: Practical 1: Assembly assessment + Abinitio annotation 11.30-12.30: Lunch 12.30-13.00: Lecture: Structural annotation programs and pipelines (part 2) 13.00-16.00: Practical 2: Structural annotation of eukaryote genomes (incl. coffee break) Friday 2nd 09.00-09.15: Summary of yesterday\u2019s exercise 09.15-09.30: Lecture: Introduction to manual curation 10.00-12.00: Practical 3: Manual curation(incl. coffee break) 11.30-12.30: Lunch 12.30-13.00: Lecture: Functional annotation 13.00-15.30: Practical 4: Functional annotation (incl. coffee break) 15.30-16.00: Wrap-up","title":"Schedule"},{"location":"nbis_annotation/schedule/#introduction-to-genome-annotation","text":"Teacher: Jacques Dainat, Ph.D. NBIS (National Bioinformatics Infrastructure Sweden) Genome Annotation Service http://nbis.se/about/staff/jacques-dainat/ http://nbis.se","title":"Introduction to genome annotation"},{"location":"nbis_annotation/schedule/#schedule","text":"Click the heading of a topic to see the lecture slides or lab instructions. Thursday 1st 09.00-09.30: Lecture: Structural annotation programs and pipelines 10.00-10.15: Coffee break 10.15-11.30: Practical 1: Assembly assessment + Abinitio annotation 11.30-12.30: Lunch 12.30-13.00: Lecture: Structural annotation programs and pipelines (part 2) 13.00-16.00: Practical 2: Structural annotation of eukaryote genomes (incl. coffee break) Friday 2nd 09.00-09.15: Summary of yesterday\u2019s exercise 09.15-09.30: Lecture: Introduction to manual curation 10.00-12.00: Practical 3: Manual curation(incl. coffee break) 11.30-12.30: Lunch 12.30-13.00: Lecture: Functional annotation 13.00-15.30: Practical 4: Functional annotation (incl. coffee break) 15.30-16.00: Wrap-up","title":"Schedule"},{"location":"nbis_annotation/practical_session/UsingWebapollo/","text":"Using WebApollo to view annotations For this course, we have set up a WebApollo installation - as a reminder, the url is http://annotation-prod.scilifelab.se:8080/NBIS_course . Login information are the following: username: userX (Where X is the number you have been assigned) password: demo When logged in to the page, select the proper project corresponding to the exercice (drosophila_melanogaster_chr4 or drosophila_melanogaster_chr4_jamboree). In the right side of the browser you will see different tabs available. The most useful one for you will be the \"Tracks\" one. In this tab will be display the different tracks available for displaying. If you wish uploading a(n) track/annotation you have created to the web portal, follow these instructions: Click on 'File' in the top left corner of the page Select 'Open' Click in 'Select Files' from the 'Local Files' section Select the file you wish to upload, and leave all settings at their defaults (you may wish to specify a more informative name for the new track though). This should upload your annotation track to the page. However, remember that tracks added in this way are only temporary and will disappear if you log out or lose connection to the server.","title":"UsingWebapollo"},{"location":"nbis_annotation/practical_session/UsingWebapollo/#using-webapollo-to-view-annotations","text":"For this course, we have set up a WebApollo installation - as a reminder, the url is http://annotation-prod.scilifelab.se:8080/NBIS_course . Login information are the following: username: userX (Where X is the number you have been assigned) password: demo When logged in to the page, select the proper project corresponding to the exercice (drosophila_melanogaster_chr4 or drosophila_melanogaster_chr4_jamboree). In the right side of the browser you will see different tabs available. The most useful one for you will be the \"Tracks\" one. In this tab will be display the different tracks available for displaying. If you wish uploading a(n) track/annotation you have created to the web portal, follow these instructions: Click on 'File' in the top left corner of the page Select 'Open' Click in 'Select Files' from the 'Local Files' section Select the file you wish to upload, and leave all settings at their defaults (you may wish to specify a more informative name for the new track though). This should upload your annotation track to the page. However, remember that tracks added in this way are only temporary and will disappear if you log out or lose connection to the server.","title":"Using WebApollo to view annotations"},{"location":"nbis_annotation/practical_session/practical1/","text":"Foreword: We will for all exercises use data for the fruit fly, Drosophila melanogaster, as that is one of the currently best annotated organisms and there is plenty of high quality data available. However, working on eukaryotes can be time consuming. Even a small genome like Drosophila would take too long to run within the time we have for this course. Thus to be sure to perform the practicals in good conditions, we will use the smallest chromosome of the drosophila (chromosome 4) like it was a whole genome. An annotation project requires numerous tools and dependencies, which can take easily many days to install for a neophyte. For your convenience and in order to focus on the art of the ANNOTATION most of the tools are already installed on your machine (Thank you Hadrien :) ). First of all Before going into the exercises below you need to connect to your virtual machine Ubuntu 16.04 following the instruction we will provide you. Once connected you will move into the annotation_course folder, where all the magic will happen. cd ~/annotation_course Now you need the data !! You must download the archive of the data and uncompress it (it could take few minutes). wget https://u-ip-81-109.hpc2n.umu.se/tickets/La34or2kms3wMdf1Gp5HdbsmT4fFCIfayQeHvew8kaU/data.tar.gz/download tar xzvf download rm download Now move into the practical1 folder and you are ready to start for this morning ! cd ~/annotation_course/practical1 1. Assembly Check Before starting an annotation project, we need to carefully inspect the assembly to identify potential problems before running expensive computes. You can look at i) the Fragmentation (N50, N90, how many short contigs); ii) the Sanity of the fasta file (Presence of Ns, presence of ambiguous nucleotides, presence of lowercase nucleotides, single line sequences vs multiline sequences); iii) completeness using BUSCO; iv) presence of organelles; v) Others (GC content, How distant the investigated species is from the others annotated species available). The two next exercices will perform some of these checks. 1.1 Checking the gene space of your assembly BUSCO provides measures for quantitative assessment of genome assembly, gene set, and transcriptome completeness. Genes that make up the BUSCO sets for each major lineage are selected from orthologous groups with genes present as single-copy orthologs in at least 90% of the species. Note: In a real-world scenario, this step should come first and foremost. Indeed, if the result is under your expectation you might be required to enhance your assembly before to go further. Exercise 1 - BUSCO -: You will run BUSCO on chromosome 4 of Drosophila melanogaster. First create a busco folder where you work: mkdir busco cd busco Then visit the busco website and choose the best data set among the vast choice. Once you know which one you want to use, right click on it and copy the link to it. Then download the dataset. /!\\ Replace http://busco.ezlab.org/datasets/metazoa_odb9.tar.gz link by the link to the dataset you have chosen. wget http://busco.ezlab.org/datasets/metazoa_odb9.tar.gz tar xzvf metazoa_odb9.tar.gz Now you are ready to launch BUSCO on our genome (reminder: the genome is the chormosome 4 called 4.fa). BUSCO.py -i ~/annotation_course/data/genome/4.fa -o 4_dmel_busco -m geno -c 8 -l metazoa_odb9 While BUSCO is running, start the exercise 2. When done, check the short_summary_4_dmel_busco. How many proteins are reported as complete? Does this sound reasonable? Tips : the chromosome 4 corresponds to less than 1% of the real size of the genome. 1.2 Various Check of your Assembly Exercise 2 : Launching the following script will provide you some useful information. fasta_statisticsAndPlot.pl -f ~/annotation_course/data/genome/4.fa If you don't see any peculiarities, you can then decide to go forward and start to perform your first wonderful annotation. 2. Running an ab initio gene finder Now we are satisfied by the quality of the assembly we can start the annotation. Ab initio gene finders: These methods have been around for a very long time, and there are many different programs to try. We will in this exercise focus on the gene finder Augustus. These gene finders use likelihoods to find the most likely genes in the genome. They are aware of start and stop codons and splice sites, and will only try to predict genes that follow these rules. The most important factor here is that the gene finder needs to be trained on the organism you are running the program on, otherwise the probabilities for introns, exons, etc. will not be correct. Luckily, these training files are available for Drosophila. Exercise 3 - Augustus: Run Augustus on your genome file using: cd ~/annotation_course/practical1 mkdir augustus cd augustus augustus --species=fly ~/annotation_course/data/genome/4.fa --gff3=yes > augustus_drosophila.gff if you wish to annotate isoforms too, use the following command: augustus --species=fly ~/annotation_course/data/genome/4.fa --gff3=yes --alternatives-from-sampling=true > augustus_drosophila_isoform.gff Take a look at the gff result file using the command \u2018less augustus_drosophila.gff\u2019. What kinds of features have been annotated? Does it tell you anything about UTRs? To better understand what contains your gff file you may use a script that will provide you some statistics like this one: gff3_sp_statistics.pl --gff augustus_drosophila.gff How many genes have you annotated ? It if of interest to view your annotation in a genome browser, this is more concrete and much nicer. A visual inspection is often the most effective way to assess the quality o your annotation. Transfer the augustus_drosophila.gff3 to your computer using scp in a new terminal: scp -i ~/.ssh/azure_rsa student@__IP__:/home/student/annotation_course/practical1/augustus/augustus_drosophila.gff . We have made a genome browser called Webapollo available for you on the address http://annotation-prod.scilifelab.se:8080/NBIS_course/ . Load the file in into the genome portal called drosophila_melanogaster_chr4 . Here find the WebApollo instruction The official Ensembl annotation is available in the genome browser. How does the Augustus annotation compare with the Ensembl annotation? Are they identical? Exercise 4 - Augustus with yeast models: Run augustus on the same genome file but using settings for yeast instead (change species to Saccharomyces). Load this result file into Webapollo and compare with your earlier results. Can you based on this draw any conclusions about how a typical yeast gene differs from a typical Drosophila gene? Closing remarks We have seen how to assess the quality of the assembly and how to launch a quick annotation using an abinitio tool. We have also seen the importance to use a species specific hmm model into the ab initio tool. Thus, the limitation of this approach is linked to the pre-trained species that are available.","title":"Practical 1"},{"location":"nbis_annotation/practical_session/practical1/#foreword","text":"We will for all exercises use data for the fruit fly, Drosophila melanogaster, as that is one of the currently best annotated organisms and there is plenty of high quality data available. However, working on eukaryotes can be time consuming. Even a small genome like Drosophila would take too long to run within the time we have for this course. Thus to be sure to perform the practicals in good conditions, we will use the smallest chromosome of the drosophila (chromosome 4) like it was a whole genome. An annotation project requires numerous tools and dependencies, which can take easily many days to install for a neophyte. For your convenience and in order to focus on the art of the ANNOTATION most of the tools are already installed on your machine (Thank you Hadrien :) ).","title":"Foreword:"},{"location":"nbis_annotation/practical_session/practical1/#first-of-all","text":"Before going into the exercises below you need to connect to your virtual machine Ubuntu 16.04 following the instruction we will provide you. Once connected you will move into the annotation_course folder, where all the magic will happen. cd ~/annotation_course Now you need the data !! You must download the archive of the data and uncompress it (it could take few minutes). wget https://u-ip-81-109.hpc2n.umu.se/tickets/La34or2kms3wMdf1Gp5HdbsmT4fFCIfayQeHvew8kaU/data.tar.gz/download tar xzvf download rm download Now move into the practical1 folder and you are ready to start for this morning ! cd ~/annotation_course/practical1","title":"First of all"},{"location":"nbis_annotation/practical_session/practical1/#1-assembly-check","text":"Before starting an annotation project, we need to carefully inspect the assembly to identify potential problems before running expensive computes. You can look at i) the Fragmentation (N50, N90, how many short contigs); ii) the Sanity of the fasta file (Presence of Ns, presence of ambiguous nucleotides, presence of lowercase nucleotides, single line sequences vs multiline sequences); iii) completeness using BUSCO; iv) presence of organelles; v) Others (GC content, How distant the investigated species is from the others annotated species available). The two next exercices will perform some of these checks.","title":"1. Assembly Check"},{"location":"nbis_annotation/practical_session/practical1/#11-checking-the-gene-space-of-your-assembly","text":"BUSCO provides measures for quantitative assessment of genome assembly, gene set, and transcriptome completeness. Genes that make up the BUSCO sets for each major lineage are selected from orthologous groups with genes present as single-copy orthologs in at least 90% of the species. Note: In a real-world scenario, this step should come first and foremost. Indeed, if the result is under your expectation you might be required to enhance your assembly before to go further. Exercise 1 - BUSCO -: You will run BUSCO on chromosome 4 of Drosophila melanogaster. First create a busco folder where you work: mkdir busco cd busco Then visit the busco website and choose the best data set among the vast choice. Once you know which one you want to use, right click on it and copy the link to it. Then download the dataset. /!\\ Replace http://busco.ezlab.org/datasets/metazoa_odb9.tar.gz link by the link to the dataset you have chosen. wget http://busco.ezlab.org/datasets/metazoa_odb9.tar.gz tar xzvf metazoa_odb9.tar.gz Now you are ready to launch BUSCO on our genome (reminder: the genome is the chormosome 4 called 4.fa). BUSCO.py -i ~/annotation_course/data/genome/4.fa -o 4_dmel_busco -m geno -c 8 -l metazoa_odb9 While BUSCO is running, start the exercise 2. When done, check the short_summary_4_dmel_busco. How many proteins are reported as complete? Does this sound reasonable? Tips : the chromosome 4 corresponds to less than 1% of the real size of the genome.","title":"1.1 Checking the gene space of your assembly"},{"location":"nbis_annotation/practical_session/practical1/#12-various-check-of-your-assembly","text":"Exercise 2 : Launching the following script will provide you some useful information. fasta_statisticsAndPlot.pl -f ~/annotation_course/data/genome/4.fa If you don't see any peculiarities, you can then decide to go forward and start to perform your first wonderful annotation.","title":"1.2 Various Check of your Assembly"},{"location":"nbis_annotation/practical_session/practical1/#2-running-an-ab-initio-gene-finder","text":"Now we are satisfied by the quality of the assembly we can start the annotation. Ab initio gene finders: These methods have been around for a very long time, and there are many different programs to try. We will in this exercise focus on the gene finder Augustus. These gene finders use likelihoods to find the most likely genes in the genome. They are aware of start and stop codons and splice sites, and will only try to predict genes that follow these rules. The most important factor here is that the gene finder needs to be trained on the organism you are running the program on, otherwise the probabilities for introns, exons, etc. will not be correct. Luckily, these training files are available for Drosophila. Exercise 3 - Augustus: Run Augustus on your genome file using: cd ~/annotation_course/practical1 mkdir augustus cd augustus augustus --species=fly ~/annotation_course/data/genome/4.fa --gff3=yes > augustus_drosophila.gff if you wish to annotate isoforms too, use the following command: augustus --species=fly ~/annotation_course/data/genome/4.fa --gff3=yes --alternatives-from-sampling=true > augustus_drosophila_isoform.gff Take a look at the gff result file using the command \u2018less augustus_drosophila.gff\u2019. What kinds of features have been annotated? Does it tell you anything about UTRs? To better understand what contains your gff file you may use a script that will provide you some statistics like this one: gff3_sp_statistics.pl --gff augustus_drosophila.gff How many genes have you annotated ? It if of interest to view your annotation in a genome browser, this is more concrete and much nicer. A visual inspection is often the most effective way to assess the quality o your annotation. Transfer the augustus_drosophila.gff3 to your computer using scp in a new terminal: scp -i ~/.ssh/azure_rsa student@__IP__:/home/student/annotation_course/practical1/augustus/augustus_drosophila.gff . We have made a genome browser called Webapollo available for you on the address http://annotation-prod.scilifelab.se:8080/NBIS_course/ . Load the file in into the genome portal called drosophila_melanogaster_chr4 . Here find the WebApollo instruction The official Ensembl annotation is available in the genome browser. How does the Augustus annotation compare with the Ensembl annotation? Are they identical? Exercise 4 - Augustus with yeast models: Run augustus on the same genome file but using settings for yeast instead (change species to Saccharomyces). Load this result file into Webapollo and compare with your earlier results. Can you based on this draw any conclusions about how a typical yeast gene differs from a typical Drosophila gene?","title":"2. Running an ab initio gene finder"},{"location":"nbis_annotation/practical_session/practical1/#closing-remarks","text":"We have seen how to assess the quality of the assembly and how to launch a quick annotation using an abinitio tool. We have also seen the importance to use a species specific hmm model into the ab initio tool. Thus, the limitation of this approach is linked to the pre-trained species that are available.","title":"Closing remarks"},{"location":"nbis_annotation/practical_session/practical2/","text":"Gathering evidence data for annotation (Optional) This exercise is meant to get you acquainted with the type of data you would normally encounter in an annotation project. 1. Protein, EST and RNA-seq data (Optional) Here will get an idea of where to download evidence sequences. 2. Assembling transcripts based on RNA-seq data (Optional) Here will get an idea how to deal with RNA-seq data Closing remarks Now you know how to obtain evidence data, which will be useful to perform a nice annotation. Running the Maker gene build pipeline Overview MAKER is a computational pipeline to automatically generate annotations from a range of input data - including proteins, ESTs, RNA-seq transcripts and ab-initio gene predictions. During this exercise, you will learn how to use Maker with different forms of input data, and how to judge the quality of the resulting annotations. The Maker pipeline can work with any combination of the following data sets: Proteins from the same species or related species Proteins from more distantly related organisms (e.g. Uniprot/Swissprot) EST sequences from the same species or very closely related species RNA-seq data from the same or very closely related species - in the form of splice sites or assembled transcripts Ab-initio predictions from one or more tools (directly supported are: Augustus, Snap, GeneMark, Fgenesh) At minimum, most annotation projects will run with a protein data set, possibly complemented by some RNA-seq data. Popular examples of this are most of the traditional model systems, including human. However, a potential shortcoming of such approaches is that the comprehensiveness of the annotation depends directly on the input data. This can become a problem if our genome of interest is taxonomically distant to well-sequenced taxonomic groups so that only few protein matches can be found. Likewise, not all genes will be expressed at all times, making the generation of a comprehensive RNA-seq data set for annotation challenging. We will therefore first run our annotation project in the traditional way, with proteins and ESTs, and then repeat the process with a well-trained ab-initio gene predictor. You can then compare the output to get an idea of how crucial the use of a gene predictor is. However, before we get our hands dirty, we need to understand Maker a little better... Maker strings together a range of different tools into a complex pipeline (e.g. blast, exonerate, repeatmasker, augustus...), fortunately all its various dependencies have been already installed for you. Check that everything is running smoothly by creating the MAKER config files: cd ~/annotation_course/practical2 mkdir maker cd maker maker -CTL Understanding Makers control files Makers behaviour and information on input data are specified in one of three control files. These are: maker_opts.ctl maker_bopts.ctl maker_exe.ctl What are these files for? 'maker_exe.ctl' holds information on the location of the various binaries required by Maker (including Blast, Repeatmasker etc). Normally, all information in this file will be extracted from $PATH, so if everything is set up correctly, you will never have to look into this file. Next, 'maker_bopts.ctl' provides access to a number of settings that control the behaviour of evidence aligners (blast, exonerate). The default settings will usually be fine, but if you want to try to annotate species with greater taxonomic distance to well-sequenced species, it may become necessary to decrease stringency of the e.g. blast alignments. Finally, 'maker_opts.ctl' holds information on the location of input files and some of the parameters controlling the decision making during the gene building. Running Maker - Drosophila genome We will annotate the genome of the fruit fly Drosophila melanogaster . First we will perforn a pure evidence based annotation (without ab-initio predictions) and afterwards with ab-initio. 1. Creating an evidence based annotation Running Maker with only evidence data 2. Creating an abinition evidence-driven annotation Running Maker with ab-initio predictions 3. Inspecting the output The running of an annotation pipeline like Maker is not actually very hard. But the complicated work is only beginning. How to we best inspect the gene builds? Count features? Visualize it? Most importantly, what steps do we need to take to create a 'finished' annotation that we can use for scientific analyses? Comparing and evaluating annotations Closing remarks This concludes the gene building part. We have learned how to use the Maker annotation pipeline and have created gene builds with and without ab-initio predictions. Moreover, we have employed some measures to describe and judge these annotations. An essential part that we decided to leave out is the training of ab-initio gene finders. The reason for this omission was that there isn't really any one best way to do this and your mileage may vary a lot based on your organism and input data. Perhaps the most direct approach available at the moment is a combination of evidence-based annotation with Maker and to use the resulting, crude gene models to train SNAP. Since Maker can improve ab-initio predictions 'on the fly', it can tolerate a bit of noise from a less-than-perfect ab-initio profile. If you are setting out on an annotation project, the NBIS annotation service would be happy to discuss the best approach for your data with you. With that being said, generating a gene build is only one part of an annotation project. Next, we will inspect the annotation in genome browser and make an attempt at functional inference for the predicted gene models.","title":"Practical 2"},{"location":"nbis_annotation/practical_session/practical2/#gathering-evidence-data-for-annotation-optional","text":"This exercise is meant to get you acquainted with the type of data you would normally encounter in an annotation project.","title":"Gathering evidence data for annotation (Optional)"},{"location":"nbis_annotation/practical_session/practical2/#1-protein-est-and-rna-seq-data-optional","text":"Here will get an idea of where to download evidence sequences.","title":"1. Protein, EST and RNA-seq data (Optional)"},{"location":"nbis_annotation/practical_session/practical2/#2-assembling-transcripts-based-on-rna-seq-data-optional","text":"Here will get an idea how to deal with RNA-seq data","title":"2. Assembling transcripts based on RNA-seq data (Optional)"},{"location":"nbis_annotation/practical_session/practical2/#closing-remarks","text":"Now you know how to obtain evidence data, which will be useful to perform a nice annotation.","title":"Closing remarks"},{"location":"nbis_annotation/practical_session/practical2/#running-the-maker-gene-build-pipeline","text":"","title":"Running the Maker gene build pipeline"},{"location":"nbis_annotation/practical_session/practical2/#overview","text":"MAKER is a computational pipeline to automatically generate annotations from a range of input data - including proteins, ESTs, RNA-seq transcripts and ab-initio gene predictions. During this exercise, you will learn how to use Maker with different forms of input data, and how to judge the quality of the resulting annotations. The Maker pipeline can work with any combination of the following data sets: Proteins from the same species or related species Proteins from more distantly related organisms (e.g. Uniprot/Swissprot) EST sequences from the same species or very closely related species RNA-seq data from the same or very closely related species - in the form of splice sites or assembled transcripts Ab-initio predictions from one or more tools (directly supported are: Augustus, Snap, GeneMark, Fgenesh) At minimum, most annotation projects will run with a protein data set, possibly complemented by some RNA-seq data. Popular examples of this are most of the traditional model systems, including human. However, a potential shortcoming of such approaches is that the comprehensiveness of the annotation depends directly on the input data. This can become a problem if our genome of interest is taxonomically distant to well-sequenced taxonomic groups so that only few protein matches can be found. Likewise, not all genes will be expressed at all times, making the generation of a comprehensive RNA-seq data set for annotation challenging. We will therefore first run our annotation project in the traditional way, with proteins and ESTs, and then repeat the process with a well-trained ab-initio gene predictor. You can then compare the output to get an idea of how crucial the use of a gene predictor is. However, before we get our hands dirty, we need to understand Maker a little better... Maker strings together a range of different tools into a complex pipeline (e.g. blast, exonerate, repeatmasker, augustus...), fortunately all its various dependencies have been already installed for you. Check that everything is running smoothly by creating the MAKER config files: cd ~/annotation_course/practical2 mkdir maker cd maker maker -CTL","title":"Overview"},{"location":"nbis_annotation/practical_session/practical2/#understanding-makers-control-files","text":"Makers behaviour and information on input data are specified in one of three control files. These are: maker_opts.ctl maker_bopts.ctl maker_exe.ctl What are these files for? 'maker_exe.ctl' holds information on the location of the various binaries required by Maker (including Blast, Repeatmasker etc). Normally, all information in this file will be extracted from $PATH, so if everything is set up correctly, you will never have to look into this file. Next, 'maker_bopts.ctl' provides access to a number of settings that control the behaviour of evidence aligners (blast, exonerate). The default settings will usually be fine, but if you want to try to annotate species with greater taxonomic distance to well-sequenced species, it may become necessary to decrease stringency of the e.g. blast alignments. Finally, 'maker_opts.ctl' holds information on the location of input files and some of the parameters controlling the decision making during the gene building.","title":"Understanding Makers control files"},{"location":"nbis_annotation/practical_session/practical2/#running-maker-drosophila-genome","text":"We will annotate the genome of the fruit fly Drosophila melanogaster . First we will perforn a pure evidence based annotation (without ab-initio predictions) and afterwards with ab-initio.","title":"Running Maker - Drosophila genome"},{"location":"nbis_annotation/practical_session/practical2/#1-creating-an-evidence-based-annotation","text":"Running Maker with only evidence data","title":"1. Creating an evidence based annotation"},{"location":"nbis_annotation/practical_session/practical2/#2-creating-an-abinition-evidence-driven-annotation","text":"Running Maker with ab-initio predictions","title":"2. Creating an abinition evidence-driven annotation"},{"location":"nbis_annotation/practical_session/practical2/#3-inspecting-the-output","text":"The running of an annotation pipeline like Maker is not actually very hard. But the complicated work is only beginning. How to we best inspect the gene builds? Count features? Visualize it? Most importantly, what steps do we need to take to create a 'finished' annotation that we can use for scientific analyses? Comparing and evaluating annotations","title":"3. Inspecting the output"},{"location":"nbis_annotation/practical_session/practical2/#closing-remarks_1","text":"This concludes the gene building part. We have learned how to use the Maker annotation pipeline and have created gene builds with and without ab-initio predictions. Moreover, we have employed some measures to describe and judge these annotations. An essential part that we decided to leave out is the training of ab-initio gene finders. The reason for this omission was that there isn't really any one best way to do this and your mileage may vary a lot based on your organism and input data. Perhaps the most direct approach available at the moment is a combination of evidence-based annotation with Maker and to use the resulting, crude gene models to train SNAP. Since Maker can improve ab-initio predictions 'on the fly', it can tolerate a bit of noise from a less-than-perfect ab-initio profile. If you are setting out on an annotation project, the NBIS annotation service would be happy to discuss the best approach for your data with you. With that being said, generating a gene build is only one part of an annotation project. Next, we will inspect the annotation in genome browser and make an attempt at functional inference for the predicted gene models.","title":"Closing remarks"},{"location":"nbis_annotation/practical_session/practical2_sub_gatherEvidence/","text":"Obtaining Protein Swissprot: Uniprot is an excellent source for high quality protein sequences. The main site can be found at http://www.uniprot.org . This is also the place to find Swissprot, a collection of manually curated non-redundant proteins that cover a wide range of organisms while still being manageable in size. Exercise 1 - Swissprot: Navigate the Uniprot site to find the download location for Swissprot in fasta-format. You do not need to download the file, just find it. In what way does Swissprot differ from Uniref (another excellent source of proteins, also available at the same site)? Uniprot: Even with Swissprot available, you also often want to include protein sequences from organisms closely related to your study organism. An approach we often use is to concatenate Swissprot with a few protein fasta-files from closely related organisms and use this in our annotation pipeline. Exercise 2 - Uniprot: Use Uniprot to find (not download) all protein sequences for all the complete genomes in the family Drosophilidae. How many complete genomes in Drosophilidae do you find? Refseq: Refseq is another good place to find non-redundant protein sequences to use in your project. The sequences are to some extent sorted by organismal group, but only to very large and inclusive groups. The best way to download large datasets from refseq is using their ftp-server at ftp://ftp.ncbi.nlm.nih.gov/refseq/ . Exercise 3 - Refseq: Navigate the Refseq ftp site to find the invertebrate collection of protein sequences. You do not need to download the sequences, just find them. The files are mixed with other types of data, which files include the protein sequences? Ensembl: The European Ensembl project makes data available for a number of genome projects, in particular vertebrate animals, through their excellent webinterface. This is a good place to find annotations for model organisms as well as download protein sequences and other types of data. They also supply the Biomart interface, which is excellent if you want to download data for a specific region, a specific gene, or create easily parsable file with gene names etc. Exercise 4 - Ensembl Biomart: Go to Biomart at http://www.ensembl.org/biomart/martview and use it to download all protein sequences for chromosome 4 in Drosophila melanogaster. Once you have downloaded the file, use some command line magic to figure out how many sequences are included in the file. Please ask the teachers if you are having problems here. Obtaining EST EST data is not commonly generated anymore, but may become useful for some projects where such data is still available. Examples may include older genomes targeted for re-annotation or genomes with available EST data for closely related species. The NCBI or EBI websites are the most appropriate places to retrieve such kind of data. Exercise 5 - NCBI: Go to the NCBI website and find how many ESTs are available for the drosophila melanogaster species. Obtaining RNA-seq Commonly, such data are produced within the project you are working on. Otherwise the most appropriate data could be retrieved on the Sequence Read Archive (SRA) website from the NCBI or the European Nucleotide Archive (ENA) from the EBI.","title":"Protein, EST and RNA-seq data"},{"location":"nbis_annotation/practical_session/practical2_sub_gatherEvidence/#obtaining-protein","text":"Swissprot: Uniprot is an excellent source for high quality protein sequences. The main site can be found at http://www.uniprot.org . This is also the place to find Swissprot, a collection of manually curated non-redundant proteins that cover a wide range of organisms while still being manageable in size. Exercise 1 - Swissprot: Navigate the Uniprot site to find the download location for Swissprot in fasta-format. You do not need to download the file, just find it. In what way does Swissprot differ from Uniref (another excellent source of proteins, also available at the same site)? Uniprot: Even with Swissprot available, you also often want to include protein sequences from organisms closely related to your study organism. An approach we often use is to concatenate Swissprot with a few protein fasta-files from closely related organisms and use this in our annotation pipeline. Exercise 2 - Uniprot: Use Uniprot to find (not download) all protein sequences for all the complete genomes in the family Drosophilidae. How many complete genomes in Drosophilidae do you find? Refseq: Refseq is another good place to find non-redundant protein sequences to use in your project. The sequences are to some extent sorted by organismal group, but only to very large and inclusive groups. The best way to download large datasets from refseq is using their ftp-server at ftp://ftp.ncbi.nlm.nih.gov/refseq/ . Exercise 3 - Refseq: Navigate the Refseq ftp site to find the invertebrate collection of protein sequences. You do not need to download the sequences, just find them. The files are mixed with other types of data, which files include the protein sequences? Ensembl: The European Ensembl project makes data available for a number of genome projects, in particular vertebrate animals, through their excellent webinterface. This is a good place to find annotations for model organisms as well as download protein sequences and other types of data. They also supply the Biomart interface, which is excellent if you want to download data for a specific region, a specific gene, or create easily parsable file with gene names etc. Exercise 4 - Ensembl Biomart: Go to Biomart at http://www.ensembl.org/biomart/martview and use it to download all protein sequences for chromosome 4 in Drosophila melanogaster. Once you have downloaded the file, use some command line magic to figure out how many sequences are included in the file. Please ask the teachers if you are having problems here.","title":"Obtaining Protein"},{"location":"nbis_annotation/practical_session/practical2_sub_gatherEvidence/#obtaining-est","text":"EST data is not commonly generated anymore, but may become useful for some projects where such data is still available. Examples may include older genomes targeted for re-annotation or genomes with available EST data for closely related species. The NCBI or EBI websites are the most appropriate places to retrieve such kind of data. Exercise 5 - NCBI: Go to the NCBI website and find how many ESTs are available for the drosophila melanogaster species.","title":"Obtaining EST"},{"location":"nbis_annotation/practical_session/practical2_sub_gatherEvidence/#obtaining-rna-seq","text":"Commonly, such data are produced within the project you are working on. Otherwise the most appropriate data could be retrieved on the Sequence Read Archive (SRA) website from the NCBI or the European Nucleotide Archive (ENA) from the EBI.","title":"Obtaining RNA-seq"},{"location":"nbis_annotation/practical_session/practical2_sub_makerAbinit/","text":"Making an abinitio evidence-driven annotation with MAKER The recommended way of running Maker is in combination with one or more ab-initio profile models. Maker natively supports input from several tools, including augustus, snap and genemark. The choice of tool depends a bit on the organism that you are annotating - for example, GeneMark-ES is mostly recommended for fungi, whereas augustus and snap have a more general use. The biggest problem with ab-initio models is the process of training them. It is usually recommended to have somewhere around 500-1000 curated gene models for this purpose. Naturally, this is a bit of a contradiction for a not-yet annotated genome. However, if one or more good ab-initio profiles are available, they can potentially greatly enhance the quality of an annotation by filling in the blanks left by missing evidence. Interestingly, Maker even works with ab-initio profiles from somewhat distantly related species since it can create so-called hints from the evidence alignments, which the gene predictor can take into account to fine-tune the predictions. Usually when no close ab-initio profile exists for the investigated species, we use the first round of annotation (evidence based) to create one. We first filter the best gene models from this annotation, which are used then to train the abinitio tools of our choice. In order to compare the performance of Maker with and without ab-initio predictions in a real-world scenario, we have first run a gene build without ab-initio predictions. Now, we run a similar analysis but enable ab-initio predictions through augustus. Prepare the input data No need to re-compute the mapping/alignment of the different lines of evidence. Indeed, this time consuming task has already been performed during the previous round of annotation (evidence based). So, we will use the corresponding gff files previously produced by MAKER. Link the gff files you want to use into your folder: repeatmasker.chr4.gff (already present) repeatrunner.chr4.gff (already present) 4.fa (already present) est_gff_stringtie.gff (transcript that have been mapped by MAKER during the evidence based round of annotation) est2genome.gff protein2genome.gff ln -s maker_no_abinitio/annotationByType/est_gff:stringtie.gff est_gff_stringtie.gff ln -s maker_no_abinitio/annotationByType/est2genome.gff ln -s maker_no_abinitio/annotationByType/protein2genome.gff This time, we do specify a reference species to be used by augustus, which will enable ab-initio gene finding and keep_preds=1 will also show abinitio prediction not supported by any evidences : augustus_species=fly #Augustus gene prediction species model (this is where you can call the database you trained for augustus) ... protein2genome=0 est2genome=0 keep_preds=1 With these settings, Maker will run augustus to predict gene loci, but inform these predictions with information from the protein and est alignments. Before running MAKER you can check you have modified the maker_opts.ctl file properly here . Run Maker with ab-initio predictions With everything configured, run Maker as you did for the previous analysis: mpiexec -n 8 maker We probably expect this to take a little bit longer than before, since we have added another step to our analysis. Compile the output When Maker has finished, compile the output: maker_merge_outputs_from_datastore.pl --output maker_with_abinitio And again, it is probably best to link the resulting output (maker.gff) to a result folder (the same as defined in the previous exercise e.g. dmel_results), under a descriptive name. Inspect the gene models To get some statistics of your annotation you could launch : gff3_sp_statistics.pl --gff maker_with_abinitio/annotationByType/maker.gff We could now also visualise the annotation in the Webapollo genome browser.","title":"Abinitio Annotation"},{"location":"nbis_annotation/practical_session/practical2_sub_makerAbinit/#making-an-abinitio-evidence-driven-annotation-with-maker","text":"The recommended way of running Maker is in combination with one or more ab-initio profile models. Maker natively supports input from several tools, including augustus, snap and genemark. The choice of tool depends a bit on the organism that you are annotating - for example, GeneMark-ES is mostly recommended for fungi, whereas augustus and snap have a more general use. The biggest problem with ab-initio models is the process of training them. It is usually recommended to have somewhere around 500-1000 curated gene models for this purpose. Naturally, this is a bit of a contradiction for a not-yet annotated genome. However, if one or more good ab-initio profiles are available, they can potentially greatly enhance the quality of an annotation by filling in the blanks left by missing evidence. Interestingly, Maker even works with ab-initio profiles from somewhat distantly related species since it can create so-called hints from the evidence alignments, which the gene predictor can take into account to fine-tune the predictions. Usually when no close ab-initio profile exists for the investigated species, we use the first round of annotation (evidence based) to create one. We first filter the best gene models from this annotation, which are used then to train the abinitio tools of our choice. In order to compare the performance of Maker with and without ab-initio predictions in a real-world scenario, we have first run a gene build without ab-initio predictions. Now, we run a similar analysis but enable ab-initio predictions through augustus.","title":"Making an abinitio evidence-driven annotation with MAKER"},{"location":"nbis_annotation/practical_session/practical2_sub_makerAbinit/#prepare-the-input-data","text":"No need to re-compute the mapping/alignment of the different lines of evidence. Indeed, this time consuming task has already been performed during the previous round of annotation (evidence based). So, we will use the corresponding gff files previously produced by MAKER. Link the gff files you want to use into your folder: repeatmasker.chr4.gff (already present) repeatrunner.chr4.gff (already present) 4.fa (already present) est_gff_stringtie.gff (transcript that have been mapped by MAKER during the evidence based round of annotation) est2genome.gff protein2genome.gff ln -s maker_no_abinitio/annotationByType/est_gff:stringtie.gff est_gff_stringtie.gff ln -s maker_no_abinitio/annotationByType/est2genome.gff ln -s maker_no_abinitio/annotationByType/protein2genome.gff This time, we do specify a reference species to be used by augustus, which will enable ab-initio gene finding and keep_preds=1 will also show abinitio prediction not supported by any evidences : augustus_species=fly #Augustus gene prediction species model (this is where you can call the database you trained for augustus) ... protein2genome=0 est2genome=0 keep_preds=1 With these settings, Maker will run augustus to predict gene loci, but inform these predictions with information from the protein and est alignments. Before running MAKER you can check you have modified the maker_opts.ctl file properly here .","title":"Prepare the input data"},{"location":"nbis_annotation/practical_session/practical2_sub_makerAbinit/#run-maker-with-ab-initio-predictions","text":"With everything configured, run Maker as you did for the previous analysis: mpiexec -n 8 maker We probably expect this to take a little bit longer than before, since we have added another step to our analysis.","title":"Run Maker with ab-initio predictions"},{"location":"nbis_annotation/practical_session/practical2_sub_makerAbinit/#compile-the-output","text":"When Maker has finished, compile the output: maker_merge_outputs_from_datastore.pl --output maker_with_abinitio And again, it is probably best to link the resulting output (maker.gff) to a result folder (the same as defined in the previous exercise e.g. dmel_results), under a descriptive name.","title":"Compile the output"},{"location":"nbis_annotation/practical_session/practical2_sub_makerAbinit/#inspect-the-gene-models","text":"To get some statistics of your annotation you could launch : gff3_sp_statistics.pl --gff maker_with_abinitio/annotationByType/maker.gff We could now also visualise the annotation in the Webapollo genome browser.","title":"Inspect the gene models"},{"location":"nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/","text":"Comparing and evaluating annotations In this exercise you will take the three annotations you have created for Drosophila - the pure abinitio one done with augustus, the evidence-based done with MAKER and the abinitio evidence drived one done with MAKER. First, we will count the features annotated in each of them and compare that number against the existing reference annotation. Next, we will perform a proper feature-level comparison to obtain a proper estimate of congruency. Evaluating annotation quality Evaluating an annotation can be done in three ways - running busco but with the proteins obtained from the annotation, in comparison with another annotation or in reference to the evidence alignments. The former isn't so much a quality check as a measure of congruency - i.e. the resulting numbers don't tell you which of the two gene builds is more correct. On the other hand, a comparison with evidence alignments is what Maker uses internally to select gene models. After synthesizing and annotating loci, the resulting model will be ranked against the filtered evidence alignments. The more congruent these two points of information are, the lower the 'annotation edit distance' (AED) will be. The AED score can be used to e.g. check an annotation for problematic models that may then be subjected to manual curation. BUSCO BUSCO is run before annotating to check if the assembly is good and therefore if the annotation will be good. It is also run after the structural annotation to then compare if we indeed find a number of genes corresponding of the first run of busco. You will need to link the protein file created by maker on the run with the ab-initio cd ~/annotation_course/practical2 mkdir busco cd busco ln -s ../maker/maker_with_abinitio/annotations.proteins.fa ln -s ~/annotation_course/practical1/busco/metazoa_odb9 BUSCO.py -i annotations.proteins.fa -o dmel_maker_abinitio -m prot -c 8 -l metazoa_odb9 if you compare with you first busco results what do you see? Comparing annotations As with many tasks within bioinformatics, it is always a great idea to first look around for existing solutions. In the case of comparing annotations, there are in fact options already out there. One such example is genometools, which we have briefly used before. Preparing the input files First you have to be situated in a folder containing the two maker annotations (with and without ab initio) and the augustus annotation. cd ~/annotation_course/practical2 mkdir compare cd compare ln -s ../maker/maker_no_abinitio/annotationByType/maker.gff maker_no_abinitio.gff ln -s ../maker/maker_with_abinitio/annotationByType/maker.gff maker_abinitio.gff Then, copy or sym-link the EnsEMBL reference annotation. ln -s ~/annotation_course/data/annotation/ensembl.chr4.gff Now we have to sort any GFF3-formatted annotation in a way that genometools accepts. sed -i '1i##gff-version 3' maker_no_abinitio.gff sed -i '1i##gff-version 3' maker_abinitio.gff gt gff3 -sort maker_no_abinitio.gff > maker_no_abinitio.sorted.gff gt gff3 -sort maker_abinitio.gff > maker_abinitio.sorted.gff gt gff3 -sort ensembl.chr4.gff > ensembl.sorted.gff Counting features Next, we get the feature counts for the three annotations and the reference from EnsEMBL: gt stat maker_no_abinitio.gff or gff3_sp_statistics.pl --gff maker_no_abinitio.gff (or whatever you decided to name the file(s). The use of the sorted file or the original one changes nothing here) As you will note, there are some differences - and of course, this is expected, since we used quite different approaches to generate the two gene builds. EnsEMBL on the other hand is originally imported from FlyBase. Obviously, a lot of manual labor and much more data has been put into the FlyBase annotation - and this highlights a common limitation of any computational pipeline. You will simply never reach the same level of quality and detail as seen in a manually curated reference annotation. Pairwise comparison of features But feature counts alone can't really give you a clear measure of overlap/differences between any two annotations. In order to properly compare them, we can use another function included in genometools. With the sorted files, we can now perform a comparison: gt eval ensembl.sorted.gff maker_no_abinitio.sorted.gff This will create a long list of measures for all relevant sequence features with respect to both the 'sensitivity' and 'specificity' - as a measure of how close the annotation comes to a reference. As a reminder, 'specificity' measures the fraction of a reference overlapping a prediction whereas 'sensitivity' measures the fraction of a prediction overlapping a reference. Note that the measures employed by genometools function in a all-or-nothing fashion. If the overlap is not 100%, it doesn't count (which is why you are unlikely to find gene-level congruencies between your gene builds and the reference annotation). From the comparison of your annotations to the Ensembl annotation, which one seems to be the most comprehensive to you ? Visualising annotations Note: The following section overlaps with some of the exercises you have done earlier (comparing augustus predictions against the reference annotation). In the previous tasks, we have looked at the overlap between different gene builds. While this gives us an indication of how similar two annotations are, it doesn't really allow us to judge the overall quality and similarity of annotations . Remember, sensitivity and specificity are 'all-or-nothing' - two annotations may be considered very different, but provide similar information, biologically. By that, we mean that two gene models don't need to be 100% identical in their coordinates to tell the scientist that a gene indeed exists in a given location and what it's product looks like. We therefore need to visually inspect and compare the gene builds. This is a crucial step in any annotation project - gene build pipelines use a set of defined rules, but human pattern recognition is needed to spot potential systematic errors. For example, a pipeline like Maker will simply take all your input and try to synthesize it into an annotation, but it doesn't do too much checks on the data itself. What if you RNA-seq data is messier than you thought? What if your protein data set includes to many 'predicted' proteins that are in clear conflict with the other data? There exist a number of 'annotation viewers' - IGV, Argo and Apollo, to name a few. A common choice for annotators is the web-based version of Apollo, WebApollo, mostly for its curation capabilities. Using WebApollo to view annotations Transfer your maker annotation files to your computer using the scp command. Then, jump to WebApollo and upload your annotation track into the genome portal called drosophila_melanogaster_chr4 . Here find the WebApollo instruction You can now compare your gene builds against this reference. Some questions to ask yourself: Do my gene builds recover all the genes found in the reference? What sort of differences are most common?","title":"Comparing Annotations"},{"location":"nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#comparing-and-evaluating-annotations","text":"In this exercise you will take the three annotations you have created for Drosophila - the pure abinitio one done with augustus, the evidence-based done with MAKER and the abinitio evidence drived one done with MAKER. First, we will count the features annotated in each of them and compare that number against the existing reference annotation. Next, we will perform a proper feature-level comparison to obtain a proper estimate of congruency.","title":"Comparing and evaluating annotations"},{"location":"nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#evaluating-annotation-quality","text":"Evaluating an annotation can be done in three ways - running busco but with the proteins obtained from the annotation, in comparison with another annotation or in reference to the evidence alignments. The former isn't so much a quality check as a measure of congruency - i.e. the resulting numbers don't tell you which of the two gene builds is more correct. On the other hand, a comparison with evidence alignments is what Maker uses internally to select gene models. After synthesizing and annotating loci, the resulting model will be ranked against the filtered evidence alignments. The more congruent these two points of information are, the lower the 'annotation edit distance' (AED) will be. The AED score can be used to e.g. check an annotation for problematic models that may then be subjected to manual curation.","title":"Evaluating annotation quality"},{"location":"nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#busco","text":"BUSCO is run before annotating to check if the assembly is good and therefore if the annotation will be good. It is also run after the structural annotation to then compare if we indeed find a number of genes corresponding of the first run of busco. You will need to link the protein file created by maker on the run with the ab-initio cd ~/annotation_course/practical2 mkdir busco cd busco ln -s ../maker/maker_with_abinitio/annotations.proteins.fa ln -s ~/annotation_course/practical1/busco/metazoa_odb9 BUSCO.py -i annotations.proteins.fa -o dmel_maker_abinitio -m prot -c 8 -l metazoa_odb9 if you compare with you first busco results what do you see?","title":"BUSCO"},{"location":"nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#comparing-annotations","text":"As with many tasks within bioinformatics, it is always a great idea to first look around for existing solutions. In the case of comparing annotations, there are in fact options already out there. One such example is genometools, which we have briefly used before.","title":"Comparing annotations"},{"location":"nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#preparing-the-input-files","text":"First you have to be situated in a folder containing the two maker annotations (with and without ab initio) and the augustus annotation. cd ~/annotation_course/practical2 mkdir compare cd compare ln -s ../maker/maker_no_abinitio/annotationByType/maker.gff maker_no_abinitio.gff ln -s ../maker/maker_with_abinitio/annotationByType/maker.gff maker_abinitio.gff Then, copy or sym-link the EnsEMBL reference annotation. ln -s ~/annotation_course/data/annotation/ensembl.chr4.gff Now we have to sort any GFF3-formatted annotation in a way that genometools accepts. sed -i '1i##gff-version 3' maker_no_abinitio.gff sed -i '1i##gff-version 3' maker_abinitio.gff gt gff3 -sort maker_no_abinitio.gff > maker_no_abinitio.sorted.gff gt gff3 -sort maker_abinitio.gff > maker_abinitio.sorted.gff gt gff3 -sort ensembl.chr4.gff > ensembl.sorted.gff","title":"Preparing the input files"},{"location":"nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#counting-features","text":"Next, we get the feature counts for the three annotations and the reference from EnsEMBL: gt stat maker_no_abinitio.gff or gff3_sp_statistics.pl --gff maker_no_abinitio.gff (or whatever you decided to name the file(s). The use of the sorted file or the original one changes nothing here) As you will note, there are some differences - and of course, this is expected, since we used quite different approaches to generate the two gene builds. EnsEMBL on the other hand is originally imported from FlyBase. Obviously, a lot of manual labor and much more data has been put into the FlyBase annotation - and this highlights a common limitation of any computational pipeline. You will simply never reach the same level of quality and detail as seen in a manually curated reference annotation.","title":"Counting features"},{"location":"nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#pairwise-comparison-of-features","text":"But feature counts alone can't really give you a clear measure of overlap/differences between any two annotations. In order to properly compare them, we can use another function included in genometools. With the sorted files, we can now perform a comparison: gt eval ensembl.sorted.gff maker_no_abinitio.sorted.gff This will create a long list of measures for all relevant sequence features with respect to both the 'sensitivity' and 'specificity' - as a measure of how close the annotation comes to a reference. As a reminder, 'specificity' measures the fraction of a reference overlapping a prediction whereas 'sensitivity' measures the fraction of a prediction overlapping a reference. Note that the measures employed by genometools function in a all-or-nothing fashion. If the overlap is not 100%, it doesn't count (which is why you are unlikely to find gene-level congruencies between your gene builds and the reference annotation). From the comparison of your annotations to the Ensembl annotation, which one seems to be the most comprehensive to you ?","title":"Pairwise comparison of features"},{"location":"nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#visualising-annotations","text":"Note: The following section overlaps with some of the exercises you have done earlier (comparing augustus predictions against the reference annotation). In the previous tasks, we have looked at the overlap between different gene builds. While this gives us an indication of how similar two annotations are, it doesn't really allow us to judge the overall quality and similarity of annotations . Remember, sensitivity and specificity are 'all-or-nothing' - two annotations may be considered very different, but provide similar information, biologically. By that, we mean that two gene models don't need to be 100% identical in their coordinates to tell the scientist that a gene indeed exists in a given location and what it's product looks like. We therefore need to visually inspect and compare the gene builds. This is a crucial step in any annotation project - gene build pipelines use a set of defined rules, but human pattern recognition is needed to spot potential systematic errors. For example, a pipeline like Maker will simply take all your input and try to synthesize it into an annotation, but it doesn't do too much checks on the data itself. What if you RNA-seq data is messier than you thought? What if your protein data set includes to many 'predicted' proteins that are in clear conflict with the other data? There exist a number of 'annotation viewers' - IGV, Argo and Apollo, to name a few. A common choice for annotators is the web-based version of Apollo, WebApollo, mostly for its curation capabilities.","title":"Visualising annotations"},{"location":"nbis_annotation/practical_session/practical2_sub_makerCompareAnnot/#using-webapollo-to-view-annotations","text":"Transfer your maker annotation files to your computer using the scp command. Then, jump to WebApollo and upload your annotation track into the genome portal called drosophila_melanogaster_chr4 . Here find the WebApollo instruction You can now compare your gene builds against this reference. Some questions to ask yourself: Do my gene builds recover all the genes found in the reference? What sort of differences are most common?","title":"Using WebApollo to view annotations"},{"location":"nbis_annotation/practical_session/practical2_sub_makerNoAbinit/","text":"Making an evidence based annotation with MAKER Overview The first run of Maker will be done without ab-initio predictions. What are your expectations for the resulting gene build? In essence, we are attempting a purely evidence-based annotation, where the best protein- and EST-alignments are chosen to build the most likely gene models. The purpose of an evidence-based annotation is simple. Basically, you may try to annotate an organism where no usable ab-initio model is available. The evidence-based annotation can then be used to create a set of genes on which a new model could be trained on (using e.g. Snap or Augustus). Selection of genes for training can be based on the annotation edit distance (AED score), which says something about how great the distance between a gene model and the evidence alignments is. A score of 0.0 would essentially say that the final model is in perfect agreement with the evidence. Let's do this step-by-step: Prepare the input data Link the raw computes you want to use into your folder. The files you will need are: the gff file of the pre-computed repeats (coordinates of repeatmasked regions) ln -s ~/annotation_course/data/raw_computes/repeatmasker.chr4.gff ln -s ~/annotation_course/data/raw_computes/repeatrunner.chr4.gff In addition, you will also need the genome sequence. ln -s ~/annotation_course/data/genome/4.fa Then you will also need EST and protein fasta file: ln -s ~/annotation_course/data/evidence/est.chr4.fa ln -s ~/annotation_course/data/evidence/proteins.chr4.fa To finish you will could use a transcriptome assembly (This one has been made using Stringtie): ln -s ~/annotation_course/data/RNAseq/stringtie/stringtie2genome.chr4.gff /!\\ Always check that the gff files you provides as protein or EST contains match / match_part (gff alignment type ) feature types rather than genes/transcripts (gff annotation type) otherwise MAKER will not use the contained data properly. Here we have to fix the stringtie gff file. gff3_sp_alignment_output_style.pl --gff stringtie2genome.chr4.gff -o stringtie2genome.chr4.ok.gff You should now have 1 repeat file, 1 EST file, 1 protein file, 1 transcript file, and the genome sequence in the working directory. For Maker to use this information, we need create the three config files, typing this command: maker -CTL You can leave the two files controlling external software behaviors untouched. In the actual maker options file called maker_opts.ctl , we need to provide: name of the genome sequence (genome=) name of the 'EST' file in fasta format (est=) name of the 'Transcript' file in gff format (est_gff=) name of the 'Protein' set file(s) (protein=) name of the repeatmasker and repeatrunner files (rm_gff=) You can list multiple files in one field by separating their names by a comma ','. This time, we do not specify a reference species to be used by augustus, which will disable ab-initio gene finding. Instead we set: protein2genome=1 est2genome=1 This will enable gene building directly from the evidence alignments. To edit the maker_opts.ctl file you can use the nano text editor: nano maker_opts.ctl Before running MAKER you can check you have modified the maker_opts.ctl file properly here . /!\\ Be sure to have deactivated the parameters model_org= # and repeat_protein= # to avoid the heavy work of repeatmasker. Run Maker If your maker_opts.ctl is configured correctly, you should be able to run maker: mpiexec -n 8 maker This will start Maker on 8 cores, if everything is configured correctly. This will take a little while and process a lot of output to the screen. Luckily, much of the heavy work - such as repeat masking - are already done, so the total running time is quite manageable, even on a small number of cores. Inspect the output (optional) Here you can find details about the MAKER output. Compile the output Once Maker is finished, compile the annotation: maker_merge_outputs_from_datastore.pl --output maker_no_abinitio We have specified a name for the output directory since we will be creating more than one annotation and need to be able to tell them apart. This should create a \"maker_no_abinitio\" directory containing a maker annotation file together with the matching protein predictions file and a sub-directory containing different annotation files including the maker.gff which is the result to keep from this analysis. => You could sym-link the maker.gff file to another folder called e.g. dmel_results, so everything is in the same place in the end. Just make sure to call the link something other than maker.gff, since any maker output will be called that. Inspect the gene models To get some statistics of your annotation you could launch : gff3_sp_statistics.pl --gff maker_no_abinitio/annotationByType/maker.gff We could now also visualise the annotation in the Webapollo genome browser.","title":"Evidence Based Annotation"},{"location":"nbis_annotation/practical_session/practical2_sub_makerNoAbinit/#making-an-evidence-based-annotation-with-maker","text":"","title":"Making an evidence based annotation with MAKER"},{"location":"nbis_annotation/practical_session/practical2_sub_makerNoAbinit/#overview","text":"The first run of Maker will be done without ab-initio predictions. What are your expectations for the resulting gene build? In essence, we are attempting a purely evidence-based annotation, where the best protein- and EST-alignments are chosen to build the most likely gene models. The purpose of an evidence-based annotation is simple. Basically, you may try to annotate an organism where no usable ab-initio model is available. The evidence-based annotation can then be used to create a set of genes on which a new model could be trained on (using e.g. Snap or Augustus). Selection of genes for training can be based on the annotation edit distance (AED score), which says something about how great the distance between a gene model and the evidence alignments is. A score of 0.0 would essentially say that the final model is in perfect agreement with the evidence. Let's do this step-by-step:","title":"Overview"},{"location":"nbis_annotation/practical_session/practical2_sub_makerNoAbinit/#prepare-the-input-data","text":"Link the raw computes you want to use into your folder. The files you will need are: the gff file of the pre-computed repeats (coordinates of repeatmasked regions) ln -s ~/annotation_course/data/raw_computes/repeatmasker.chr4.gff ln -s ~/annotation_course/data/raw_computes/repeatrunner.chr4.gff In addition, you will also need the genome sequence. ln -s ~/annotation_course/data/genome/4.fa Then you will also need EST and protein fasta file: ln -s ~/annotation_course/data/evidence/est.chr4.fa ln -s ~/annotation_course/data/evidence/proteins.chr4.fa To finish you will could use a transcriptome assembly (This one has been made using Stringtie): ln -s ~/annotation_course/data/RNAseq/stringtie/stringtie2genome.chr4.gff /!\\ Always check that the gff files you provides as protein or EST contains match / match_part (gff alignment type ) feature types rather than genes/transcripts (gff annotation type) otherwise MAKER will not use the contained data properly. Here we have to fix the stringtie gff file. gff3_sp_alignment_output_style.pl --gff stringtie2genome.chr4.gff -o stringtie2genome.chr4.ok.gff You should now have 1 repeat file, 1 EST file, 1 protein file, 1 transcript file, and the genome sequence in the working directory. For Maker to use this information, we need create the three config files, typing this command: maker -CTL You can leave the two files controlling external software behaviors untouched. In the actual maker options file called maker_opts.ctl , we need to provide: name of the genome sequence (genome=) name of the 'EST' file in fasta format (est=) name of the 'Transcript' file in gff format (est_gff=) name of the 'Protein' set file(s) (protein=) name of the repeatmasker and repeatrunner files (rm_gff=) You can list multiple files in one field by separating their names by a comma ','. This time, we do not specify a reference species to be used by augustus, which will disable ab-initio gene finding. Instead we set: protein2genome=1 est2genome=1 This will enable gene building directly from the evidence alignments. To edit the maker_opts.ctl file you can use the nano text editor: nano maker_opts.ctl Before running MAKER you can check you have modified the maker_opts.ctl file properly here . /!\\ Be sure to have deactivated the parameters model_org= # and repeat_protein= # to avoid the heavy work of repeatmasker.","title":"Prepare the input data"},{"location":"nbis_annotation/practical_session/practical2_sub_makerNoAbinit/#run-maker","text":"If your maker_opts.ctl is configured correctly, you should be able to run maker: mpiexec -n 8 maker This will start Maker on 8 cores, if everything is configured correctly. This will take a little while and process a lot of output to the screen. Luckily, much of the heavy work - such as repeat masking - are already done, so the total running time is quite manageable, even on a small number of cores.","title":"Run Maker"},{"location":"nbis_annotation/practical_session/practical2_sub_makerNoAbinit/#inspect-the-output-optional","text":"Here you can find details about the MAKER output.","title":"Inspect the output (optional)"},{"location":"nbis_annotation/practical_session/practical2_sub_makerNoAbinit/#compile-the-output","text":"Once Maker is finished, compile the annotation: maker_merge_outputs_from_datastore.pl --output maker_no_abinitio We have specified a name for the output directory since we will be creating more than one annotation and need to be able to tell them apart. This should create a \"maker_no_abinitio\" directory containing a maker annotation file together with the matching protein predictions file and a sub-directory containing different annotation files including the maker.gff which is the result to keep from this analysis. => You could sym-link the maker.gff file to another folder called e.g. dmel_results, so everything is in the same place in the end. Just make sure to call the link something other than maker.gff, since any maker output will be called that.","title":"Compile the output"},{"location":"nbis_annotation/practical_session/practical2_sub_makerNoAbinit/#inspect-the-gene-models","text":"To get some statistics of your annotation you could launch : gff3_sp_statistics.pl --gff maker_no_abinitio/annotationByType/maker.gff We could now also visualise the annotation in the Webapollo genome browser.","title":"Inspect the gene models"},{"location":"nbis_annotation/practical_session/practical2_sub_transcriptome/","text":"Assembling transcripts based on RNA-seq data Rna-seq data is in general very useful in annotation projects as the data usually comes from the actual organism you are studying and thus avoids the danger of introducing errors caused by differences in gene structure between your study organism and other species. Important remarks to remember before starting working with RNA-seq: - Check if RNAseq are paired or not. Last generation of sequenced short reads (since 2013) are almost all paired. Anyway, it is important to check that information, which will be useful for the tools used in the next steps. - Check if RNAseq are stranded. Indeed this information will be useful for the tools used in the next steps. (In general way we recommend to use stranded RNAseq to avoid transcript fusion during the transcript assembly process. That gives more reliable results. ) - Left / L / forward / 1 are identical meaning. It is the same for Right / R /Reverse / 2 First create a dedicated folder to work in: cd ~/annotation_course/practical2 mkdir RNAseq cd RNAseq 1. Genome guided transcriptome assembly: Checking encoding version and fastq quality score format To check the technology used to sequences the RNAseq and get some extra information we have to use fastqc tool. mkdir fastqc cd fastqc mkdir fastqc_reports fastqc ~/annotation_course/data/RNAseq/fastq/ERR305399.left.fastq.gz -o fastqc_reports/ Transfer the html file resulting of fastqc to your computer using scp in another terminal: scp -i ~/.ssh/azure_rsa student@__IP__:/home/student/annotation_course/practical1/augustus/augustus_drosophila.gff . Open it. What kind of result do you have? Checking the fastq quality score format fastq_guessMyFormat.pl -i ~/annotation_course/data/RNAseq/fastq/ERR305399.left.fastq.gz In the normal mode, it differentiates between Sanger/Illumina1.8+ and Solexa/Illumina1.3+/Illumina1.5+. In the advanced mode, it will try to pinpoint exactly which scoring system is used. More test can be made and should be made on RNA-seq data before doing the assembly, we have not time to do all of them during this course. have a look here Trimmomatic (trimming reads) Trimmomatic performs a variety of useful trimming tasks for illumina paired-end and single ended data.The selection of trimming steps and their associated parameters are supplied on the command line. The following command line will perform the following: \u2022 Remove adapters (ILLUMINACLIP:TruSeq3-PE.fa:2:30:10) \u2022 Remove leading low quality or N bases (below quality 3) (LEADING:3) \u2022 Remove trailing low quality or N bases (below quality 3) (TRAILING:3) \u2022 Scan the read with a 4-base wide sliding window, cutting when the average quality per base drops below 15 (SLIDINGWINDOW:4:15) \u2022 Drop reads below the 36 bases long (MINLEN:36) cd ~/annotation_course/practical2/RNAseq mkdir trimmomatic cd trimmomatic java -jar trimmomatic-0.32.jar PE -threads 8 ~/annotation_course/data/RNAseq/fastq/ERR305399.left.fastq.gz ~/annotation_course/data/RNAseq/fastq/ERR305399.right.fastq.gz ERR305399.left_paired.fastq.gz ERR305399.left_unpaired.fastq.gz ERR305399.right_paired.fastq.gz ERR305399.right_unpaired.fastq.gz ILLUMINACLIP:trimmomatic/0.32/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36 Tophat (splice-aware mapping reads to genome) Once the reads have been trimmed, we use tophat to align the RNA-seq reads to a genome in order to identify exon-exon splice junctions. It is built on the ultrafast short read mapping program Bowtie . cd ~/annotation_course/practical2/RNAseq mkdir tophat cd tophat tophat --library-type=fr-firststrand ~/annotation_course/data/genome/4.fa ../trimmomatic/ERR305399.left_paired.fastq.gz ../trimmomatic/ERR305399.right_paired.fastq.gz -p 8 This step will take a really long time so you can use the bam file located here ~/annotation_course/data/RNAseq/tophat/accepted_hits.bam Stringtie (Assembling reads into transcripts) StringTie is a fast and highly efficient assembler of RNA-Seq alignments into potential transcripts. It uses a novel network flow algorithm as well as an optional de novo assembly step to assemble and quantitate full-length transcripts representing multiple splice variants for each gene locus. Its input can include not only the alignments of raw reads used by other transcript assemblers, but also alignments longer sequences that have been assembled from those reads. cd ~/annotation_course/practical2/RNAseq mkdir stringtie cd stringtie stringtie ../tophat/tophat_out/accepted_hits.bam -o outdir/transcripts.gtf When done you can find your results in the directory \u2018outdir\u2019. The file transcripts.gtf includes your assembled transcripts. As Webapollo doesn't like the gtf format file you should convert it in gff3 format. gxf_to_gff3.pl --gff transcripts.gtf -o transcripts.gff3 Then, transfer the gff3 file to your computer and load it into Webapollo . How well does it compare with your Augustus results? Looking at your results, are you happy with the default values of Stringtie (which we used in this exercise) or is there something you would like to change? 2. De-novo transcriptome assembly: Trinity Trinity assemblies can be used as complementary evidence, particularly when trying to polish a gene build with Pasa. Before you start, check how big the raw read data is that you wish to assemble to avoid unreasonably long run times. cd ~/annotation_course/practical2/RNAseq mkdir trinity cd trinity Trinity --seqType fq --max_memory 64G --left ~/annotation_course/data/RNAseq/ERR305399.left.fastq.gz --right ~/annotation_course/data/RNAseq/ERR305399.right.fastq.gz --CPU 8 --output trinity_result --SS_lib_type RF Trinity takes a long time to run if you want to have a look at the results, look in ~/annotation_course/course_material/data/dmel/chromosome_4/RNAseq/ the output that will be used later on for the annotation will be Trinity.fasta Closing remarks You have now successfully perform transcript assemblies. You have seen how to perform a genome-guided assembly as well as de-no assembly.","title":"Assembling transcripts"},{"location":"nbis_annotation/practical_session/practical2_sub_transcriptome/#assembling-transcripts-based-on-rna-seq-data","text":"Rna-seq data is in general very useful in annotation projects as the data usually comes from the actual organism you are studying and thus avoids the danger of introducing errors caused by differences in gene structure between your study organism and other species. Important remarks to remember before starting working with RNA-seq: - Check if RNAseq are paired or not. Last generation of sequenced short reads (since 2013) are almost all paired. Anyway, it is important to check that information, which will be useful for the tools used in the next steps. - Check if RNAseq are stranded. Indeed this information will be useful for the tools used in the next steps. (In general way we recommend to use stranded RNAseq to avoid transcript fusion during the transcript assembly process. That gives more reliable results. ) - Left / L / forward / 1 are identical meaning. It is the same for Right / R /Reverse / 2 First create a dedicated folder to work in: cd ~/annotation_course/practical2 mkdir RNAseq cd RNAseq","title":"Assembling transcripts based on RNA-seq data"},{"location":"nbis_annotation/practical_session/practical2_sub_transcriptome/#1-genome-guided-transcriptome-assembly","text":"","title":"1. Genome guided transcriptome assembly:"},{"location":"nbis_annotation/practical_session/practical2_sub_transcriptome/#checking-encoding-version-and-fastq-quality-score-format","text":"To check the technology used to sequences the RNAseq and get some extra information we have to use fastqc tool. mkdir fastqc cd fastqc mkdir fastqc_reports fastqc ~/annotation_course/data/RNAseq/fastq/ERR305399.left.fastq.gz -o fastqc_reports/ Transfer the html file resulting of fastqc to your computer using scp in another terminal: scp -i ~/.ssh/azure_rsa student@__IP__:/home/student/annotation_course/practical1/augustus/augustus_drosophila.gff . Open it. What kind of result do you have? Checking the fastq quality score format fastq_guessMyFormat.pl -i ~/annotation_course/data/RNAseq/fastq/ERR305399.left.fastq.gz In the normal mode, it differentiates between Sanger/Illumina1.8+ and Solexa/Illumina1.3+/Illumina1.5+. In the advanced mode, it will try to pinpoint exactly which scoring system is used. More test can be made and should be made on RNA-seq data before doing the assembly, we have not time to do all of them during this course. have a look here","title":"Checking encoding version and fastq quality score format"},{"location":"nbis_annotation/practical_session/practical2_sub_transcriptome/#trimmomatic-trimming-reads","text":"Trimmomatic performs a variety of useful trimming tasks for illumina paired-end and single ended data.The selection of trimming steps and their associated parameters are supplied on the command line. The following command line will perform the following: \u2022 Remove adapters (ILLUMINACLIP:TruSeq3-PE.fa:2:30:10) \u2022 Remove leading low quality or N bases (below quality 3) (LEADING:3) \u2022 Remove trailing low quality or N bases (below quality 3) (TRAILING:3) \u2022 Scan the read with a 4-base wide sliding window, cutting when the average quality per base drops below 15 (SLIDINGWINDOW:4:15) \u2022 Drop reads below the 36 bases long (MINLEN:36) cd ~/annotation_course/practical2/RNAseq mkdir trimmomatic cd trimmomatic java -jar trimmomatic-0.32.jar PE -threads 8 ~/annotation_course/data/RNAseq/fastq/ERR305399.left.fastq.gz ~/annotation_course/data/RNAseq/fastq/ERR305399.right.fastq.gz ERR305399.left_paired.fastq.gz ERR305399.left_unpaired.fastq.gz ERR305399.right_paired.fastq.gz ERR305399.right_unpaired.fastq.gz ILLUMINACLIP:trimmomatic/0.32/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36","title":"Trimmomatic (trimming reads)"},{"location":"nbis_annotation/practical_session/practical2_sub_transcriptome/#tophat-splice-aware-mapping-reads-to-genome","text":"Once the reads have been trimmed, we use tophat to align the RNA-seq reads to a genome in order to identify exon-exon splice junctions. It is built on the ultrafast short read mapping program Bowtie . cd ~/annotation_course/practical2/RNAseq mkdir tophat cd tophat tophat --library-type=fr-firststrand ~/annotation_course/data/genome/4.fa ../trimmomatic/ERR305399.left_paired.fastq.gz ../trimmomatic/ERR305399.right_paired.fastq.gz -p 8 This step will take a really long time so you can use the bam file located here ~/annotation_course/data/RNAseq/tophat/accepted_hits.bam","title":"Tophat (splice-aware mapping reads to genome)"},{"location":"nbis_annotation/practical_session/practical2_sub_transcriptome/#stringtie-assembling-reads-into-transcripts","text":"StringTie is a fast and highly efficient assembler of RNA-Seq alignments into potential transcripts. It uses a novel network flow algorithm as well as an optional de novo assembly step to assemble and quantitate full-length transcripts representing multiple splice variants for each gene locus. Its input can include not only the alignments of raw reads used by other transcript assemblers, but also alignments longer sequences that have been assembled from those reads. cd ~/annotation_course/practical2/RNAseq mkdir stringtie cd stringtie stringtie ../tophat/tophat_out/accepted_hits.bam -o outdir/transcripts.gtf When done you can find your results in the directory \u2018outdir\u2019. The file transcripts.gtf includes your assembled transcripts. As Webapollo doesn't like the gtf format file you should convert it in gff3 format. gxf_to_gff3.pl --gff transcripts.gtf -o transcripts.gff3 Then, transfer the gff3 file to your computer and load it into Webapollo . How well does it compare with your Augustus results? Looking at your results, are you happy with the default values of Stringtie (which we used in this exercise) or is there something you would like to change?","title":"Stringtie (Assembling reads into transcripts)"},{"location":"nbis_annotation/practical_session/practical2_sub_transcriptome/#2-de-novo-transcriptome-assembly","text":"","title":"2. De-novo transcriptome assembly:"},{"location":"nbis_annotation/practical_session/practical2_sub_transcriptome/#trinity","text":"Trinity assemblies can be used as complementary evidence, particularly when trying to polish a gene build with Pasa. Before you start, check how big the raw read data is that you wish to assemble to avoid unreasonably long run times. cd ~/annotation_course/practical2/RNAseq mkdir trinity cd trinity Trinity --seqType fq --max_memory 64G --left ~/annotation_course/data/RNAseq/ERR305399.left.fastq.gz --right ~/annotation_course/data/RNAseq/ERR305399.right.fastq.gz --CPU 8 --output trinity_result --SS_lib_type RF Trinity takes a long time to run if you want to have a look at the results, look in ~/annotation_course/course_material/data/dmel/chromosome_4/RNAseq/ the output that will be used later on for the annotation will be Trinity.fasta","title":"Trinity"},{"location":"nbis_annotation/practical_session/practical2_sub_transcriptome/#closing-remarks","text":"You have now successfully perform transcript assemblies. You have seen how to perform a genome-guided assembly as well as de-no assembly.","title":"Closing remarks"},{"location":"nbis_annotation/practical_session/practical2_supl2_maker/","text":"Inspect the output Finding your way around By default, Maker will write the output of its different analyses into a folder named: <name_of_genome_fasta>.maker.output In our case: 4.maker.output Within the main output directory, Maker keeps a copy of the config files, a database (here: 4.db), directories for the blast databases created from your evidence data and a file called 4_master_datastore_index.log. Out of these files, only the 4_master_datastore_index is really interesting to us. It includes a log of all the contigs included in the genome fasta file - together with their processing status (ideally: FINISHED) and the location of the output files. Since Maker can technically run in parallel on a large number of contigs, it creates separate folders for each of these input data. For larger genomes, this can generate a very deep and confusing folder tree. The 4_master_datastore_index helps you make sense of it: 4 4\\_datastore/A8/7F/4/ STARTED 4 4\\_datastore/A8/7F/4/ FINISHED This meens the sequence 4 was started - and finished, with all data (annotation, protein predictions etc) written to the subfolder 4_datastore/A8/7F/4/. If you look into that folder, you will find the finished Maker annotation for this contig. rw-rw-r- 1 student student 472193 Mar 24 10:16 4.gff <br/> \\*rw-rw-r- 1 student student 3599 Mar 24 10:16 4.maker.augustus\\_masked.proteins.fasta <br/> \\*rw-rw-r- 1 student student 10388 Mar 24 10:16 4.maker.augustus\\_masked.transcripts.fasta <br/> \\*rw-rw-r- 1 student student 176 Mar 24 10:16 4.maker.non\\_overlapping\\_ab\\_initio.proteins.fasta <br/> \\*rw-rw-r- 1 student student 328 Mar 24 10:16 4.maker.non\\_overlapping\\_ab\\_initio.transcripts.fasta <br/> rw-rw-r- 1 student student 3931 Mar 24 10:16 4.maker.proteins.fasta <br/> rw-rw-r- 1 student student 20865 Mar 24 10:16 4.maker.transcripts.fasta <br/> rw-rw-r- 1 student student 4248 Mar 24 10:15 run.log <br/> drwxrwsr-x 3 student student 4096 Mar 24 10:16 theVoid.4 * only if an abinitio tool has been activated The main annotation file is '4.gff' - including both the finished gene models and all the raw compute data. The other files include fasta files for the different sequence features that have been annotated - based on ab-initio predictions through augustus as well as on the finished gene models. The folder 'theVoid' include all the raw computations that Maker has performed to synthesize the evidence into gene models. Understanding a Maker annotation You have two options now for gathering the output in some usable form - copy select files by hand to wherever you want them. Or you can use a script that does the job for you (we have included an example in the script folder). From the folder you have run Maker, run the script called 'maker_merge_outputs_from_datastore' to create an output file for all annotations and protein files: maker_merge_outputs_from_datastore.pl This will create a directory called \" annotations \" containing: -annotations.gff -annotations.proteins.fa -annotationByType/ annotations.gff file It's a mix of all the gff tracks produced/handled by maker. It contains the annotation done by maker mixed up with other gff lines like the protein alignments, repeats, etc.. If you use 'less' to read the annotation file annotations.gff ( GFF3 format ), you will see a range of different features: ##gff-version 3 4 . contig 1 1351857 . . . ID=4;Name=4 4 maker gene 24134 25665 . + . ID=maker-4-exonerate_protein2genome-gene-0.0;Name=maker-4-exonerate_protein2genome-gene-0.0 4 maker mRNA 24134 25665 917 + . ID=maker-4-exonerate_protein2genome-gene-0.0-mRNA-1;Parent=maker-4-exonerate_protein2genome-gene-0.0;Name=maker-4-exonerate_protein2genome-gene-0.0-mRNA-1;_AED=0.09;_eAED=0.09;_QI=0|0.33|0.25|1|0|0|4|44|290 ... For example, the above lines read: A new contig is being shown, with the id '4' and a length of 1351857 nucleotides On this contig, a gene feature is located from position 24134 to 25665, on the plus strand and with the id 'maker-4-exonerate_protein2genome-gene-0.0'. On this contig, belonging to the gene, is located a transcript from position 24134 to 25665, on the plus strand and with the id 'maker-4-exonerate_protein2genome-gene-0.0-mRNA-1'. It's quality, or AED score, is 0.09 - which means that the evidence alignments are close to be in perfect agreement with the transcript model. And so on. annotations.proteins.fa file This file contains the proteins translated from the CDS of gene models predicted. annotationByType directory The different types of information present in the annotation file (annotations.gff) are separated into independent file into the \"annotationByType\" directory.This is useful for a number of applications, like visualizing it as separate tracks in a genome browser. Or to compute some intersting numbers from the gene models. This should contains a bunch of files, including ' maker.gff ' - which contains the actual gene models.","title":"Inspecting Maker output"},{"location":"nbis_annotation/practical_session/practical2_supl2_maker/#inspect-the-output","text":"","title":"Inspect the output"},{"location":"nbis_annotation/practical_session/practical2_supl2_maker/#finding-your-way-around","text":"By default, Maker will write the output of its different analyses into a folder named: <name_of_genome_fasta>.maker.output In our case: 4.maker.output Within the main output directory, Maker keeps a copy of the config files, a database (here: 4.db), directories for the blast databases created from your evidence data and a file called 4_master_datastore_index.log. Out of these files, only the 4_master_datastore_index is really interesting to us. It includes a log of all the contigs included in the genome fasta file - together with their processing status (ideally: FINISHED) and the location of the output files. Since Maker can technically run in parallel on a large number of contigs, it creates separate folders for each of these input data. For larger genomes, this can generate a very deep and confusing folder tree. The 4_master_datastore_index helps you make sense of it: 4 4\\_datastore/A8/7F/4/ STARTED 4 4\\_datastore/A8/7F/4/ FINISHED This meens the sequence 4 was started - and finished, with all data (annotation, protein predictions etc) written to the subfolder 4_datastore/A8/7F/4/. If you look into that folder, you will find the finished Maker annotation for this contig. rw-rw-r- 1 student student 472193 Mar 24 10:16 4.gff <br/> \\*rw-rw-r- 1 student student 3599 Mar 24 10:16 4.maker.augustus\\_masked.proteins.fasta <br/> \\*rw-rw-r- 1 student student 10388 Mar 24 10:16 4.maker.augustus\\_masked.transcripts.fasta <br/> \\*rw-rw-r- 1 student student 176 Mar 24 10:16 4.maker.non\\_overlapping\\_ab\\_initio.proteins.fasta <br/> \\*rw-rw-r- 1 student student 328 Mar 24 10:16 4.maker.non\\_overlapping\\_ab\\_initio.transcripts.fasta <br/> rw-rw-r- 1 student student 3931 Mar 24 10:16 4.maker.proteins.fasta <br/> rw-rw-r- 1 student student 20865 Mar 24 10:16 4.maker.transcripts.fasta <br/> rw-rw-r- 1 student student 4248 Mar 24 10:15 run.log <br/> drwxrwsr-x 3 student student 4096 Mar 24 10:16 theVoid.4 * only if an abinitio tool has been activated The main annotation file is '4.gff' - including both the finished gene models and all the raw compute data. The other files include fasta files for the different sequence features that have been annotated - based on ab-initio predictions through augustus as well as on the finished gene models. The folder 'theVoid' include all the raw computations that Maker has performed to synthesize the evidence into gene models.","title":"Finding your way around"},{"location":"nbis_annotation/practical_session/practical2_supl2_maker/#understanding-a-maker-annotation","text":"You have two options now for gathering the output in some usable form - copy select files by hand to wherever you want them. Or you can use a script that does the job for you (we have included an example in the script folder). From the folder you have run Maker, run the script called 'maker_merge_outputs_from_datastore' to create an output file for all annotations and protein files: maker_merge_outputs_from_datastore.pl This will create a directory called \" annotations \" containing: -annotations.gff -annotations.proteins.fa -annotationByType/ annotations.gff file It's a mix of all the gff tracks produced/handled by maker. It contains the annotation done by maker mixed up with other gff lines like the protein alignments, repeats, etc.. If you use 'less' to read the annotation file annotations.gff ( GFF3 format ), you will see a range of different features: ##gff-version 3 4 . contig 1 1351857 . . . ID=4;Name=4 4 maker gene 24134 25665 . + . ID=maker-4-exonerate_protein2genome-gene-0.0;Name=maker-4-exonerate_protein2genome-gene-0.0 4 maker mRNA 24134 25665 917 + . ID=maker-4-exonerate_protein2genome-gene-0.0-mRNA-1;Parent=maker-4-exonerate_protein2genome-gene-0.0;Name=maker-4-exonerate_protein2genome-gene-0.0-mRNA-1;_AED=0.09;_eAED=0.09;_QI=0|0.33|0.25|1|0|0|4|44|290 ... For example, the above lines read: A new contig is being shown, with the id '4' and a length of 1351857 nucleotides On this contig, a gene feature is located from position 24134 to 25665, on the plus strand and with the id 'maker-4-exonerate_protein2genome-gene-0.0'. On this contig, belonging to the gene, is located a transcript from position 24134 to 25665, on the plus strand and with the id 'maker-4-exonerate_protein2genome-gene-0.0-mRNA-1'. It's quality, or AED score, is 0.09 - which means that the evidence alignments are close to be in perfect agreement with the transcript model. And so on. annotations.proteins.fa file This file contains the proteins translated from the CDS of gene models predicted. annotationByType directory The different types of information present in the annotation file (annotations.gff) are separated into independent file into the \"annotationByType\" directory.This is useful for a number of applications, like visualizing it as separate tracks in a genome browser. Or to compute some intersting numbers from the gene models. This should contains a bunch of files, including ' maker.gff ' - which contains the actual gene models.","title":"Understanding a Maker annotation"},{"location":"nbis_annotation/practical_session/practical2_supl3_maker/","text":"Configure the maker_opts.ctl properly for the abinitio evidence-driven annotation: #-----Genome (these are always required) genome=4.fa #genome sequence (fasta file or fasta embeded in GFF3 file) organism_type=eukaryotic #eukaryotic or prokaryotic. Default is eukaryotic ... #-----EST Evidence (for best results provide a file for at least one) est= #set of ESTs or assembled mRNA-seq in fasta format altest= #EST/cDNA sequence file in fasta format from an alternate organism est_gff=est2genome.gff, est_gff_stringtie.gff #aligned ESTs or mRNA-seq from an external GFF3 file altest_gff= #aligned ESTs from a closly relate species in GFF3 format ... #-----Protein Homology Evidence (for best results provide a file for at least one) protein= #protein sequence file in fasta format (i.e. from mutiple oransisms) protein_gff=protein2genome.gff #aligned protein homology evidence from an external GFF3 file ... #-----Repeat Masking (leave values blank to skip repeat masking) model_org= #select a model organism for RepBase masking in RepeatMasker rmlib= #provide an organism specific repeat library in fasta format for RepeatMasker repeat_protein= #provide a fasta file of transposable element proteins for RepeatRunner rm_gff=repeatmasker.chr4.gff,repeatrunner.chr4.gff #pre-identified repeat elements from an external GFF3 file prok_rm=0 #forces MAKER to repeatmask prokaryotes (no reason to change this), 1 = yes, 0 = no softmask=1 #use soft-masking rather than hard-masking in BLAST (i.e. seg and dust filtering) ... #-----Gene Prediction snaphmm= #SNAP HMM file gmhmm= #GeneMark HMM file augustus_species=fly #Augustus gene prediction species model fgenesh_par_file= #FGENESH parameter file pred_gff= #ab-initio predictions from an external GFF3 file model_gff= #annotated gene models from an external GFF3 file (annotation pass-through) est2genome=0 #infer gene predictions directly from ESTs, 1 = yes, 0 = no protein2genome=0 #infer predictions from protein homology, 1 = yes, 0 = no trna=0 #find tRNAs with tRNAscan, 1 = yes, 0 = no snoscan_rrna= #rRNA file to have Snoscan find snoRNAs unmask=0 #also run ab-initio prediction programs on unmasked sequence, 1 = yes, 0 = no ... keep_preds=1 ... To better understand the different parameters you can have a look here","title":"Configuring Maker (I)"},{"location":"nbis_annotation/practical_session/practical2_supl3_maker/#configure-the-maker_optsctl-properly-for-the-abinitio-evidence-driven-annotation","text":"#-----Genome (these are always required) genome=4.fa #genome sequence (fasta file or fasta embeded in GFF3 file) organism_type=eukaryotic #eukaryotic or prokaryotic. Default is eukaryotic ... #-----EST Evidence (for best results provide a file for at least one) est= #set of ESTs or assembled mRNA-seq in fasta format altest= #EST/cDNA sequence file in fasta format from an alternate organism est_gff=est2genome.gff, est_gff_stringtie.gff #aligned ESTs or mRNA-seq from an external GFF3 file altest_gff= #aligned ESTs from a closly relate species in GFF3 format ... #-----Protein Homology Evidence (for best results provide a file for at least one) protein= #protein sequence file in fasta format (i.e. from mutiple oransisms) protein_gff=protein2genome.gff #aligned protein homology evidence from an external GFF3 file ... #-----Repeat Masking (leave values blank to skip repeat masking) model_org= #select a model organism for RepBase masking in RepeatMasker rmlib= #provide an organism specific repeat library in fasta format for RepeatMasker repeat_protein= #provide a fasta file of transposable element proteins for RepeatRunner rm_gff=repeatmasker.chr4.gff,repeatrunner.chr4.gff #pre-identified repeat elements from an external GFF3 file prok_rm=0 #forces MAKER to repeatmask prokaryotes (no reason to change this), 1 = yes, 0 = no softmask=1 #use soft-masking rather than hard-masking in BLAST (i.e. seg and dust filtering) ... #-----Gene Prediction snaphmm= #SNAP HMM file gmhmm= #GeneMark HMM file augustus_species=fly #Augustus gene prediction species model fgenesh_par_file= #FGENESH parameter file pred_gff= #ab-initio predictions from an external GFF3 file model_gff= #annotated gene models from an external GFF3 file (annotation pass-through) est2genome=0 #infer gene predictions directly from ESTs, 1 = yes, 0 = no protein2genome=0 #infer predictions from protein homology, 1 = yes, 0 = no trna=0 #find tRNAs with tRNAscan, 1 = yes, 0 = no snoscan_rrna= #rRNA file to have Snoscan find snoRNAs unmask=0 #also run ab-initio prediction programs on unmasked sequence, 1 = yes, 0 = no ... keep_preds=1 ... To better understand the different parameters you can have a look here","title":"Configure the maker_opts.ctl properly for the abinitio evidence-driven annotation:"},{"location":"nbis_annotation/practical_session/practical2_supl_maker/","text":"Configure your maker project : The maker_opts.ctl file in detail: When executing the command \"maker -CTL\" MAKER creates 3 control files. Of these, only maker_opts.ctl is of concern to us. Have a look at the following sections and fill in the information as shown: #-----Genome (these are always required) genome= 4.fa #genome sequence (fasta file or fasta embeded in GFF3 file) organism_type=eukaryotic #eukaryotic or prokaryotic. Default is eukaryotic ... #-----EST Evidence (for best results provide a file for at least one) est=est.chr4.fa #set of ESTs or assembled mRNA-seq in fasta format altest= #EST/cDNA sequence file in fasta format from an alternate organism est_gff=stringtie2genome.chr4.ok.gff #aligned ESTs or mRNA-seq from an external GFF3 file altest_gff= #aligned ESTs from a closly relate species in GFF3 format ... #-----Protein Homology Evidence (for best results provide a file for at least one) protein=proteins.chr4.fa #protein sequence file in fasta format (i.e. from mutiple oransisms) protein_gff= #aligned protein homology evidence from an external GFF3 file ... #-----Repeat Masking (leave values blank to skip repeat masking) model_org= #select a model organism for RepBase masking in RepeatMasker rmlib= #provide an organism specific repeat library in fasta format for RepeatMasker repeat_protein= #provide a fasta file of transposable element proteins for RepeatRunner rm_gff= repeatmasker.chr4.gff,repeatrunner.chr4.gff #pre-identified repeat elements from an external GFF3 file prok_rm=0 #forces MAKER to repeatmask prokaryotes (no reason to change this), 1 = yes, 0 = no softmask=1 #use soft-masking rather than hard-masking in BLAST (i.e. seg and dust filtering) ... #-----Gene Prediction snaphmm= #SNAP HMM file gmhmm= #GeneMark HMM file augustus_species= #Augustus gene prediction species model fgenesh_par_file= #FGENESH parameter file pred_gff= #ab-initio predictions from an external GFF3 file model_gff= #annotated gene models from an external GFF3 file (annotation pass-through) est2genome=1 #infer gene predictions directly from ESTs, 1 = yes, 0 = no protein2genome=1 #infer predictions from protein homology, 1 = yes, 0 = no trna=0 #find tRNAs with tRNAscan, 1 = yes, 0 = no snoscan_rrna= #rRNA file to have Snoscan find snoRNAs unmask=0 #also run ab-initio prediction programs on unmasked sequence, 1 = yes, 0 = no To better understand the different parameters you can have a look here","title":"Configuring Maker (I)"},{"location":"nbis_annotation/practical_session/practical2_supl_maker/#configure-your-maker-project-the-maker_optsctl-file-in-detail","text":"When executing the command \"maker -CTL\" MAKER creates 3 control files. Of these, only maker_opts.ctl is of concern to us. Have a look at the following sections and fill in the information as shown: #-----Genome (these are always required) genome= 4.fa #genome sequence (fasta file or fasta embeded in GFF3 file) organism_type=eukaryotic #eukaryotic or prokaryotic. Default is eukaryotic ... #-----EST Evidence (for best results provide a file for at least one) est=est.chr4.fa #set of ESTs or assembled mRNA-seq in fasta format altest= #EST/cDNA sequence file in fasta format from an alternate organism est_gff=stringtie2genome.chr4.ok.gff #aligned ESTs or mRNA-seq from an external GFF3 file altest_gff= #aligned ESTs from a closly relate species in GFF3 format ... #-----Protein Homology Evidence (for best results provide a file for at least one) protein=proteins.chr4.fa #protein sequence file in fasta format (i.e. from mutiple oransisms) protein_gff= #aligned protein homology evidence from an external GFF3 file ... #-----Repeat Masking (leave values blank to skip repeat masking) model_org= #select a model organism for RepBase masking in RepeatMasker rmlib= #provide an organism specific repeat library in fasta format for RepeatMasker repeat_protein= #provide a fasta file of transposable element proteins for RepeatRunner rm_gff= repeatmasker.chr4.gff,repeatrunner.chr4.gff #pre-identified repeat elements from an external GFF3 file prok_rm=0 #forces MAKER to repeatmask prokaryotes (no reason to change this), 1 = yes, 0 = no softmask=1 #use soft-masking rather than hard-masking in BLAST (i.e. seg and dust filtering) ... #-----Gene Prediction snaphmm= #SNAP HMM file gmhmm= #GeneMark HMM file augustus_species= #Augustus gene prediction species model fgenesh_par_file= #FGENESH parameter file pred_gff= #ab-initio predictions from an external GFF3 file model_gff= #annotated gene models from an external GFF3 file (annotation pass-through) est2genome=1 #infer gene predictions directly from ESTs, 1 = yes, 0 = no protein2genome=1 #infer predictions from protein homology, 1 = yes, 0 = no trna=0 #find tRNAs with tRNAscan, 1 = yes, 0 = no snoscan_rrna= #rRNA file to have Snoscan find snoRNAs unmask=0 #also run ab-initio prediction programs on unmasked sequence, 1 = yes, 0 = no To better understand the different parameters you can have a look here","title":"Configure your maker project : The maker_opts.ctl file in detail:"},{"location":"nbis_annotation/practical_session/practical3_manualCuration/","text":"Manual curation Overview It is easy to understand that automated gene build pipelines will never reach 100% accuracy in their reconstruction. This is due to a number of factors, including ambiguous information from competing input data, inherent uncertainties of ab-initio predictions as well as simplified decision processes when synthesising all available information into a transcript structure. It is therefore always important to manually inspect a gene build - and in basically all cases manual curation is highly recommended. Manual curation is a common step in any genome project, often referred to as a jamboree. All researchers involved in the project will meet - virtually or physically - and together inspect the gene build(s) to correct remaining issues prior to publication or downstream analyses. Here we will learn about manual curation tools and best practices, which you can then employ in your own annotation project. Meet: WebApollo You have already encountered WebApollo in the previous exercise on gene building. There, you used its visualisation capabilities to look at several gene builds and compared them against the evidence alignments. However, what do you do if you find problems with your annotation? Basically, there are two options: The problems seem systematic and related to issues with the input data or settings. In this case the best is to investigate and eliminate the issue(s) from the raw data and re-run the pipeline. Examples would be poorly assembled RNA-seq data or incompletely or badly sampled protein data. Another issue may be severe problems with the genome assembly. This of course is outside of your annotation task - and a discussion with the assembly team may be necessary. The problem is sporadic and looks otherwise non-systematic and complex Complex, non-systematic errors are harder to rectify by just rerunning the pipeline. The goal of the computational gene build should be to generate a solid basis on which to build future analyses. An error rate of 20% is well within the expected margins and it is important to remember that a computational prediction will always be of lesser quality than a manually curated annotation. A sensible suggestion is to under-shoot rather than over-shoot. In other words, it is often better to be a little more conservative rather than to include as much information as possible. This is controlled by e.g. the way you have compiled your input data and settings within maker. Using WebApollo to curate gene models Manual curation is an integral part of any annotation project. It reveals issues that exist in the gene build and can be used to add further detail - like references to external data sources, or isoforms etc. The aim of manual curation is to compare a gene model against existing evidence from sources such as ab-initio predictions, protein alignments, RNA-seq as well as related species and fix those parts that are in clear conflict with the evidence. During the course, we will present a few basic features of WebApollo - but there is also a fairly comprehensive handbook available here: http://icebox.lbl.gov/webapollo/docs/webapollo_user_guide.pdf Jamboree For this exercise, we have set up a specific Webapollo instance of a drosophila melanogaster annotation of the chromosome 4. It is called drosophila_melanogaster_chr4_jamboree . The tracks available are: Augustus_drosophila : a pure ab initio annotation using Augustus with the drosophila model. Maker_evidence : A maker annotation using Evidence-based approach. Maker_abinitio : A maker annotation using Ab initio evidence-drived approach. Proteins : track of reviewed proteins aligned by Maker. tophat_larva4 : RNAseq data (bam file) aligned to the genome by tophat. Cufflinks_larva4 : A cufflinks transcript assembly aligned by MAKER. Stringtie_ERR305399 : A stringtie transcript assembly aligned by MAKER. EST_from_NCBI : The ESTs aligned by maker during the annotation process. A genomic region of the chrosmosome is assigned to each of you. Your aim is to manualy annotate your assigned part using all the information available in the different tracks. Genomic region has been assigned without any biological consideration. So, if genes straddle two regions don't stop you at the end of yours :). NOTES: Isoforms are allowed. Start each gene annotation by dragging-and-dropping the gene model that you think be the best. 1 : 50 000 - 140 500 2 : 140 500 - 227 500 3 : 227 500 - 314 500 4 : 314 500 - 401 500 5 : 401 500 - 488 500 6 : 488 500 - 575 500 7 : 575 500 - 662 500 8 : 662 500 - 749 500 9 : 749 500 - 836 500 10 : 836 500 - 923 500 11 : 923 500 - 1 010 500 12 : 1 010 500 - 1 097 500 13 : 1 097 500 - 1 184 500 14 : 1 184 500 - 1 268 000 The work you performed was only on small genome portion (1,3 Mbp). That gives you a flavour of the time cost to do a manual curation on a small genome, and an idea of the amount of work needed to manually curate a big genome (>1 Gbp). Check Before the end of this practical session we will load the reference annotation of drosophila melanogaster allowing you to check your manual annotation. You should just refresh your web page to display this new track. Do not be disappointed if your annotation differs a lot from the reference one. Keep in mind that the reference annotation has been curated by experienced experts, and that have used more complete evidence.","title":"Practical 3"},{"location":"nbis_annotation/practical_session/practical3_manualCuration/#manual-curation","text":"","title":"Manual curation"},{"location":"nbis_annotation/practical_session/practical3_manualCuration/#overview","text":"It is easy to understand that automated gene build pipelines will never reach 100% accuracy in their reconstruction. This is due to a number of factors, including ambiguous information from competing input data, inherent uncertainties of ab-initio predictions as well as simplified decision processes when synthesising all available information into a transcript structure. It is therefore always important to manually inspect a gene build - and in basically all cases manual curation is highly recommended. Manual curation is a common step in any genome project, often referred to as a jamboree. All researchers involved in the project will meet - virtually or physically - and together inspect the gene build(s) to correct remaining issues prior to publication or downstream analyses. Here we will learn about manual curation tools and best practices, which you can then employ in your own annotation project.","title":"Overview"},{"location":"nbis_annotation/practical_session/practical3_manualCuration/#meet-webapollo","text":"You have already encountered WebApollo in the previous exercise on gene building. There, you used its visualisation capabilities to look at several gene builds and compared them against the evidence alignments. However, what do you do if you find problems with your annotation? Basically, there are two options: The problems seem systematic and related to issues with the input data or settings. In this case the best is to investigate and eliminate the issue(s) from the raw data and re-run the pipeline. Examples would be poorly assembled RNA-seq data or incompletely or badly sampled protein data. Another issue may be severe problems with the genome assembly. This of course is outside of your annotation task - and a discussion with the assembly team may be necessary. The problem is sporadic and looks otherwise non-systematic and complex Complex, non-systematic errors are harder to rectify by just rerunning the pipeline. The goal of the computational gene build should be to generate a solid basis on which to build future analyses. An error rate of 20% is well within the expected margins and it is important to remember that a computational prediction will always be of lesser quality than a manually curated annotation. A sensible suggestion is to under-shoot rather than over-shoot. In other words, it is often better to be a little more conservative rather than to include as much information as possible. This is controlled by e.g. the way you have compiled your input data and settings within maker.","title":"Meet: WebApollo"},{"location":"nbis_annotation/practical_session/practical3_manualCuration/#using-webapollo-to-curate-gene-models","text":"Manual curation is an integral part of any annotation project. It reveals issues that exist in the gene build and can be used to add further detail - like references to external data sources, or isoforms etc. The aim of manual curation is to compare a gene model against existing evidence from sources such as ab-initio predictions, protein alignments, RNA-seq as well as related species and fix those parts that are in clear conflict with the evidence. During the course, we will present a few basic features of WebApollo - but there is also a fairly comprehensive handbook available here: http://icebox.lbl.gov/webapollo/docs/webapollo_user_guide.pdf","title":"Using WebApollo to curate gene models"},{"location":"nbis_annotation/practical_session/practical3_manualCuration/#jamboree","text":"For this exercise, we have set up a specific Webapollo instance of a drosophila melanogaster annotation of the chromosome 4. It is called drosophila_melanogaster_chr4_jamboree . The tracks available are: Augustus_drosophila : a pure ab initio annotation using Augustus with the drosophila model. Maker_evidence : A maker annotation using Evidence-based approach. Maker_abinitio : A maker annotation using Ab initio evidence-drived approach. Proteins : track of reviewed proteins aligned by Maker. tophat_larva4 : RNAseq data (bam file) aligned to the genome by tophat. Cufflinks_larva4 : A cufflinks transcript assembly aligned by MAKER. Stringtie_ERR305399 : A stringtie transcript assembly aligned by MAKER. EST_from_NCBI : The ESTs aligned by maker during the annotation process. A genomic region of the chrosmosome is assigned to each of you. Your aim is to manualy annotate your assigned part using all the information available in the different tracks. Genomic region has been assigned without any biological consideration. So, if genes straddle two regions don't stop you at the end of yours :). NOTES: Isoforms are allowed. Start each gene annotation by dragging-and-dropping the gene model that you think be the best. 1 : 50 000 - 140 500 2 : 140 500 - 227 500 3 : 227 500 - 314 500 4 : 314 500 - 401 500 5 : 401 500 - 488 500 6 : 488 500 - 575 500 7 : 575 500 - 662 500 8 : 662 500 - 749 500 9 : 749 500 - 836 500 10 : 836 500 - 923 500 11 : 923 500 - 1 010 500 12 : 1 010 500 - 1 097 500 13 : 1 097 500 - 1 184 500 14 : 1 184 500 - 1 268 000 The work you performed was only on small genome portion (1,3 Mbp). That gives you a flavour of the time cost to do a manual curation on a small genome, and an idea of the amount of work needed to manually curate a big genome (>1 Gbp).","title":"Jamboree"},{"location":"nbis_annotation/practical_session/practical3_manualCuration/#check","text":"Before the end of this practical session we will load the reference annotation of drosophila melanogaster allowing you to check your manual annotation. You should just refresh your web page to display this new track. Do not be disappointed if your annotation differs a lot from the reference one. Keep in mind that the reference annotation has been curated by experienced experts, and that have used more complete evidence.","title":"Check"},{"location":"nbis_annotation/practical_session/practical4_funcAnnotInterp/","text":"Functional annotation Functional annotation is the process during which we try to put names to faces - what do genes that we have annotated and curated? Basically all existing approaches accomplish this by means of similarity. If a translation product has strong similarity to a protein that has previously been assigned a function, the function in this newly annotated transcript is probably the same. Of course, this thinking is a bit problematic (where do other functional annotations come from...?) and the method will break down the more distant a newly annotated genome is to existing reference data. A complementary strategy is to scan for more limited similarity - specifically to look for the motifs of functionally characterized protein domains. It doesn't directly tell you what the protein is doing exactly, but it can provide some first indication. In this exercise we will use an approach that combines the search for full-sequence simliarity by means of 'Blast' against large public databases with more targeted characterization of functional elements through the InterproScan pipeline. Interproscan is a meta-search engine that can compare protein queries against numerous databases. The output from Blast and Interproscan can then be used to add some information to our annotation. Prepare the input data Since we do not wish to spend too much time on this, we will again limit our analysis to chromosome 4. It is also probably best to choose the analysis with ab-initio predictions enabled (unless you found the other build to be more convincing). Maker produces a protein fasta file (called \"annotations.proteins.fa\") together with the annotation and this file should be located in your maker directory. Move in the proper folder: cd ~/annotation_course/practical4 Now link the annotation you choose to work with. The command will looks like: ln -s ~/annotation_course/practical2/maker/maker_with_abinitio/annotationByType/maker.gff maker_with_abinitio.gff Extract the corresponding proteins from the gff file: ln -s ~/annotation_course/data/genome/4.fa gff3_sp_extract_sequences.pl --gff maker_with_abinitio.gff -f 4.fa -p --cfs -o AA.fa Interproscan approach Interproscan combines a number of searches for conserved motifs and curated data sets of protein clusters etc. This step may take fairly long time. It is recommended to paralellize it for huge amount of data by doing analysis of chunks of tens or hundreds proteins. Perform InterproScan analysis InterproScan can be run through a website or from the command line on a linux server. Here we are interested in the command line approach. Interproscan allows to look up pathways, families, domains, sites, repeats, structural domains and other sequence features. Launch Interproscan with the option -h if you want have a look about all the parameters. The '-app' option allows defining the database used. Here we will use the PfamA,ProDom and SuperFamily databases. Interproscan uses an internal database that related entries in public databases to established GO terms. By running the '-goterms' option, we can add this information to our data set. If you enable the InterPro lookup ('-iprlookup'), you can also get the InterPro identifier corresponding to each motif retrieved: for example, the same motif is known as PF01623 in Pfam and as IPR002568 in InterPro. The option '-pa' provides mappings from matches to pathway information (MetaCyc,UniPathway,KEGG,Reactome). interproscan.sh -i AA.fa -t p -dp -pa -appl Pfam,ProDom-2006.1,SuperFamily-1.75 --goterms --iprlookup The analysis shoud take 2-3 secs per protein request - depending on how many sequences you have submitted, you can make a fairly deducted guess regarding the running time. You will obtain 3 result files with the following extension '.gff3', '.tsv' and '.xml'. Explanation of these output are available >>here<< . load the retrieved functional information in your annotation file: Next, you could write scripts of your own to merge interproscan output into your annotation. Incidentially, Maker comes with utility scripts that can take InterProscan output and add it to a Maker annotation file (you need to load maker). ipr_update_gff: adds searchable tags to the gene and mRNA features in the GFF3 files. iprscan2gff3: adds physical viewable features for domains that can be displayed in JBrowse, Gbrowse, and Web Apollo. ipr_update_gff maker_with_abinitio.gff AA.fa.tsv > maker_with_abinitio_with_interpro.gff Where a match is found, the new file will now include features called Dbxref and/or Ontology_term in the gene and transcript feature field (9th column). BLAST approach Blast searches provide an indication about potential homology to known proteins. A 'full' Blast analysis can run for several days and consume several GB of Ram. Consequently, for a huge amount of data it is recommended to parallelize this step doing analysis of chunks of tens or hundreds proteins. This approach can be used to give a name to the genes and a function to the transcripts. Perform Blast searches from the command line on Uppmax: To run Blast on your data, use the Ncbi Blast+ package against a Drosophila-specific database (included in the folder we have provided for you, under annotation_course/data/blastdb/uniprot_dmel/uniprot_dmel.fa ) - of course, any other NCBI database would also work: blastp -db ~/annotation_course/data/blastdb/uniprot_dmel/uniprot_dmel.fa -query AA.fa -outfmt 6 -out blast.out -num_threads 8 Against the Drosophila-specific database, the blast search takes about 2 secs per protein request - depending on how many sequences you have submitted, you can make a fairly deducted guess regarding the running time. Process the blast outout with Annie The Blast outputs must be processed to retrieve the information of the closest protein (best e-value) found by Blast. This work will be done using annie . First download annie: git clone https://github.com/genomeannotation/Annie.git Now launch annie: Annie/annie.py -b blast.out -db ~/annotation_course/data/blastdb/uniprot_dmel/uniprot_dmel.fa -g maker_with_abinitio.gff -o annotation_blast.annie Annie writes in a 3-column table format file, providing gene name and mRNA product information. The purpose of annie is relatively simple. It recovers the information in the sequence header of the uniprot fasta file, from the best sequence found by Blast (the lowest e-value). load the retrieved information in your annotation file: Now you should be able to use the following script: maker_gff3manager_JD_v8.pl -f maker_with_abinitio_with_interpro.gff -b annotation_blast.annie --ID FLY -o finalOutputDir That will add the name attribute to the \"gene\" feature and the description attribute (corresponding to the product information) to the \"mRNA\" feature into you annotation file. This script may be used for other purpose like to modify the ID value by something more convenient (i.e FLYG00000001 instead of maker-4-exonerate_protein2genome-gene-8.41). The improved annotation is a file named \"codingGeneFeatures.gff\" inside the finalOutputDir. For displaying the product attribute in Webapollo you can change this attribute by description using this script: /home/student/.local/GAAS/annotation/WebApollo/gff3_webApollo_compliant.pl --gff finalOutputDir/codingGeneFeatures.gff -o final_annotation.gff Visualise the final annotation Transfer the final_annotation.gff file to your computer using scp in a new terminal: scp -i ~/.ssh/azure_rsa student@ IP :/home/student/annotation_course/practical4/final_annotation.gff . Load the file in into the genome portal called drosophila_melanogaster_chr4 in the Webapollo genome browser available at the address http://annotation-prod.scilifelab.se:8080/NBIS_course/ . Here find the WebApollo instruction Wondeful ! insn't it ? What's next? Because of Makers' compatibility with GMOD standards, an annotation augmented in one or both of this way can be loaded into e.g. WebApollo and will save annotators a lot of work when e.g. adding meta data to transcript models. Submission to public repository (creation of an EMBL file) Once your are satisfied by the wonderful annotation you have done, it would useful important to submit it to a public repostiroy. Fisrt you will be applaused by the community because you share your nice work, secondly this is often mandatory if you wish to publish some work related to this annotation. Current state-of-the-art genome annotation tools use the GFF3 format as output, while this format is not accepted as submission format by the International Nucleotide Sequence Database Collaboration (INSDC) databases. Converting the GFF3 format to a format accepted by one of the three INSDC databases is a key step in the achievement of genome annotation projects. However, the flexibility existing in the GFF3 format makes this conversion task difficult to perform. In order to submit to NCBI , the use of a tool like GAG will save you lot time. In order to submit to EBI , the use of a tool like EMBLmyGFF3 will be your best choice. Let's prepare your annotation to submit to ENA (EBI) In real life, prior to a submission to ENA, you need to create an account and create a project asking a locus_tag for your annotation. You have also to fill lot of metada information related to the assembly and so on. We will skip those tasks using fake information. First you need to download and install EMBLmyGFF3: pip install --user git+https://github.com/NBISweden/EMBLmyGFF3.git EMBLmyGFF3 finalOutputDir/codingGeneFeatures.gff 4.fa -o my_annotation_ready_to_submit.embl You now have a EMBL flat file ready to submit. In theory to finsish the submission, you will have to send this archived file to their ftp server and finish the submission process in the website side too. But we will not go further. We are done. CONGRATULATION you know most of the secrets needed to understand the annotations on and perform your own !","title":"Practical 4"},{"location":"nbis_annotation/practical_session/practical4_funcAnnotInterp/#functional-annotation","text":"Functional annotation is the process during which we try to put names to faces - what do genes that we have annotated and curated? Basically all existing approaches accomplish this by means of similarity. If a translation product has strong similarity to a protein that has previously been assigned a function, the function in this newly annotated transcript is probably the same. Of course, this thinking is a bit problematic (where do other functional annotations come from...?) and the method will break down the more distant a newly annotated genome is to existing reference data. A complementary strategy is to scan for more limited similarity - specifically to look for the motifs of functionally characterized protein domains. It doesn't directly tell you what the protein is doing exactly, but it can provide some first indication. In this exercise we will use an approach that combines the search for full-sequence simliarity by means of 'Blast' against large public databases with more targeted characterization of functional elements through the InterproScan pipeline. Interproscan is a meta-search engine that can compare protein queries against numerous databases. The output from Blast and Interproscan can then be used to add some information to our annotation.","title":"Functional annotation"},{"location":"nbis_annotation/practical_session/practical4_funcAnnotInterp/#prepare-the-input-data","text":"Since we do not wish to spend too much time on this, we will again limit our analysis to chromosome 4. It is also probably best to choose the analysis with ab-initio predictions enabled (unless you found the other build to be more convincing). Maker produces a protein fasta file (called \"annotations.proteins.fa\") together with the annotation and this file should be located in your maker directory. Move in the proper folder: cd ~/annotation_course/practical4 Now link the annotation you choose to work with. The command will looks like: ln -s ~/annotation_course/practical2/maker/maker_with_abinitio/annotationByType/maker.gff maker_with_abinitio.gff Extract the corresponding proteins from the gff file: ln -s ~/annotation_course/data/genome/4.fa gff3_sp_extract_sequences.pl --gff maker_with_abinitio.gff -f 4.fa -p --cfs -o AA.fa","title":"Prepare the input data"},{"location":"nbis_annotation/practical_session/practical4_funcAnnotInterp/#interproscan-approach","text":"Interproscan combines a number of searches for conserved motifs and curated data sets of protein clusters etc. This step may take fairly long time. It is recommended to paralellize it for huge amount of data by doing analysis of chunks of tens or hundreds proteins.","title":"Interproscan approach"},{"location":"nbis_annotation/practical_session/practical4_funcAnnotInterp/#perform-interproscan-analysis","text":"InterproScan can be run through a website or from the command line on a linux server. Here we are interested in the command line approach. Interproscan allows to look up pathways, families, domains, sites, repeats, structural domains and other sequence features. Launch Interproscan with the option -h if you want have a look about all the parameters. The '-app' option allows defining the database used. Here we will use the PfamA,ProDom and SuperFamily databases. Interproscan uses an internal database that related entries in public databases to established GO terms. By running the '-goterms' option, we can add this information to our data set. If you enable the InterPro lookup ('-iprlookup'), you can also get the InterPro identifier corresponding to each motif retrieved: for example, the same motif is known as PF01623 in Pfam and as IPR002568 in InterPro. The option '-pa' provides mappings from matches to pathway information (MetaCyc,UniPathway,KEGG,Reactome). interproscan.sh -i AA.fa -t p -dp -pa -appl Pfam,ProDom-2006.1,SuperFamily-1.75 --goterms --iprlookup The analysis shoud take 2-3 secs per protein request - depending on how many sequences you have submitted, you can make a fairly deducted guess regarding the running time. You will obtain 3 result files with the following extension '.gff3', '.tsv' and '.xml'. Explanation of these output are available >>here<< .","title":"Perform InterproScan analysis"},{"location":"nbis_annotation/practical_session/practical4_funcAnnotInterp/#load-the-retrieved-functional-information-in-your-annotation-file","text":"Next, you could write scripts of your own to merge interproscan output into your annotation. Incidentially, Maker comes with utility scripts that can take InterProscan output and add it to a Maker annotation file (you need to load maker). ipr_update_gff: adds searchable tags to the gene and mRNA features in the GFF3 files. iprscan2gff3: adds physical viewable features for domains that can be displayed in JBrowse, Gbrowse, and Web Apollo. ipr_update_gff maker_with_abinitio.gff AA.fa.tsv > maker_with_abinitio_with_interpro.gff Where a match is found, the new file will now include features called Dbxref and/or Ontology_term in the gene and transcript feature field (9th column).","title":"load the retrieved functional information in your annotation file:"},{"location":"nbis_annotation/practical_session/practical4_funcAnnotInterp/#blast-approach","text":"Blast searches provide an indication about potential homology to known proteins. A 'full' Blast analysis can run for several days and consume several GB of Ram. Consequently, for a huge amount of data it is recommended to parallelize this step doing analysis of chunks of tens or hundreds proteins. This approach can be used to give a name to the genes and a function to the transcripts.","title":"BLAST approach"},{"location":"nbis_annotation/practical_session/practical4_funcAnnotInterp/#perform-blast-searches-from-the-command-line-on-uppmax","text":"To run Blast on your data, use the Ncbi Blast+ package against a Drosophila-specific database (included in the folder we have provided for you, under annotation_course/data/blastdb/uniprot_dmel/uniprot_dmel.fa ) - of course, any other NCBI database would also work: blastp -db ~/annotation_course/data/blastdb/uniprot_dmel/uniprot_dmel.fa -query AA.fa -outfmt 6 -out blast.out -num_threads 8 Against the Drosophila-specific database, the blast search takes about 2 secs per protein request - depending on how many sequences you have submitted, you can make a fairly deducted guess regarding the running time.","title":"Perform Blast searches from the command line on Uppmax:"},{"location":"nbis_annotation/practical_session/practical4_funcAnnotInterp/#process-the-blast-outout-with-annie","text":"The Blast outputs must be processed to retrieve the information of the closest protein (best e-value) found by Blast. This work will be done using annie . First download annie: git clone https://github.com/genomeannotation/Annie.git Now launch annie: Annie/annie.py -b blast.out -db ~/annotation_course/data/blastdb/uniprot_dmel/uniprot_dmel.fa -g maker_with_abinitio.gff -o annotation_blast.annie Annie writes in a 3-column table format file, providing gene name and mRNA product information. The purpose of annie is relatively simple. It recovers the information in the sequence header of the uniprot fasta file, from the best sequence found by Blast (the lowest e-value).","title":"Process the blast outout with Annie"},{"location":"nbis_annotation/practical_session/practical4_funcAnnotInterp/#load-the-retrieved-information-in-your-annotation-file","text":"Now you should be able to use the following script: maker_gff3manager_JD_v8.pl -f maker_with_abinitio_with_interpro.gff -b annotation_blast.annie --ID FLY -o finalOutputDir That will add the name attribute to the \"gene\" feature and the description attribute (corresponding to the product information) to the \"mRNA\" feature into you annotation file. This script may be used for other purpose like to modify the ID value by something more convenient (i.e FLYG00000001 instead of maker-4-exonerate_protein2genome-gene-8.41). The improved annotation is a file named \"codingGeneFeatures.gff\" inside the finalOutputDir. For displaying the product attribute in Webapollo you can change this attribute by description using this script: /home/student/.local/GAAS/annotation/WebApollo/gff3_webApollo_compliant.pl --gff finalOutputDir/codingGeneFeatures.gff -o final_annotation.gff","title":"load the retrieved information in your annotation file:"},{"location":"nbis_annotation/practical_session/practical4_funcAnnotInterp/#visualise-the-final-annotation","text":"Transfer the final_annotation.gff file to your computer using scp in a new terminal: scp -i ~/.ssh/azure_rsa student@ IP :/home/student/annotation_course/practical4/final_annotation.gff . Load the file in into the genome portal called drosophila_melanogaster_chr4 in the Webapollo genome browser available at the address http://annotation-prod.scilifelab.se:8080/NBIS_course/ . Here find the WebApollo instruction Wondeful ! insn't it ?","title":"Visualise the final annotation"},{"location":"nbis_annotation/practical_session/practical4_funcAnnotInterp/#whats-next","text":"Because of Makers' compatibility with GMOD standards, an annotation augmented in one or both of this way can be loaded into e.g. WebApollo and will save annotators a lot of work when e.g. adding meta data to transcript models.","title":"What's next?"},{"location":"nbis_annotation/practical_session/practical4_funcAnnotInterp/#submission-to-public-repository-creation-of-an-embl-file","text":"Once your are satisfied by the wonderful annotation you have done, it would useful important to submit it to a public repostiroy. Fisrt you will be applaused by the community because you share your nice work, secondly this is often mandatory if you wish to publish some work related to this annotation. Current state-of-the-art genome annotation tools use the GFF3 format as output, while this format is not accepted as submission format by the International Nucleotide Sequence Database Collaboration (INSDC) databases. Converting the GFF3 format to a format accepted by one of the three INSDC databases is a key step in the achievement of genome annotation projects. However, the flexibility existing in the GFF3 format makes this conversion task difficult to perform. In order to submit to NCBI , the use of a tool like GAG will save you lot time. In order to submit to EBI , the use of a tool like EMBLmyGFF3 will be your best choice. Let's prepare your annotation to submit to ENA (EBI) In real life, prior to a submission to ENA, you need to create an account and create a project asking a locus_tag for your annotation. You have also to fill lot of metada information related to the assembly and so on. We will skip those tasks using fake information. First you need to download and install EMBLmyGFF3: pip install --user git+https://github.com/NBISweden/EMBLmyGFF3.git EMBLmyGFF3 finalOutputDir/codingGeneFeatures.gff 4.fa -o my_annotation_ready_to_submit.embl You now have a EMBL flat file ready to submit. In theory to finsish the submission, you will have to send this archived file to their ftp server and finish the submission process in the website side too. But we will not go further. We are done. CONGRATULATION you know most of the secrets needed to understand the annotations on and perform your own !","title":"Submission to public repository (creation of an EMBL file)"},{"location":"tutorials/","text":"Tutorials This repository contains basic tutorials and walkthroughs on various bioinformatics subjects: Dev First clone the repository and install mkdocs and the theme using pipenv git clone https://github.com/HadrienG/tutorials.git cd tutorials pipenv install For a live preview in your browser do pipenv run dev Deploy The following command will build and push your website to a gh-pages branch. Only do this if you want your own version of the website! If you are modifying the original, please open a pull request. pipenv run deploy","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"This repository contains basic tutorials and walkthroughs on various bioinformatics subjects:","title":"Tutorials"},{"location":"tutorials/#dev","text":"First clone the repository and install mkdocs and the theme using pipenv git clone https://github.com/HadrienG/tutorials.git cd tutorials pipenv install For a live preview in your browser do pipenv run dev","title":"Dev"},{"location":"tutorials/#deploy","text":"The following command will build and push your website to a gh-pages branch. Only do this if you want your own version of the website! If you are modifying the original, please open a pull request. pipenv run deploy","title":"Deploy"},{"location":"tutorials/docs/","text":"Home Welcome! This website is a collection of Bioinformatics tutorials that I've accumulated over the years, while teaching a bioinformatics course at the Swedish University of Agricultural Sciences and during various bioinformatics workshops around the globe. Feel free to follow them online, or to use and modify them for your own teaching. Available lessons Home The command-line File Formats Quality Control and Trimming Mapping and Variant Calling De-novo Genome Assembly Genome Annotation Pan-Genome Analysis Metabarcoding Whole Metagenome Sequencing Metagenome assembly RNA-Seq Introduction to Nanopore Sequencing Contributing A typo? Something that irks you? Submit an issue or a pull request. In no particular order, the follwing people have contributed to these tutorials: Hadrien Gourl\u00e9 Oskar Karlsson-Lindsj\u00f6 Juliette Hayer License This work is licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.","title":"Home"},{"location":"tutorials/docs/#home","text":"Welcome! This website is a collection of Bioinformatics tutorials that I've accumulated over the years, while teaching a bioinformatics course at the Swedish University of Agricultural Sciences and during various bioinformatics workshops around the globe. Feel free to follow them online, or to use and modify them for your own teaching.","title":"Home"},{"location":"tutorials/docs/#available-lessons","text":"Home The command-line File Formats Quality Control and Trimming Mapping and Variant Calling De-novo Genome Assembly Genome Annotation Pan-Genome Analysis Metabarcoding Whole Metagenome Sequencing Metagenome assembly RNA-Seq Introduction to Nanopore Sequencing","title":"Available lessons"},{"location":"tutorials/docs/#contributing","text":"A typo? Something that irks you? Submit an issue or a pull request. In no particular order, the follwing people have contributed to these tutorials: Hadrien Gourl\u00e9 Oskar Karlsson-Lindsj\u00f6 Juliette Hayer","title":"Contributing"},{"location":"tutorials/docs/#license","text":"This work is licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.","title":"License"},{"location":"tutorials/docs/16S/","text":"Metabarcoding This tutorial is aimed at being a walkthrough of the DADA2 pipeline. It uses the data of the now famous MiSeq SOP by the Mothur authors but analyses the data using DADA2. DADA2 is a relatively new method to analyse amplicon data which uses exact variants instead of OTUs. The advantages of the DADA2 method is described in the paper Before Starting There are two ways to follow this tutorial: you can copy and paste all the codes blocks below in R directly, or you can download this document in the Rmarkdown format and execute the cells. Link to the document in Rmarkdown Install and Load Packages First install DADA2 and other necessary packages source('https://bioconductor.org/biocLite.R') biocLite('dada2') biocLite('phyloseq') biocLite('DECIPHER') install.packages('ggplot2') install.packages('phangorn') Now load the packages and verify you have the correct DADA2 version library(dada2) library(ggplot2) library(phyloseq) library(phangorn) library(DECIPHER) packageVersion('dada2') Download the Data You will also need to download the data, as well as the SILVA database Warning If you are following the tutorial on the website, the following block of commands has to be executed outside of R. If you run this tutorial with the R notebook, you can simply execute the cell block wget http://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip unzip MiSeqSOPData.zip rm -r __MACOSX/ cd MiSeq_SOP wget https://zenodo.org/record/824551/files/silva_nr_v128_train_set.fa.gz wget https://zenodo.org/record/824551/files/silva_species_assignment_v128.fa.gz cd .. Back in R, check that you have downloaded the data path <- 'MiSeq_SOP' list.files(path) Filtering and Trimming First we create two lists with the sorted name of the reads: one for the forward reads, one for the reverse reads raw_forward <- sort(list.files(path, pattern=\"_R1_001.fastq\", full.names=TRUE)) raw_reverse <- sort(list.files(path, pattern=\"_R2_001.fastq\", full.names=TRUE)) # we also need the sample names sample_names <- sapply(strsplit(basename(raw_forward), \"_\"), `[`, # extracts the first element of a subset 1) then we visualise the quality of our reads plotQualityProfile(raw_forward[1:2]) plotQualityProfile(raw_reverse[1:2]) Question What do you think of the read quality? The forward reads are good quality (although dropping a bit at the end as usual) while the reverse are way worse. Based on these profiles, we will truncate the forward reads at position 240 and the reverse reads at position 160 where the quality distribution crashes. Note in this tutorial we perform the trimming using DADA2's own functions. If you wish to do it outside of DADA2, you can refer to the Quality Control tutorial Dada2 requires us to define the name of our output files # place filtered files in filtered/ subdirectory filtered_path <- file.path(path, \"filtered\") filtered_forward <- file.path(filtered_path, paste0(sample_names, \"_R1_trimmed.fastq.gz\")) filtered_reverse <- file.path(filtered_path, paste0(sample_names, \"_R2_trimmed.fastq.gz\")) We\u2019ll use standard filtering parameters: maxN=0 (DADA22 requires no Ns), truncQ=2 , rm.phix=TRUE and maxEE=2 . The maxEE parameter sets the maximum number of \u201cexpected errors\u201d allowed in a read, which according to the USEARCH authors is a better filter than simply averaging quality scores. out <- filterAndTrim(raw_forward, filtered_forward, raw_reverse, filtered_reverse, truncLen=c(240,160), maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE) head(out) Learn the Error Rates The DADA2 algorithm depends on a parametric error model and every amplicon dataset has a slightly different error rate. The learnErrors of Dada2 learns the error model from the data and will help DADA2 to fits its method to your data errors_forward <- learnErrors(filtered_forward, multithread=TRUE) errors_reverse <- learnErrors(filtered_reverse, multithread=TRUE) then we visualise the estimated error rates plotErrors(errors_forward, nominalQ=TRUE) + theme_minimal() Question Do you think the error model fits your data correctly? Dereplication From the Dada2 documentation: Dereplication combines all identical sequencing reads into into \u201cunique sequences\u201d with a corresponding \u201cabundance\u201d: the number of reads with that unique sequence. Dereplication substantially reduces computation time by eliminating redundant comparisons. derep_forward <- derepFastq(filtered_forward, verbose=TRUE) derep_reverse <- derepFastq(filtered_reverse, verbose=TRUE) # name the derep-class objects by the sample names names(derep_forward) <- sample_names names(derep_reverse) <- sample_names Sample inference We are now ready to apply the core sequence-variant inference algorithm to the dereplicated data. dada_forward <- dada(derep_forward, err=errors_forward, multithread=TRUE) dada_reverse <- dada(derep_reverse, err=errors_reverse, multithread=TRUE) # inspect the dada-class object dada_forward[[1]] The DADA2 algorithm inferred 128 real sequence variants from the 1979 unique sequences in the first sample. Merge Paired-end Reads Now that the reads are trimmed, dereplicated and error-corrected we can merge them together merged_reads <- mergePairs(dada_forward, derep_forward, dada_reverse, derep_reverse, verbose=TRUE) # inspect the merger data.frame from the first sample head(merged_reads[[1]]) Construct Sequence Table We can now construct a sequence table of our mouse samples, a higher-resolution version of the OTU table produced by traditional methods. seq_table <- makeSequenceTable(merged_reads) dim(seq_table) # inspect distribution of sequence lengths table(nchar(getSequences(seq_table))) Remove Chimeras The dada method used earlier removes substitutions and indel errors but chimeras remain. We remove the chimeras with seq_table_nochim <- removeBimeraDenovo(seq_table, method='consensus', multithread=TRUE, verbose=TRUE) dim(seq_table_nochim) # which percentage of our reads did we keep? sum(seq_table_nochim) / sum(seq_table) As a final check of our progress, we\u2019ll look at the number of reads that made it through each step in the pipeline get_n <- function(x) sum(getUniques(x)) track <- cbind(out, sapply(dada_forward, get_n), sapply(merged_reads, get_n), rowSums(seq_table), rowSums(seq_table_nochim)) colnames(track) <- c('input', 'filtered', 'denoised', 'merged', 'tabled', 'nonchim') rownames(track) <- sample_names head(track) We kept the majority of our reads! Assign Taxonomy Now we assign taxonomy to our sequences using the SILVA database taxa <- assignTaxonomy(seq_table_nochim, 'MiSeq_SOP/silva_nr_v128_train_set.fa.gz', multithread=TRUE) taxa <- addSpecies(taxa, 'MiSeq_SOP/silva_species_assignment_v128.fa.gz') for inspecting the classification taxa_print <- taxa # removing sequence rownames for display only rownames(taxa_print) <- NULL head(taxa_print) Phylogenetic Tree DADA2 is reference-free so we have to build the tree ourselves We first align our sequences sequences <- getSequences(seq_table) names(sequences) <- sequences # this propagates to the tip labels of the tree alignment <- AlignSeqs(DNAStringSet(sequences), anchor=NA) Then we build a neighbour-joining tree then fit a maximum likelihood tree using the neighbour-joining tree as a starting point phang_align <- phyDat(as(alignment, 'matrix'), type='DNA') dm <- dist.ml(phang_align) treeNJ <- NJ(dm) # note, tip order != sequence order fit = pml(treeNJ, data=phang_align) ## negative edges length changed to 0! fitGTR <- update(fit, k=4, inv=0.2) fitGTR <- optim.pml(fitGTR, model='GTR', optInv=TRUE, optGamma=TRUE, rearrangement = 'stochastic', control = pml.control(trace = 0)) detach('package:phangorn', unload=TRUE) Phyloseq First load the metadata sample_data <- read.table( 'https://hadrieng.github.io/tutorials/data/16S_metadata.txt', header=TRUE, row.names=\"sample_name\") We can now construct a phyloseq object from our output and newly created metadata physeq <- phyloseq(otu_table(seq_table_nochim, taxa_are_rows=FALSE), sample_data(sample_data), tax_table(taxa), phy_tree(fitGTR$tree)) # remove mock sample physeq <- prune_samples(sample_names(physeq) != 'Mock', physeq) physeq Let's look at the alpha diversity of our samples plot_richness(physeq, x='day', measures=c('Shannon', 'Fisher'), color='when') + theme_minimal() No obvious differences. Let's look at ordination methods (beta diversity) We can perform an MDS with euclidean distance (mathematically equivalent to a PCA) ord <- ordinate(physeq, 'MDS', 'euclidean') plot_ordination(physeq, ord, type='samples', color='when', title='PCA of the samples from the MiSeq SOP') + theme_minimal() now with the Bray-Curtis distance ord <- ordinate(physeq, 'NMDS', 'bray') plot_ordination(physeq, ord, type='samples', color='when', title='PCA of the samples from the MiSeq SOP') + theme_minimal() There we can see a clear difference between our samples. Let us take a look a the distribution of the most abundant families top20 <- names(sort(taxa_sums(physeq), decreasing=TRUE))[1:20] physeq_top20 <- transform_sample_counts(physeq, function(OTU) OTU/sum(OTU)) physeq_top20 <- prune_taxa(top20, physeq_top20) plot_bar(physeq_top20, x='day', fill='Family') + facet_wrap(~when, scales='free_x') + theme_minimal() We can place them in a tree bacteroidetes <- subset_taxa(physeq, Phylum %in% c('Bacteroidetes')) plot_tree(bacteroidetes, ladderize='left', size='abundance', color='when', label.tips='Family')","title":"Metabarcoding"},{"location":"tutorials/docs/16S/#metabarcoding","text":"This tutorial is aimed at being a walkthrough of the DADA2 pipeline. It uses the data of the now famous MiSeq SOP by the Mothur authors but analyses the data using DADA2. DADA2 is a relatively new method to analyse amplicon data which uses exact variants instead of OTUs. The advantages of the DADA2 method is described in the paper","title":"Metabarcoding"},{"location":"tutorials/docs/16S/#before-starting","text":"There are two ways to follow this tutorial: you can copy and paste all the codes blocks below in R directly, or you can download this document in the Rmarkdown format and execute the cells. Link to the document in Rmarkdown","title":"Before Starting"},{"location":"tutorials/docs/16S/#install-and-load-packages","text":"First install DADA2 and other necessary packages source('https://bioconductor.org/biocLite.R') biocLite('dada2') biocLite('phyloseq') biocLite('DECIPHER') install.packages('ggplot2') install.packages('phangorn') Now load the packages and verify you have the correct DADA2 version library(dada2) library(ggplot2) library(phyloseq) library(phangorn) library(DECIPHER) packageVersion('dada2')","title":"Install and Load Packages"},{"location":"tutorials/docs/16S/#download-the-data","text":"You will also need to download the data, as well as the SILVA database Warning If you are following the tutorial on the website, the following block of commands has to be executed outside of R. If you run this tutorial with the R notebook, you can simply execute the cell block wget http://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip unzip MiSeqSOPData.zip rm -r __MACOSX/ cd MiSeq_SOP wget https://zenodo.org/record/824551/files/silva_nr_v128_train_set.fa.gz wget https://zenodo.org/record/824551/files/silva_species_assignment_v128.fa.gz cd .. Back in R, check that you have downloaded the data path <- 'MiSeq_SOP' list.files(path)","title":"Download the Data"},{"location":"tutorials/docs/16S/#filtering-and-trimming","text":"First we create two lists with the sorted name of the reads: one for the forward reads, one for the reverse reads raw_forward <- sort(list.files(path, pattern=\"_R1_001.fastq\", full.names=TRUE)) raw_reverse <- sort(list.files(path, pattern=\"_R2_001.fastq\", full.names=TRUE)) # we also need the sample names sample_names <- sapply(strsplit(basename(raw_forward), \"_\"), `[`, # extracts the first element of a subset 1) then we visualise the quality of our reads plotQualityProfile(raw_forward[1:2]) plotQualityProfile(raw_reverse[1:2]) Question What do you think of the read quality? The forward reads are good quality (although dropping a bit at the end as usual) while the reverse are way worse. Based on these profiles, we will truncate the forward reads at position 240 and the reverse reads at position 160 where the quality distribution crashes. Note in this tutorial we perform the trimming using DADA2's own functions. If you wish to do it outside of DADA2, you can refer to the Quality Control tutorial Dada2 requires us to define the name of our output files # place filtered files in filtered/ subdirectory filtered_path <- file.path(path, \"filtered\") filtered_forward <- file.path(filtered_path, paste0(sample_names, \"_R1_trimmed.fastq.gz\")) filtered_reverse <- file.path(filtered_path, paste0(sample_names, \"_R2_trimmed.fastq.gz\")) We\u2019ll use standard filtering parameters: maxN=0 (DADA22 requires no Ns), truncQ=2 , rm.phix=TRUE and maxEE=2 . The maxEE parameter sets the maximum number of \u201cexpected errors\u201d allowed in a read, which according to the USEARCH authors is a better filter than simply averaging quality scores. out <- filterAndTrim(raw_forward, filtered_forward, raw_reverse, filtered_reverse, truncLen=c(240,160), maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE) head(out)","title":"Filtering and Trimming"},{"location":"tutorials/docs/16S/#learn-the-error-rates","text":"The DADA2 algorithm depends on a parametric error model and every amplicon dataset has a slightly different error rate. The learnErrors of Dada2 learns the error model from the data and will help DADA2 to fits its method to your data errors_forward <- learnErrors(filtered_forward, multithread=TRUE) errors_reverse <- learnErrors(filtered_reverse, multithread=TRUE) then we visualise the estimated error rates plotErrors(errors_forward, nominalQ=TRUE) + theme_minimal() Question Do you think the error model fits your data correctly?","title":"Learn the Error Rates"},{"location":"tutorials/docs/16S/#dereplication","text":"From the Dada2 documentation: Dereplication combines all identical sequencing reads into into \u201cunique sequences\u201d with a corresponding \u201cabundance\u201d: the number of reads with that unique sequence. Dereplication substantially reduces computation time by eliminating redundant comparisons. derep_forward <- derepFastq(filtered_forward, verbose=TRUE) derep_reverse <- derepFastq(filtered_reverse, verbose=TRUE) # name the derep-class objects by the sample names names(derep_forward) <- sample_names names(derep_reverse) <- sample_names","title":"Dereplication"},{"location":"tutorials/docs/16S/#sample-inference","text":"We are now ready to apply the core sequence-variant inference algorithm to the dereplicated data. dada_forward <- dada(derep_forward, err=errors_forward, multithread=TRUE) dada_reverse <- dada(derep_reverse, err=errors_reverse, multithread=TRUE) # inspect the dada-class object dada_forward[[1]] The DADA2 algorithm inferred 128 real sequence variants from the 1979 unique sequences in the first sample.","title":"Sample inference"},{"location":"tutorials/docs/16S/#merge-paired-end-reads","text":"Now that the reads are trimmed, dereplicated and error-corrected we can merge them together merged_reads <- mergePairs(dada_forward, derep_forward, dada_reverse, derep_reverse, verbose=TRUE) # inspect the merger data.frame from the first sample head(merged_reads[[1]])","title":"Merge Paired-end Reads"},{"location":"tutorials/docs/16S/#construct-sequence-table","text":"We can now construct a sequence table of our mouse samples, a higher-resolution version of the OTU table produced by traditional methods. seq_table <- makeSequenceTable(merged_reads) dim(seq_table) # inspect distribution of sequence lengths table(nchar(getSequences(seq_table)))","title":"Construct Sequence Table"},{"location":"tutorials/docs/16S/#remove-chimeras","text":"The dada method used earlier removes substitutions and indel errors but chimeras remain. We remove the chimeras with seq_table_nochim <- removeBimeraDenovo(seq_table, method='consensus', multithread=TRUE, verbose=TRUE) dim(seq_table_nochim) # which percentage of our reads did we keep? sum(seq_table_nochim) / sum(seq_table) As a final check of our progress, we\u2019ll look at the number of reads that made it through each step in the pipeline get_n <- function(x) sum(getUniques(x)) track <- cbind(out, sapply(dada_forward, get_n), sapply(merged_reads, get_n), rowSums(seq_table), rowSums(seq_table_nochim)) colnames(track) <- c('input', 'filtered', 'denoised', 'merged', 'tabled', 'nonchim') rownames(track) <- sample_names head(track) We kept the majority of our reads!","title":"Remove Chimeras"},{"location":"tutorials/docs/16S/#assign-taxonomy","text":"Now we assign taxonomy to our sequences using the SILVA database taxa <- assignTaxonomy(seq_table_nochim, 'MiSeq_SOP/silva_nr_v128_train_set.fa.gz', multithread=TRUE) taxa <- addSpecies(taxa, 'MiSeq_SOP/silva_species_assignment_v128.fa.gz') for inspecting the classification taxa_print <- taxa # removing sequence rownames for display only rownames(taxa_print) <- NULL head(taxa_print)","title":"Assign Taxonomy"},{"location":"tutorials/docs/16S/#phylogenetic-tree","text":"DADA2 is reference-free so we have to build the tree ourselves We first align our sequences sequences <- getSequences(seq_table) names(sequences) <- sequences # this propagates to the tip labels of the tree alignment <- AlignSeqs(DNAStringSet(sequences), anchor=NA) Then we build a neighbour-joining tree then fit a maximum likelihood tree using the neighbour-joining tree as a starting point phang_align <- phyDat(as(alignment, 'matrix'), type='DNA') dm <- dist.ml(phang_align) treeNJ <- NJ(dm) # note, tip order != sequence order fit = pml(treeNJ, data=phang_align) ## negative edges length changed to 0! fitGTR <- update(fit, k=4, inv=0.2) fitGTR <- optim.pml(fitGTR, model='GTR', optInv=TRUE, optGamma=TRUE, rearrangement = 'stochastic', control = pml.control(trace = 0)) detach('package:phangorn', unload=TRUE)","title":"Phylogenetic Tree"},{"location":"tutorials/docs/16S/#phyloseq","text":"First load the metadata sample_data <- read.table( 'https://hadrieng.github.io/tutorials/data/16S_metadata.txt', header=TRUE, row.names=\"sample_name\") We can now construct a phyloseq object from our output and newly created metadata physeq <- phyloseq(otu_table(seq_table_nochim, taxa_are_rows=FALSE), sample_data(sample_data), tax_table(taxa), phy_tree(fitGTR$tree)) # remove mock sample physeq <- prune_samples(sample_names(physeq) != 'Mock', physeq) physeq Let's look at the alpha diversity of our samples plot_richness(physeq, x='day', measures=c('Shannon', 'Fisher'), color='when') + theme_minimal() No obvious differences. Let's look at ordination methods (beta diversity) We can perform an MDS with euclidean distance (mathematically equivalent to a PCA) ord <- ordinate(physeq, 'MDS', 'euclidean') plot_ordination(physeq, ord, type='samples', color='when', title='PCA of the samples from the MiSeq SOP') + theme_minimal() now with the Bray-Curtis distance ord <- ordinate(physeq, 'NMDS', 'bray') plot_ordination(physeq, ord, type='samples', color='when', title='PCA of the samples from the MiSeq SOP') + theme_minimal() There we can see a clear difference between our samples. Let us take a look a the distribution of the most abundant families top20 <- names(sort(taxa_sums(physeq), decreasing=TRUE))[1:20] physeq_top20 <- transform_sample_counts(physeq, function(OTU) OTU/sum(OTU)) physeq_top20 <- prune_taxa(top20, physeq_top20) plot_bar(physeq_top20, x='day', fill='Family') + facet_wrap(~when, scales='free_x') + theme_minimal() We can place them in a tree bacteroidetes <- subset_taxa(physeq, Phylum %in% c('Bacteroidetes')) plot_tree(bacteroidetes, ladderize='left', size='abundance', color='when', label.tips='Family')","title":"Phyloseq"},{"location":"tutorials/docs/16S_mothur/","text":"Metabarcoding This tutorial is largely inspired of the MiSeq SOP from the Schloss Lab. Kozich JJ, Westcott SL, Baxter NT, Highlander SK, Schloss PD. (2013): Development of a dual-index sequencing strategy and curation pipeline for analyzing amplicon sequence data on the MiSeq Illumina sequencing platform. Applied and Environmental Microbiology. 79(17):5112-20. Table of Contents Introduction Softwares Required for this Tutorial Downloading the Data and Start Mothur Reducing Sequencing and PCR Errors Processing Improved Sequences Analysis OTUs Batch Mode Introduction The 16S rRNA gene is a section of prokaryotic DNA found in all bacteria and archaea. This gene codes for an rRNA, and this rRNA in turn makes up part of the ribosome. The first 'r' in rRNA stands for ribosomal. The ribosome is composed of two subunits, the large subunit (LSU) and the small subunit (SSU). The 16S rRNA gene is a commonly used tool for identifying bacteria for several reasons. First, traditional characterization depended upon phenotypic traits like gram positive or gram negative, bacillus or coccus, etc. Taxonomists today consider analysis of an organism's DNA more reliable than classification based solely on phenotypes. Secondly, researchers may, for a number of reasons, want to identify or classify only the bacteria within a given environmental or medical sample. While there is a homologous gene in eukaryotes, the 18S rRNA gene, it is distinct, thereby rendering the 16S rRNA gene a useful tool for extracting and identifying bacteria as separate from plant, animal, fungal, and protist DNA within the same sample. Thirdly, the 16S rRNA gene is relatively short at 1.5 kb, making it faster and cheaper to sequence than many other unique bacterial genes. Mothur is a command-line computer program for analyzing sequence data from microbial communities and namely 16s data. mothur is licensed under the GPL and is free to use. Softwares Required for this Tutorial mothur mothur_krona Downloading the Data and Start Mothur Firstly, download and unzip the sample dataset: wget http://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip unzip MiSeqSOPData.zip In the MiSeq_SOP directory, you'll find the reads files in fastq format, as well as a file called stability.files The first lines of stability.files look like this: F3D0 F3D0_S188_L001_R1_001.fastq F3D0_S188_L001_R2_001.fastq F3D141 F3D141_S207_L001_R1_001.fastq F3D141_S207_L001_R2_001.fastq F3D142 F3D142_S208_L001_R1_001.fastq F3D142_S208_L001_R2_001.fastq F3D143 F3D143_S209_L001_R1_001.fastq F3D143_S209_L001_R2_001.fastq The first column is the name of the sample. The second column is the name of the forward read for that sample and the third columns in the name of the reverse read for that sample. Now it's time to start mothur. Type mothur in your terminal. You should see your prompt changing to mothur > Reducing Sequencing and PCR Errors The first thing we want to do is combine our two sets of reads for each sample and then to combine the data from all of the samples. This is done using the make.contigs command, which requires stability.files as input. This command will extract the sequence and quality score data from your fastq files, create the reverse complement of the reverse read and then join the reads into contigs. make.contigs(file=stability.files, processors=8) It took 30 secs to process 152360 sequences. Group count: F3D0 7793 F3D1 5869 F3D141 5958 F3D142 3183 F3D143 3178 F3D144 4827 F3D145 7377 F3D146 5021 F3D147 17070 F3D148 12405 F3D149 13083 F3D150 5509 F3D2 19620 F3D3 6758 F3D5 4448 F3D6 7989 F3D7 5129 F3D8 5294 F3D9 7070 Mock 4779 Total of all groups is 152360 Output File Names: stability.trim.contigs.fasta stability.trim.contigs.qual stability.contigs.report stability.scrap.contigs.fasta stability.scrap.contigs.qual stability.contigs.groups The stability.contigs.report file will tell you something about the contig assembly for each read. Let's see what these sequences look like using the summary.seqs command: summary.seqs(fasta=stability.trim.contigs.fasta) Start End NBases Ambigs Polymer NumSeqs Minimum: 1 248 248 0 3 1 2.5%-tile: 1 252 252 0 3 3810 25%-tile: 1 252 252 0 4 38091 Median: 1 252 252 0 4 76181 75%-tile: 1 253 253 0 5 114271 97.5%-tile: 1 253 253 6 6 148552 Maximum: 1 502 502 249 243 152360 Mean: 1 252.811 252.811 0.70063 4.44854 # of Seqs: 152360 This tells us that we have 152360 sequences that for the most part vary between 248 and 253 bases. Interestingly, the longest read in the dataset is 502 bp. Be suspicious of this, the reads are supposed to be 251 bp each. This read clearly didn't assemble well (or at all). Also, note that at least 2.5% of our sequences had some ambiguous base calls. We'll take care of these issues in the next step when we run screen.seqs . screen.seqs(fasta=stability.trim.contigs.fasta, group=stability.contigs.groups, maxambig=0, maxlength=275) You'll notice that mothur remembered that we used 8 processors in make.contigs . To see what else mothur knows about you, run the following: get.current() Current files saved by mothur: fasta=stability.trim.contigs.good.fasta group=stability.contigs.good.groups qfile=stability.trim.contigs.qual processors=8 summary=stability.trim.contigs.summary What this means is that mothur remembers your latest fasta file and group file as well as the number of processors you have. So you could run: mothur > summary.seqs(fasta=stability.trim.contigs.good.fasta) mothur > summary.seqs(fasta=current) mothur > summary.seqs() and get the same output for each command. But, now that we have filtered the sequencing errors, let's move to the next step. Processing Improved Sequences We anticipate that many of our sequences are duplicates of each other. Because it's computationally wasteful to align the same sequences several times, we'll make our sequences unique: unique.seqs(fasta=stability.trim.contigs.good.fasta) If two sequences have the same identical sequence, then they're considered duplicates and will get merged. In the screen output there are two columns - the first is the number of sequences characterized and the second is the number of unique sequences remaining Another thing to do to make our lives easier is to simplify the names and group files. If you look at the most recent versions of those files you'll see together they are 13 MB. This may not seem like much, but with a full MiSeq run those long sequence names can add up and make life tedious. So we'll run count.seqs to generate a table where the rows are the names of the unique seqeunces and the columns are the names of the groups. The table is then filled with the number of times each unique sequence shows up in each group. This will generate a file called stability.trim.contigs.good.count_table. In subsequent commands we'll use it by using the count option: count.seqs(name=stability.trim.contigs.good.names, group=stability.contigs.good.groups) summary.seqs(count=stability.trim.contigs.good.count_table) Using stability.trim.contigs.good.unique.fasta as input file for the fasta parameter. Using 8 processors. Start End NBases Ambigs Polymer NumSeqs Minimum: 1 250 250 0 3 1 2.5%-tile: 1 252 252 0 3 3227 25%-tile: 1 252 252 0 4 32265 Median: 1 252 252 0 4 64530 75%-tile: 1 253 253 0 5 96794 97.5%-tile: 1 253 253 0 6 125832 Maximum: 1 270 270 0 12 129058 Mean: 1 252.462 252.462 0 4.36663 # of unique seqs: 16477 total # of seqs: 129058 Now we need to align our sequences to the reference alignment. First we need to download the SILVA database. # This step should be done outside mothur wget http://www.mothur.org/w/images/9/98/Silva.bacteria.zip unzip Silva.bacteria.zip If you have quit mothur to download the database, rerun the mothur command, then take a look at the database you have downloaded: summary.seqs(fasta=silva.bacteria/silva.bacteria.fasta, processors=8) Now do the alignment using align.seqs : align.seqs(fasta=stability.trim.contigs.good.unique.fasta, reference=silva.bacteria/silva.bacteria.fasta) We can then run summary.seqs again to get a summary of our alignment: summary.seqs(fasta=stability.trim.contigs.good.unique.align, count=stability.trim.contigs.good.count_table) You'll see that the bulk of the sequences start at position 13862 and end at position 23444. Some sequences start at position 13144 or 13876 and end at 22587 or 25294. These deviants from the mode positions are likely due to an insertion or deletion at the terminal ends of the aliignments. Sometimes you'll see sequences that start and end at the same position indicating a very poor alignment, which is generally due to non-specific amplification. To make sure that everything overlaps the same region we'll re-run screen.seqs to get sequences that start at or before position 1968 and end at or after position 11550. We'll also set the maximum homopolymer length to 8 since there's nothing in the database with a stretch of 9 or more of the same base in a row (this really could have been done in the first execution of screen.seqs above). Note that we need the count table so that we can update the table for the sequences we're removing and we're also using the summary file so we don't have to figure out again all the start and stop positions: screen.seqs(fasta=stability.trim.contigs.good.unique.align, count=stability.trim.contigs.good.count_table, summary=stability.trim.contigs.good.unique.summary, start=13862, end=23444, maxhomop=8) summary.seqs(fasta=current, count=current) No we can trim both ends of the aligned reads to be sure the all overlap exactly the same region. We can do this with fliter.seqs filter.seqs(fasta=stability.trim.contigs.good.unique.good.align, vertical=T, trump=.) We may have introduced redundancy by trimming the ends of the sequences, so we will re-run unique.seqs unique.seqs(fasta=stability.trim.contigs.good.unique.good.filter.fasta, count=stability.trim.contigs.good.good.count_table) This identified 3 duplicate sequences that we've now merged with previous unique sequences. The next thing we want to do to further de-noise our sequences is to pre-cluster the sequences using the pre.cluster command allowing for up to 2 differences between sequences. This command will split the sequences by group and then sort them by abundance and go from most abundant to least and identify sequences that are within 2 nt of each other. If they are then they get merged. We generally favor allowing 1 difference for every 100 bp of sequence: pre.cluster(fasta=stability.trim.contigs.good.unique.good.filter.unique.fasta, count=stability.trim.contigs.good.unique.good.filter.count_table, diffs=2) At this point we have removed as much sequencing error as we can and it is time to turn our attention to removing chimeras. We'll do this using the UCHIME algorithm that is called within mothur using the chimera.uchime command. Again, this command will split the data by sample and check for chimeras. Our preferred way of doing this is to use the abundant sequences as our reference. In addition, if a sequence is flagged as chimeric in one sample, the the default (dereplicate=F) is to remove it from all samples. Our experience suggests that this is a bit aggressive since we've seen rare sequences get flagged as chimeric when they're the most abundant sequence in another sample. This is how we do it: chimera.uchime(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.count_table, dereplicate=t) Running chimera.uchime with the count file will remove the chimeric sequences from the count file. But you still need to remove those sequences from the fasta file. We do this using remove.seqs : remove.seqs(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.fasta, accnos=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.accnos) As a final quality control step, we need to see if there are any \"undesirables\" in our dataset. Sometimes when we pick a primer set they will amplify other stuff that gets to this point in the pipeline such as 18S rRNA gene fragments or 16S rRNA from Archaea, chloroplasts, and mitochondira. There's also just the random stuff that we want to get rid of. Let's go ahead and classify those sequences using the Bayesian classifier with the classify.seqs command: classify.seqs(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.count_table, reference=silva.bacteria/silva.bacteria.fasta, taxonomy=silva.bacteria/silva.bacteria.rdp.tax, cutoff=80) Now that everything is classified we want to remove our undesirables. We do this with the remove.lineage command: remove.lineage(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.count_table, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.rdp.wang.taxonomy, taxon=Chloroplast-Mitochondria-unknown-Archaea-Eukaryota) Analysis OTUs We will use cluster.split for clustering sequences into OTUs cluster.split(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.count_table, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.rdp.wang.pick.taxonomy, splitmethod=classify, taxlevel=4, cutoff=0.15) We used taxlevel=4 , which corresponds to the level of Order Next we want to know how many sequences are in each OTU from each group and we can do this using the make.shared command . Here we tell mothur that we're really only interested in the 0.03 cutoff level: make.shared(list=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.an.unique_list.list, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.count_table, label=0.03) We also want to know the taxonomy for each of our OTUs. We can get the consensus taxonomy for each OTU using the classify.otu command classify.otu(list=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.an.unique_list.list, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.count_table, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.rdp.wang.pick.taxonomy, label=0.03) If you open the file stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.an.unique_list.0.03.cons.taxonomy , you can get information about your OTUs. OTU Size Taxonomy Otu0001 12328 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100); Otu0002 8918 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100); Otu0003 7850 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100); Otu0004 7478 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100); Otu0005 7478 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100); Otu0006 6650 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100); Otu0007 6341 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Bacteroidaceae(100);Bacteroides(100);Bacteroides_unclassified(100); Otu0008 5374 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Rikenellaceae(100);Alistipes(100);Alistipes_unclassified(100); Otu0009 3618 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100); This is telling you that Otu0001 was observed 12328 times in your sample and that 100% of the sequences were from Barnesiella In order to vizualise the composition of our datasets, we'll use phyloseq, a R package to work with microbiom data. Most of the phyloseq functionalities require aand a tree file. We need to generate it with mothur: dist.seqs(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, output=lt, processors=8) clearcut(phylip=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.phylip.dist) Batch Mode It is perfectly acceptable to enter the commands for your analysis from within mothur. We call this the interactive mode. If you are doing a lot these types of analysis or you want to use this SOP on your own data without thinking too much, you can run mothur in batch mode using ./mothur script.batch where script.batch (or whatever name you want, really) is a text file containing all the commands that you previously entered in interactive mode. If you have time, copy all the commands from this tutorial in a file, a try to make mothur work in batch mode! PhyloSeq Analysis First, install and load the phyloseq package: source('http://bioconductor.org/biocLite.R') biocLite('phyloseq') library(\"phyloseq\") library(\"ggplot2\") library(\"plyr\") theme_set(theme_bw()) # set the ggplot theme The PhyloSeq package has an import_mothur function that you can use to import the files you generated with mothur. As an example, import the example mothur data provided by phyloseq as an example: mothlist <- system.file(\"extdata\", \"esophagus.fn.list.gz\", package=\"phyloseq\") mothgroup <- system.file(\"extdata\", \"esophagus.good.groups.gz\", package=\"phyloseq\") mothtree <- system.file(\"extdata\", \"esophagus.tree.gz\", package=\"phyloseq\") show_mothur_cutoffs(mothlist) cutoff <- '0.10' x <- import_mothur(mothlist, mothgroup, mothtree, cutoff) x Note: If if you ever work with 16s data and decide to use QIIME instead of mothur, phyloseq also has an import_qiime function. Also, newer version of qiime and mothur have the ability to produce a .biom file. \u201cThe biom file format (canonically pronounced \u2018biome\u2019) is designed to be a general-use format for representing counts of observations in one or more biological samples. BIOM is a recognized standard for the Earth Microbiome Project and is a Genomics Standards Consortium candidate project.\u201d More info on http://biom-format.org/ For the rest of this tutorial, we will work with an example dataset provided by the phyloseq package. Load the data with the following command: data(enterotype) data(\"GlobalPatterns\") Ordination and distance-based analysis Let's do some preliminary filtering. Remove the OTUs that included all unassigned sequences (\"-1\") enterotype <- subset_species(enterotype, Genus != \"-1\") The available distance methods coded in the phyloseq package: dist_methods <- unlist(distanceMethodList) print(dist_methods) ## UniFrac1 UniFrac2 DPCoA JSD vegdist1 ## \"unifrac\" \"wunifrac\" \"dpcoa\" \"jsd\" \"manhattan\" ## vegdist2 vegdist3 vegdist4 vegdist5 vegdist6 ## \"euclidean\" \"canberra\" \"bray\" \"kulczynski\" \"jaccard\" ## vegdist7 vegdist8 vegdist9 vegdist10 vegdist11 ## \"gower\" \"altGower\" \"morisita\" \"horn\" \"mountford\" ## vegdist12 vegdist13 vegdist14 vegdist15 betadiver1 ## \"raup\" \"binomial\" \"chao\" \"cao\" \"w\" ## betadiver2 betadiver3 betadiver4 betadiver5 betadiver6 ## \"-1\" \"c\" \"wb\" \"r\" \"I\" ## betadiver7 betadiver8 betadiver9 betadiver10 betadiver11 ## \"e\" \"t\" \"me\" \"j\" \"sor\" ## betadiver12 betadiver13 betadiver14 betadiver15 betadiver16 ## \"m\" \"-2\" \"co\" \"cc\" \"g\" ## betadiver17 betadiver18 betadiver19 betadiver20 betadiver21 ## \"-3\" \"l\" \"19\" \"hk\" \"rlb\" ## betadiver22 betadiver23 betadiver24 dist1 dist2 ## \"sim\" \"gl\" \"z\" \"maximum\" \"binary\" ## dist3 designdist ## \"minkowski\" \"ANY\" Remove the two distance-methods that require a tree, and the generic custom method that requires user-defined distance arguments. # These require tree dist_methods[(1:3)] # Remove them from the vector dist_methods <- dist_methods[-(1:3)] # This is the user-defined method: dist_methods[\"designdist\"] # Remove the user-defined distance dist_methods = dist_methods[-which(dist_methods==\"ANY\")] Loop through each distance method, save each plot to a list, called plist. plist <- vector(\"list\", length(dist_methods)) names(plist) = dist_methods for( i in dist_methods ){ # Calculate distance matrix iDist <- distance(enterotype, method=i) # Calculate ordination iMDS <- ordinate(enterotype, \"MDS\", distance=iDist) ## Make plot # Don't carry over previous plot (if error, p will be blank) p <- NULL # Create plot, store as temp variable, p p <- plot_ordination(enterotype, iMDS, color=\"SeqTech\", shape=\"Enterotype\") # Add title to each plot p <- p + ggtitle(paste(\"MDS using distance method \", i, sep=\"\")) # Save the graphic to file. plist[[i]] = p } Combine results and shade according to Sequencing technology: df = ldply(plist, function(x) x$data) names(df)[1] <- \"distance\" p = ggplot(df, aes(Axis.1, Axis.2, color=SeqTech, shape=Enterotype)) p = p + geom_point(size=3, alpha=0.5) p = p + facet_wrap(~distance, scales=\"free\") p = p + ggtitle(\"MDS on various distance metrics for Enterotype dataset\") p Print individual plots: print(plist[[\"jsd\"]]) print(plist[[\"jaccard\"]]) print(plist[[\"bray\"]]) print(plist[[\"euclidean\"]]) Alpha diversity graphics Here is the default graphic produced by the plot_richness function on the GP example dataset: GP <- prune_species(speciesSums(GlobalPatterns) > 0, GlobalPatterns) plot_richness(GP) Note that in this case, the Fisher calculation results in a warning (but still plots). We can avoid this by specifying a measures argument to plot_richness, which will include just the alpha-diversity measures that we want. plot_richness(GP, measures=c(\"Chao1\", \"Shannon\")) We can specify a sample variable on which to group/organize samples along the horizontal (x) axis. An experimentally meaningful categorical variable is usually a good choice \u2013 in this case, the \"SampleType\" variable works much better than attempting to interpret the sample names directly (as in the previous plot): plot_richness(GP, x=\"SampleType\", measures=c(\"Chao1\", \"Shannon\")) Now suppose we wanted to use an external variable in the plot that isn\u2019t in the GP dataset already \u2013 for example, a logical that indicated whether or not the samples are human-associated. First, define this new variable, human, as a factor (other vectors could also work; or other data you might have describing the samples). sampleData(GP)$human <- getVariable(GP, \"SampleType\") %in% c(\"Feces\", \"Mock\", \"Skin\", \"Tongue\") Now tell plot_richness to map the new human variable on the horizontal axis, and shade the points in different color groups, according to which \"SampleType\" they belong. plot_richness(GP, x=\"human\", color=\"SampleType\", measures=c(\"Chao1\", \"Shannon\")) We can merge samples that are from the environment (SampleType), and make the points bigger with a ggplot2 layer. First, merge the samples. GPst = merge_samples(GP, \"SampleType\") # repair variables that were damaged during merge (coerced to numeric) sample_data(GPst)$SampleType <- factor(sample_names(GPst)) sample_data(GPst)$human <- as.logical(sample_data(GPst)$human) p = plot_richness(GPst, x=\"human\", color=\"SampleType\", measures=c(\"Chao1\", \"Shannon\")) p + geom_point(size=5, alpha=0.7) Trees head(phy_tree(GlobalPatterns)$node.label, 10) The node data from the GlobalPatterns dataset are strange. They look like they might be bootstrap values, but they sometimes have two decimals. phy_tree(GlobalPatterns)$node.label = substr(phy_tree(GlobalPatterns)$node.label, 1, 4) Additionally, the dataset has many OTUs, too many to fit them all on a tree. Let's take the 50 more abundant and plot a basic tree: physeq = prune_taxa(taxa_names(GlobalPatterns)[1:50], GlobalPatterns) plot_tree(physeq) dots are annotated next to tips (OTUs) in the tree, one for each sample in which that OTU was observed. Let's color the dots by taxonomic ranks, and sample covariates: plot_tree(physeq, nodelabf=nodeplotboot(), ladderize=\"left\", color=\"SampleType\") by taxonomic class: plot_tree(physeq, nodelabf=nodeplotboot(), ladderize=\"left\", color=\"Class\") It can be useful to label the tips: plot_tree(physeq, color=\"SampleType\", label.tips=\"Genus\") Making a radial tree is easy with ggplot2, simply recognizing that our vertically-oriented tree is a cartesian mapping of the data to a graphic \u2013 and that a radial tree is the same mapping, but with polar coordinates instead. plot_tree(physeq, nodelabf=nodeplotboot(60,60,3), color=\"SampleType\", shape=\"Class\", ladderize=\"left\") + coord_polar(theta=\"y\") Bar plots Bar plots are one of the easiest way to vizualize your data. But be careful, they can be misleading if grouping sample! Let's take a subset of the GlobalPatterns dataset, and produce a basic bar plot: gp.ch = subset_taxa(GlobalPatterns, Phylum == \"Chlamydiae\") plot_bar(gp.ch) The dataset is plotted with every sample mapped individually to the horizontal (x) axis, and abundance values mapped to the veritcal (y) axis. At each sample\u2019s horizontal position, the abundance values for each OTU are stacked in order from greatest to least, separate by a thin horizontal line. As long as the parameters you choose to separate the data result in more than one OTU abundance value at the respective position in the plot, the values will be stacked in order as a means of displaying both the sum total value while still representing the individual OTU abundances. The bar plot will be clearer with color to represent the Genus to which each OTU belongs. plot_bar(gp.ch, fill=\"Genus\") Now keep the same fill color, and group the samples together by the SampleType variable; essentially, the environment from which the sample was taken and sequenced. plot_bar(gp.ch, x=\"SampleType\", fill=\"Genus\") A more complex example using facets: plot_bar(gp.ch, \"Family\", fill=\"Genus\", facet_grid=~SampleType) Heatmaps The following two lines subset the dataset to just the top 300 most abundant Bacteria taxa across all samples (in this case, with no prior preprocessing. Not recommended, but quick). data(\"GlobalPatterns\") gpt <- subset_taxa(GlobalPatterns, Kingdom==\"Bacteria\") gpt <- prune_taxa(names(sort(taxa_sums(gpt),TRUE)[1:300]), gpt) plot_heatmap(gpt, sample.label=\"SampleType\") subset a smaller dataset based on an Archaeal phylum gpac <- subset_taxa(GlobalPatterns, Phylum==\"Crenarchaeota\") plot_heatmap(gpac) Plot microbiome network There is a random aspect to some of the network layout methods. For complete reproducibility of the images produced later in this tutorial, it is possible to set the random number generator seed explicitly: set.seed(711L) Because we want to use the enterotype designations as a plot feature in these plots, we need to remove the 9 samples for which no enterotype designation was assigned (this will save us the hassle of some pesky warning messages, but everything still works; the offending samples are anyway omitted). enterotype = subset_samples(enterotype, !is.na(Enterotype)) Create an igraph-based network based on the default distance method, \u201cJaccard\u201d, and a maximum distance between connected nodes of 0.3. ig <- make_network(enterotype, max.dist=0.3) plot_network(ig, enterotype) The previous graphic displayed some interesting structure, with one or two major subgraphs comprising a majority of samples. Furthermore, there seemed to be a correlation in the sample naming scheme and position within the network. Instead of trying to read all of the sample names to understand the pattern, let\u2019s map some of the sample variables onto this graphic as color and shape: plot_network(ig, enterotype, color=\"SeqTech\", shape=\"Enterotype\", line_weight=0.4, label=NULL) In the previous examples, the choice of maximum-distance and distance method were informed, but arbitrary. Let\u2019s see what happens when the maximum distance is lowered, decreasing the number of edges in the network ig <- make_network(enterotype, max.dist=0.2) plot_network(ig, enterotype, color=\"SeqTech\", shape=\"Enterotype\", line_weight=0.4, label=NULL) Let\u2019s repeat the previous exercise, but replace the Jaccard (default) distance method with Bray-Curtis ig <- make_network(enterotype, dist.fun=\"bray\", max.dist=0.3) plot_network(ig, enterotype, color=\"SeqTech\", shape=\"Enterotype\", line_weight=0.4, label=NULL)","title":"Metabarcoding"},{"location":"tutorials/docs/16S_mothur/#metabarcoding","text":"This tutorial is largely inspired of the MiSeq SOP from the Schloss Lab. Kozich JJ, Westcott SL, Baxter NT, Highlander SK, Schloss PD. (2013): Development of a dual-index sequencing strategy and curation pipeline for analyzing amplicon sequence data on the MiSeq Illumina sequencing platform. Applied and Environmental Microbiology. 79(17):5112-20.","title":"Metabarcoding"},{"location":"tutorials/docs/16S_mothur/#table-of-contents","text":"Introduction Softwares Required for this Tutorial Downloading the Data and Start Mothur Reducing Sequencing and PCR Errors Processing Improved Sequences Analysis OTUs Batch Mode","title":"Table of Contents"},{"location":"tutorials/docs/16S_mothur/#introduction","text":"The 16S rRNA gene is a section of prokaryotic DNA found in all bacteria and archaea. This gene codes for an rRNA, and this rRNA in turn makes up part of the ribosome. The first 'r' in rRNA stands for ribosomal. The ribosome is composed of two subunits, the large subunit (LSU) and the small subunit (SSU). The 16S rRNA gene is a commonly used tool for identifying bacteria for several reasons. First, traditional characterization depended upon phenotypic traits like gram positive or gram negative, bacillus or coccus, etc. Taxonomists today consider analysis of an organism's DNA more reliable than classification based solely on phenotypes. Secondly, researchers may, for a number of reasons, want to identify or classify only the bacteria within a given environmental or medical sample. While there is a homologous gene in eukaryotes, the 18S rRNA gene, it is distinct, thereby rendering the 16S rRNA gene a useful tool for extracting and identifying bacteria as separate from plant, animal, fungal, and protist DNA within the same sample. Thirdly, the 16S rRNA gene is relatively short at 1.5 kb, making it faster and cheaper to sequence than many other unique bacterial genes. Mothur is a command-line computer program for analyzing sequence data from microbial communities and namely 16s data. mothur is licensed under the GPL and is free to use.","title":"Introduction"},{"location":"tutorials/docs/16S_mothur/#softwares-required-for-this-tutorial","text":"mothur mothur_krona","title":"Softwares Required for this Tutorial"},{"location":"tutorials/docs/16S_mothur/#downloading-the-data-and-start-mothur","text":"Firstly, download and unzip the sample dataset: wget http://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip unzip MiSeqSOPData.zip In the MiSeq_SOP directory, you'll find the reads files in fastq format, as well as a file called stability.files The first lines of stability.files look like this: F3D0 F3D0_S188_L001_R1_001.fastq F3D0_S188_L001_R2_001.fastq F3D141 F3D141_S207_L001_R1_001.fastq F3D141_S207_L001_R2_001.fastq F3D142 F3D142_S208_L001_R1_001.fastq F3D142_S208_L001_R2_001.fastq F3D143 F3D143_S209_L001_R1_001.fastq F3D143_S209_L001_R2_001.fastq The first column is the name of the sample. The second column is the name of the forward read for that sample and the third columns in the name of the reverse read for that sample. Now it's time to start mothur. Type mothur in your terminal. You should see your prompt changing to mothur >","title":"Downloading the Data and Start Mothur"},{"location":"tutorials/docs/16S_mothur/#reducing-sequencing-and-pcr-errors","text":"The first thing we want to do is combine our two sets of reads for each sample and then to combine the data from all of the samples. This is done using the make.contigs command, which requires stability.files as input. This command will extract the sequence and quality score data from your fastq files, create the reverse complement of the reverse read and then join the reads into contigs. make.contigs(file=stability.files, processors=8) It took 30 secs to process 152360 sequences. Group count: F3D0 7793 F3D1 5869 F3D141 5958 F3D142 3183 F3D143 3178 F3D144 4827 F3D145 7377 F3D146 5021 F3D147 17070 F3D148 12405 F3D149 13083 F3D150 5509 F3D2 19620 F3D3 6758 F3D5 4448 F3D6 7989 F3D7 5129 F3D8 5294 F3D9 7070 Mock 4779 Total of all groups is 152360 Output File Names: stability.trim.contigs.fasta stability.trim.contigs.qual stability.contigs.report stability.scrap.contigs.fasta stability.scrap.contigs.qual stability.contigs.groups The stability.contigs.report file will tell you something about the contig assembly for each read. Let's see what these sequences look like using the summary.seqs command: summary.seqs(fasta=stability.trim.contigs.fasta) Start End NBases Ambigs Polymer NumSeqs Minimum: 1 248 248 0 3 1 2.5%-tile: 1 252 252 0 3 3810 25%-tile: 1 252 252 0 4 38091 Median: 1 252 252 0 4 76181 75%-tile: 1 253 253 0 5 114271 97.5%-tile: 1 253 253 6 6 148552 Maximum: 1 502 502 249 243 152360 Mean: 1 252.811 252.811 0.70063 4.44854 # of Seqs: 152360 This tells us that we have 152360 sequences that for the most part vary between 248 and 253 bases. Interestingly, the longest read in the dataset is 502 bp. Be suspicious of this, the reads are supposed to be 251 bp each. This read clearly didn't assemble well (or at all). Also, note that at least 2.5% of our sequences had some ambiguous base calls. We'll take care of these issues in the next step when we run screen.seqs . screen.seqs(fasta=stability.trim.contigs.fasta, group=stability.contigs.groups, maxambig=0, maxlength=275) You'll notice that mothur remembered that we used 8 processors in make.contigs . To see what else mothur knows about you, run the following: get.current() Current files saved by mothur: fasta=stability.trim.contigs.good.fasta group=stability.contigs.good.groups qfile=stability.trim.contigs.qual processors=8 summary=stability.trim.contigs.summary What this means is that mothur remembers your latest fasta file and group file as well as the number of processors you have. So you could run: mothur > summary.seqs(fasta=stability.trim.contigs.good.fasta) mothur > summary.seqs(fasta=current) mothur > summary.seqs() and get the same output for each command. But, now that we have filtered the sequencing errors, let's move to the next step.","title":"Reducing Sequencing and PCR Errors"},{"location":"tutorials/docs/16S_mothur/#processing-improved-sequences","text":"We anticipate that many of our sequences are duplicates of each other. Because it's computationally wasteful to align the same sequences several times, we'll make our sequences unique: unique.seqs(fasta=stability.trim.contigs.good.fasta) If two sequences have the same identical sequence, then they're considered duplicates and will get merged. In the screen output there are two columns - the first is the number of sequences characterized and the second is the number of unique sequences remaining Another thing to do to make our lives easier is to simplify the names and group files. If you look at the most recent versions of those files you'll see together they are 13 MB. This may not seem like much, but with a full MiSeq run those long sequence names can add up and make life tedious. So we'll run count.seqs to generate a table where the rows are the names of the unique seqeunces and the columns are the names of the groups. The table is then filled with the number of times each unique sequence shows up in each group. This will generate a file called stability.trim.contigs.good.count_table. In subsequent commands we'll use it by using the count option: count.seqs(name=stability.trim.contigs.good.names, group=stability.contigs.good.groups) summary.seqs(count=stability.trim.contigs.good.count_table) Using stability.trim.contigs.good.unique.fasta as input file for the fasta parameter. Using 8 processors. Start End NBases Ambigs Polymer NumSeqs Minimum: 1 250 250 0 3 1 2.5%-tile: 1 252 252 0 3 3227 25%-tile: 1 252 252 0 4 32265 Median: 1 252 252 0 4 64530 75%-tile: 1 253 253 0 5 96794 97.5%-tile: 1 253 253 0 6 125832 Maximum: 1 270 270 0 12 129058 Mean: 1 252.462 252.462 0 4.36663 # of unique seqs: 16477 total # of seqs: 129058 Now we need to align our sequences to the reference alignment. First we need to download the SILVA database. # This step should be done outside mothur wget http://www.mothur.org/w/images/9/98/Silva.bacteria.zip unzip Silva.bacteria.zip If you have quit mothur to download the database, rerun the mothur command, then take a look at the database you have downloaded: summary.seqs(fasta=silva.bacteria/silva.bacteria.fasta, processors=8) Now do the alignment using align.seqs : align.seqs(fasta=stability.trim.contigs.good.unique.fasta, reference=silva.bacteria/silva.bacteria.fasta) We can then run summary.seqs again to get a summary of our alignment: summary.seqs(fasta=stability.trim.contigs.good.unique.align, count=stability.trim.contigs.good.count_table) You'll see that the bulk of the sequences start at position 13862 and end at position 23444. Some sequences start at position 13144 or 13876 and end at 22587 or 25294. These deviants from the mode positions are likely due to an insertion or deletion at the terminal ends of the aliignments. Sometimes you'll see sequences that start and end at the same position indicating a very poor alignment, which is generally due to non-specific amplification. To make sure that everything overlaps the same region we'll re-run screen.seqs to get sequences that start at or before position 1968 and end at or after position 11550. We'll also set the maximum homopolymer length to 8 since there's nothing in the database with a stretch of 9 or more of the same base in a row (this really could have been done in the first execution of screen.seqs above). Note that we need the count table so that we can update the table for the sequences we're removing and we're also using the summary file so we don't have to figure out again all the start and stop positions: screen.seqs(fasta=stability.trim.contigs.good.unique.align, count=stability.trim.contigs.good.count_table, summary=stability.trim.contigs.good.unique.summary, start=13862, end=23444, maxhomop=8) summary.seqs(fasta=current, count=current) No we can trim both ends of the aligned reads to be sure the all overlap exactly the same region. We can do this with fliter.seqs filter.seqs(fasta=stability.trim.contigs.good.unique.good.align, vertical=T, trump=.) We may have introduced redundancy by trimming the ends of the sequences, so we will re-run unique.seqs unique.seqs(fasta=stability.trim.contigs.good.unique.good.filter.fasta, count=stability.trim.contigs.good.good.count_table) This identified 3 duplicate sequences that we've now merged with previous unique sequences. The next thing we want to do to further de-noise our sequences is to pre-cluster the sequences using the pre.cluster command allowing for up to 2 differences between sequences. This command will split the sequences by group and then sort them by abundance and go from most abundant to least and identify sequences that are within 2 nt of each other. If they are then they get merged. We generally favor allowing 1 difference for every 100 bp of sequence: pre.cluster(fasta=stability.trim.contigs.good.unique.good.filter.unique.fasta, count=stability.trim.contigs.good.unique.good.filter.count_table, diffs=2) At this point we have removed as much sequencing error as we can and it is time to turn our attention to removing chimeras. We'll do this using the UCHIME algorithm that is called within mothur using the chimera.uchime command. Again, this command will split the data by sample and check for chimeras. Our preferred way of doing this is to use the abundant sequences as our reference. In addition, if a sequence is flagged as chimeric in one sample, the the default (dereplicate=F) is to remove it from all samples. Our experience suggests that this is a bit aggressive since we've seen rare sequences get flagged as chimeric when they're the most abundant sequence in another sample. This is how we do it: chimera.uchime(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.count_table, dereplicate=t) Running chimera.uchime with the count file will remove the chimeric sequences from the count file. But you still need to remove those sequences from the fasta file. We do this using remove.seqs : remove.seqs(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.fasta, accnos=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.accnos) As a final quality control step, we need to see if there are any \"undesirables\" in our dataset. Sometimes when we pick a primer set they will amplify other stuff that gets to this point in the pipeline such as 18S rRNA gene fragments or 16S rRNA from Archaea, chloroplasts, and mitochondira. There's also just the random stuff that we want to get rid of. Let's go ahead and classify those sequences using the Bayesian classifier with the classify.seqs command: classify.seqs(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.count_table, reference=silva.bacteria/silva.bacteria.fasta, taxonomy=silva.bacteria/silva.bacteria.rdp.tax, cutoff=80) Now that everything is classified we want to remove our undesirables. We do this with the remove.lineage command: remove.lineage(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.count_table, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.rdp.wang.taxonomy, taxon=Chloroplast-Mitochondria-unknown-Archaea-Eukaryota)","title":"Processing Improved Sequences"},{"location":"tutorials/docs/16S_mothur/#analysis","text":"","title":"Analysis"},{"location":"tutorials/docs/16S_mothur/#otus","text":"We will use cluster.split for clustering sequences into OTUs cluster.split(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.count_table, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.rdp.wang.pick.taxonomy, splitmethod=classify, taxlevel=4, cutoff=0.15) We used taxlevel=4 , which corresponds to the level of Order Next we want to know how many sequences are in each OTU from each group and we can do this using the make.shared command . Here we tell mothur that we're really only interested in the 0.03 cutoff level: make.shared(list=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.an.unique_list.list, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.count_table, label=0.03) We also want to know the taxonomy for each of our OTUs. We can get the consensus taxonomy for each OTU using the classify.otu command classify.otu(list=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.an.unique_list.list, count=stability.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.count_table, taxonomy=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.rdp.wang.pick.taxonomy, label=0.03) If you open the file stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.an.unique_list.0.03.cons.taxonomy , you can get information about your OTUs. OTU Size Taxonomy Otu0001 12328 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100); Otu0002 8918 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100); Otu0003 7850 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100); Otu0004 7478 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100); Otu0005 7478 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100); Otu0006 6650 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100); Otu0007 6341 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Bacteroidaceae(100);Bacteroides(100);Bacteroides_unclassified(100); Otu0008 5374 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Rikenellaceae(100);Alistipes(100);Alistipes_unclassified(100); Otu0009 3618 Bacteria(100);Bacteroidetes(100);Bacteroidia(100);Bacteroidales(100);Porphyromonadaceae(100);Barnesiella(100);Barnesiella_unclassified(100); This is telling you that Otu0001 was observed 12328 times in your sample and that 100% of the sequences were from Barnesiella In order to vizualise the composition of our datasets, we'll use phyloseq, a R package to work with microbiom data. Most of the phyloseq functionalities require aand a tree file. We need to generate it with mothur: dist.seqs(fasta=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta, output=lt, processors=8) clearcut(phylip=stability.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.phylip.dist)","title":"OTUs"},{"location":"tutorials/docs/16S_mothur/#batch-mode","text":"It is perfectly acceptable to enter the commands for your analysis from within mothur. We call this the interactive mode. If you are doing a lot these types of analysis or you want to use this SOP on your own data without thinking too much, you can run mothur in batch mode using ./mothur script.batch where script.batch (or whatever name you want, really) is a text file containing all the commands that you previously entered in interactive mode. If you have time, copy all the commands from this tutorial in a file, a try to make mothur work in batch mode!","title":"Batch Mode"},{"location":"tutorials/docs/16S_mothur/#phyloseq-analysis","text":"First, install and load the phyloseq package: source('http://bioconductor.org/biocLite.R') biocLite('phyloseq') library(\"phyloseq\") library(\"ggplot2\") library(\"plyr\") theme_set(theme_bw()) # set the ggplot theme The PhyloSeq package has an import_mothur function that you can use to import the files you generated with mothur. As an example, import the example mothur data provided by phyloseq as an example: mothlist <- system.file(\"extdata\", \"esophagus.fn.list.gz\", package=\"phyloseq\") mothgroup <- system.file(\"extdata\", \"esophagus.good.groups.gz\", package=\"phyloseq\") mothtree <- system.file(\"extdata\", \"esophagus.tree.gz\", package=\"phyloseq\") show_mothur_cutoffs(mothlist) cutoff <- '0.10' x <- import_mothur(mothlist, mothgroup, mothtree, cutoff) x Note: If if you ever work with 16s data and decide to use QIIME instead of mothur, phyloseq also has an import_qiime function. Also, newer version of qiime and mothur have the ability to produce a .biom file. \u201cThe biom file format (canonically pronounced \u2018biome\u2019) is designed to be a general-use format for representing counts of observations in one or more biological samples. BIOM is a recognized standard for the Earth Microbiome Project and is a Genomics Standards Consortium candidate project.\u201d More info on http://biom-format.org/ For the rest of this tutorial, we will work with an example dataset provided by the phyloseq package. Load the data with the following command: data(enterotype) data(\"GlobalPatterns\")","title":"PhyloSeq Analysis"},{"location":"tutorials/docs/16S_mothur/#ordination-and-distance-based-analysis","text":"Let's do some preliminary filtering. Remove the OTUs that included all unassigned sequences (\"-1\") enterotype <- subset_species(enterotype, Genus != \"-1\") The available distance methods coded in the phyloseq package: dist_methods <- unlist(distanceMethodList) print(dist_methods) ## UniFrac1 UniFrac2 DPCoA JSD vegdist1 ## \"unifrac\" \"wunifrac\" \"dpcoa\" \"jsd\" \"manhattan\" ## vegdist2 vegdist3 vegdist4 vegdist5 vegdist6 ## \"euclidean\" \"canberra\" \"bray\" \"kulczynski\" \"jaccard\" ## vegdist7 vegdist8 vegdist9 vegdist10 vegdist11 ## \"gower\" \"altGower\" \"morisita\" \"horn\" \"mountford\" ## vegdist12 vegdist13 vegdist14 vegdist15 betadiver1 ## \"raup\" \"binomial\" \"chao\" \"cao\" \"w\" ## betadiver2 betadiver3 betadiver4 betadiver5 betadiver6 ## \"-1\" \"c\" \"wb\" \"r\" \"I\" ## betadiver7 betadiver8 betadiver9 betadiver10 betadiver11 ## \"e\" \"t\" \"me\" \"j\" \"sor\" ## betadiver12 betadiver13 betadiver14 betadiver15 betadiver16 ## \"m\" \"-2\" \"co\" \"cc\" \"g\" ## betadiver17 betadiver18 betadiver19 betadiver20 betadiver21 ## \"-3\" \"l\" \"19\" \"hk\" \"rlb\" ## betadiver22 betadiver23 betadiver24 dist1 dist2 ## \"sim\" \"gl\" \"z\" \"maximum\" \"binary\" ## dist3 designdist ## \"minkowski\" \"ANY\" Remove the two distance-methods that require a tree, and the generic custom method that requires user-defined distance arguments. # These require tree dist_methods[(1:3)] # Remove them from the vector dist_methods <- dist_methods[-(1:3)] # This is the user-defined method: dist_methods[\"designdist\"] # Remove the user-defined distance dist_methods = dist_methods[-which(dist_methods==\"ANY\")] Loop through each distance method, save each plot to a list, called plist. plist <- vector(\"list\", length(dist_methods)) names(plist) = dist_methods for( i in dist_methods ){ # Calculate distance matrix iDist <- distance(enterotype, method=i) # Calculate ordination iMDS <- ordinate(enterotype, \"MDS\", distance=iDist) ## Make plot # Don't carry over previous plot (if error, p will be blank) p <- NULL # Create plot, store as temp variable, p p <- plot_ordination(enterotype, iMDS, color=\"SeqTech\", shape=\"Enterotype\") # Add title to each plot p <- p + ggtitle(paste(\"MDS using distance method \", i, sep=\"\")) # Save the graphic to file. plist[[i]] = p } Combine results and shade according to Sequencing technology: df = ldply(plist, function(x) x$data) names(df)[1] <- \"distance\" p = ggplot(df, aes(Axis.1, Axis.2, color=SeqTech, shape=Enterotype)) p = p + geom_point(size=3, alpha=0.5) p = p + facet_wrap(~distance, scales=\"free\") p = p + ggtitle(\"MDS on various distance metrics for Enterotype dataset\") p Print individual plots: print(plist[[\"jsd\"]]) print(plist[[\"jaccard\"]]) print(plist[[\"bray\"]]) print(plist[[\"euclidean\"]])","title":"Ordination and distance-based analysis"},{"location":"tutorials/docs/16S_mothur/#alpha-diversity-graphics","text":"Here is the default graphic produced by the plot_richness function on the GP example dataset: GP <- prune_species(speciesSums(GlobalPatterns) > 0, GlobalPatterns) plot_richness(GP) Note that in this case, the Fisher calculation results in a warning (but still plots). We can avoid this by specifying a measures argument to plot_richness, which will include just the alpha-diversity measures that we want. plot_richness(GP, measures=c(\"Chao1\", \"Shannon\")) We can specify a sample variable on which to group/organize samples along the horizontal (x) axis. An experimentally meaningful categorical variable is usually a good choice \u2013 in this case, the \"SampleType\" variable works much better than attempting to interpret the sample names directly (as in the previous plot): plot_richness(GP, x=\"SampleType\", measures=c(\"Chao1\", \"Shannon\")) Now suppose we wanted to use an external variable in the plot that isn\u2019t in the GP dataset already \u2013 for example, a logical that indicated whether or not the samples are human-associated. First, define this new variable, human, as a factor (other vectors could also work; or other data you might have describing the samples). sampleData(GP)$human <- getVariable(GP, \"SampleType\") %in% c(\"Feces\", \"Mock\", \"Skin\", \"Tongue\") Now tell plot_richness to map the new human variable on the horizontal axis, and shade the points in different color groups, according to which \"SampleType\" they belong. plot_richness(GP, x=\"human\", color=\"SampleType\", measures=c(\"Chao1\", \"Shannon\")) We can merge samples that are from the environment (SampleType), and make the points bigger with a ggplot2 layer. First, merge the samples. GPst = merge_samples(GP, \"SampleType\") # repair variables that were damaged during merge (coerced to numeric) sample_data(GPst)$SampleType <- factor(sample_names(GPst)) sample_data(GPst)$human <- as.logical(sample_data(GPst)$human) p = plot_richness(GPst, x=\"human\", color=\"SampleType\", measures=c(\"Chao1\", \"Shannon\")) p + geom_point(size=5, alpha=0.7)","title":"Alpha diversity graphics"},{"location":"tutorials/docs/16S_mothur/#trees","text":"head(phy_tree(GlobalPatterns)$node.label, 10) The node data from the GlobalPatterns dataset are strange. They look like they might be bootstrap values, but they sometimes have two decimals. phy_tree(GlobalPatterns)$node.label = substr(phy_tree(GlobalPatterns)$node.label, 1, 4) Additionally, the dataset has many OTUs, too many to fit them all on a tree. Let's take the 50 more abundant and plot a basic tree: physeq = prune_taxa(taxa_names(GlobalPatterns)[1:50], GlobalPatterns) plot_tree(physeq) dots are annotated next to tips (OTUs) in the tree, one for each sample in which that OTU was observed. Let's color the dots by taxonomic ranks, and sample covariates: plot_tree(physeq, nodelabf=nodeplotboot(), ladderize=\"left\", color=\"SampleType\") by taxonomic class: plot_tree(physeq, nodelabf=nodeplotboot(), ladderize=\"left\", color=\"Class\") It can be useful to label the tips: plot_tree(physeq, color=\"SampleType\", label.tips=\"Genus\") Making a radial tree is easy with ggplot2, simply recognizing that our vertically-oriented tree is a cartesian mapping of the data to a graphic \u2013 and that a radial tree is the same mapping, but with polar coordinates instead. plot_tree(physeq, nodelabf=nodeplotboot(60,60,3), color=\"SampleType\", shape=\"Class\", ladderize=\"left\") + coord_polar(theta=\"y\")","title":"Trees"},{"location":"tutorials/docs/16S_mothur/#bar-plots","text":"Bar plots are one of the easiest way to vizualize your data. But be careful, they can be misleading if grouping sample! Let's take a subset of the GlobalPatterns dataset, and produce a basic bar plot: gp.ch = subset_taxa(GlobalPatterns, Phylum == \"Chlamydiae\") plot_bar(gp.ch) The dataset is plotted with every sample mapped individually to the horizontal (x) axis, and abundance values mapped to the veritcal (y) axis. At each sample\u2019s horizontal position, the abundance values for each OTU are stacked in order from greatest to least, separate by a thin horizontal line. As long as the parameters you choose to separate the data result in more than one OTU abundance value at the respective position in the plot, the values will be stacked in order as a means of displaying both the sum total value while still representing the individual OTU abundances. The bar plot will be clearer with color to represent the Genus to which each OTU belongs. plot_bar(gp.ch, fill=\"Genus\") Now keep the same fill color, and group the samples together by the SampleType variable; essentially, the environment from which the sample was taken and sequenced. plot_bar(gp.ch, x=\"SampleType\", fill=\"Genus\") A more complex example using facets: plot_bar(gp.ch, \"Family\", fill=\"Genus\", facet_grid=~SampleType)","title":"Bar plots"},{"location":"tutorials/docs/16S_mothur/#heatmaps","text":"The following two lines subset the dataset to just the top 300 most abundant Bacteria taxa across all samples (in this case, with no prior preprocessing. Not recommended, but quick). data(\"GlobalPatterns\") gpt <- subset_taxa(GlobalPatterns, Kingdom==\"Bacteria\") gpt <- prune_taxa(names(sort(taxa_sums(gpt),TRUE)[1:300]), gpt) plot_heatmap(gpt, sample.label=\"SampleType\") subset a smaller dataset based on an Archaeal phylum gpac <- subset_taxa(GlobalPatterns, Phylum==\"Crenarchaeota\") plot_heatmap(gpac)","title":"Heatmaps"},{"location":"tutorials/docs/16S_mothur/#plot-microbiome-network","text":"There is a random aspect to some of the network layout methods. For complete reproducibility of the images produced later in this tutorial, it is possible to set the random number generator seed explicitly: set.seed(711L) Because we want to use the enterotype designations as a plot feature in these plots, we need to remove the 9 samples for which no enterotype designation was assigned (this will save us the hassle of some pesky warning messages, but everything still works; the offending samples are anyway omitted). enterotype = subset_samples(enterotype, !is.na(Enterotype)) Create an igraph-based network based on the default distance method, \u201cJaccard\u201d, and a maximum distance between connected nodes of 0.3. ig <- make_network(enterotype, max.dist=0.3) plot_network(ig, enterotype) The previous graphic displayed some interesting structure, with one or two major subgraphs comprising a majority of samples. Furthermore, there seemed to be a correlation in the sample naming scheme and position within the network. Instead of trying to read all of the sample names to understand the pattern, let\u2019s map some of the sample variables onto this graphic as color and shape: plot_network(ig, enterotype, color=\"SeqTech\", shape=\"Enterotype\", line_weight=0.4, label=NULL) In the previous examples, the choice of maximum-distance and distance method were informed, but arbitrary. Let\u2019s see what happens when the maximum distance is lowered, decreasing the number of edges in the network ig <- make_network(enterotype, max.dist=0.2) plot_network(ig, enterotype, color=\"SeqTech\", shape=\"Enterotype\", line_weight=0.4, label=NULL) Let\u2019s repeat the previous exercise, but replace the Jaccard (default) distance method with Bray-Curtis ig <- make_network(enterotype, dist.fun=\"bray\", max.dist=0.3) plot_network(ig, enterotype, color=\"SeqTech\", shape=\"Enterotype\", line_weight=0.4, label=NULL)","title":"Plot microbiome network"},{"location":"tutorials/docs/annotation/","text":"Genome Annotation After you have de novo assembled your genome sequencing reads into contigs, it is useful to know what genomic features are on those contigs. The process of identifying and labelling those features is called genome annotation. Prokka is a \u201cwrapper\u201d; it collects together several pieces of software (from various authors), and so avoids \u201cre-inventing the wheel\u201d. Prokka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using Prodigal ; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found here . Input data Prokka requires assembled contigs. You will need your best assembly from the assembly tutorial. Alternatively, you can download an assembly here Running prokka module load prokka awk '/^>/{print \">ctg\" ++i; next}{print}' < assembly.fasta > good_contigs.fasta prokka --outdir annotation --kingdom Bacteria \\ --proteins m_genitalium.faa good_contigs.fasta Once Prokka has finished, examine each of its output files. The GFF and GBK files contain all of the information about the features annotated (in different formats.) The .txt file contains a summary of the number of features annotated. The .faa file contains the protein sequences of the genes annotated. The .ffn file contains the nucleotide sequences of the genes annotated. Visualising the annotation Artemis is a graphical Java program to browse annotated genomes. Download it here and install it on your local computer. Copy the .gff file produced by prokka on your computer, and open it with artemis. You will be overwhelmed and/or confused at first, and possibly permanently. Here are some tips: There are 3 panels: feature map (top), sequence (middle), feature list (bottom) Click right-mouse-button on bottom panel and select Show products Zooming is done via the verrtical scroll bars in the two top panels","title":"Genome Annotation"},{"location":"tutorials/docs/annotation/#genome-annotation","text":"After you have de novo assembled your genome sequencing reads into contigs, it is useful to know what genomic features are on those contigs. The process of identifying and labelling those features is called genome annotation. Prokka is a \u201cwrapper\u201d; it collects together several pieces of software (from various authors), and so avoids \u201cre-inventing the wheel\u201d. Prokka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using Prodigal ; second, the function of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found here .","title":"Genome Annotation"},{"location":"tutorials/docs/annotation/#input-data","text":"Prokka requires assembled contigs. You will need your best assembly from the assembly tutorial. Alternatively, you can download an assembly here","title":"Input data"},{"location":"tutorials/docs/annotation/#running-prokka","text":"module load prokka awk '/^>/{print \">ctg\" ++i; next}{print}' < assembly.fasta > good_contigs.fasta prokka --outdir annotation --kingdom Bacteria \\ --proteins m_genitalium.faa good_contigs.fasta Once Prokka has finished, examine each of its output files. The GFF and GBK files contain all of the information about the features annotated (in different formats.) The .txt file contains a summary of the number of features annotated. The .faa file contains the protein sequences of the genes annotated. The .ffn file contains the nucleotide sequences of the genes annotated.","title":"Running prokka"},{"location":"tutorials/docs/annotation/#visualising-the-annotation","text":"Artemis is a graphical Java program to browse annotated genomes. Download it here and install it on your local computer. Copy the .gff file produced by prokka on your computer, and open it with artemis. You will be overwhelmed and/or confused at first, and possibly permanently. Here are some tips: There are 3 panels: feature map (top), sequence (middle), feature list (bottom) Click right-mouse-button on bottom panel and select Show products Zooming is done via the verrtical scroll bars in the two top panels","title":"Visualising the annotation"},{"location":"tutorials/docs/assembly/","text":"De-novo Genome Assembly Lecture Practical In this practical we will perform the assembly of M. genitalium , a bacterium published in 1995 by Fraser et al in Science ( abstract link ). Getting the data M. genitalium was sequenced using the MiSeq platform (2 * 150bp). The reads were deposited in the ENA Short Read Archive under the accession ERR486840 Download the 2 fastq files associated with the run. wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_1.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_2.fastq.gz The files that were deposited in ENA were already trimmed, so we do not have to trim ourselves! Question How many reads are in the files? De-novo assembly We will be using the MEGAHIT assembler to assemble our bacterium megahit -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz -o m_genitalium This will take a few minutes. The result of the assembly is in the directory m_genitalium under the name final.contigs.fa Let's make a copy of it cp m_genitalium/final.contigs.fa m_genitalium.fasta and look at it head m_genitalium.fasta Quality of the Assembly QUAST is a software evaluating the quality of genome assemblies by computing various metrics, including Run Quast on your assembly quast.py m_genitalium.fasta -o m_genitalium_report and take a look at the text report cat m_genitalium_report/report.txt You should see something like All statistics are based on contigs of size >= 500 bp, unless otherwise noted (e.g., \"# contigs (>= 0 bp)\" and \"Total length (>= 0 bp)\" include all contigs). Assembly m_genitalium # contigs (>= 0 bp) 17 # contigs (>= 1000 bp) 8 # contigs (>= 5000 bp) 7 # contigs (>= 10000 bp) 6 # contigs (>= 25000 bp) 5 # contigs (>= 50000 bp) 2 Total length (>= 0 bp) 584267 Total length (>= 1000 bp) 580160 Total length (>= 5000 bp) 577000 Total length (>= 10000 bp) 570240 Total length (>= 25000 bp) 554043 Total length (>= 50000 bp) 446481 # contigs 11 Largest contig 368542 Total length 582257 GC (%) 31.71 N50 368542 N75 77939 L50 1 L75 2 # N's per 100 kbp 0.00 which is a summary stats about our assembly. Additionally, the file m_genitalium_report/report.html You can either download it and open it in your own web browser, or we make it available for your convenience: m_genitalium_report/report.html Note N50: length for which the collection of all contigs of that length or longer covers at least 50% of assembly length Question How well does the assembly total consensus size and coverage correspond to your earlier estimation? Question How many contigs in total did the assembly produce? Question What is the N50 of the assembly? What does this mean? Fixing misassemblies Pilon is a software tool which can be used to automatically improve draft assemblies. It attempts to make improvements to the input genome, including: Single base differences Small Indels Larger Indels or block substitution events Gap filling Identification of local misassemblies, including optional opening of new gaps Pilon then outputs a FASTA file containing an improved representation of the genome from the read data and an optional VCF file detailing variation seen between the read data and the input genome. Before running Pilon itself, we have to align our reads against the assembly bowtie2-build m_genitalium.fasta m_genitalium bowtie2 -x m_genitalium -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz | \\ samtools view -bS -o m_genitalium.bam samtools sort m_genitalium.bam -o m_genitalium.sorted.bam samtools index m_genitalium.sorted.bam then we run Pilon pilon --genome m_genitalium.fasta --frags m_genitalium.sorted.bam --output m_genitalium_improved which will correct eventual mismatches in our assembly and write the new improved assembly to m_genitalium_improved.fasta Assembly Completeness Although quast output a range of metric to assess how contiguous our assembly is, having a long N50 does not guarantee a good assembly: it could be riddled by misassemblies! We will run busco to try to find marker genes in our assembly. Marker genes are conserved across a range of species and finding intact conserved genes in our assembly would be a good indication of its quality First we need to download and unpack the bacterial datasets used by busco wget http://busco.ezlab.org/datasets/bacteria_odb9.tar.gz tar xzf bacteria_odb9.tar.gz then we can run busco with BUSCO.py -i m_genitalium.fasta -l bacteria_odb9 -o busco_genitalium -m genome Question How many marker genes has busco found? Course literature Course litteraturer for today is: Next-Generation Sequence Assembly: Four Stages of Data Processing and Computational Challenges: https://doi.org/10.1371/journal.pcbi.1003345","title":"De-novo Genome Assembly"},{"location":"tutorials/docs/assembly/#de-novo-genome-assembly","text":"","title":"De-novo Genome Assembly"},{"location":"tutorials/docs/assembly/#lecture","text":"","title":"Lecture"},{"location":"tutorials/docs/assembly/#practical","text":"In this practical we will perform the assembly of M. genitalium , a bacterium published in 1995 by Fraser et al in Science ( abstract link ).","title":"Practical"},{"location":"tutorials/docs/assembly/#getting-the-data","text":"M. genitalium was sequenced using the MiSeq platform (2 * 150bp). The reads were deposited in the ENA Short Read Archive under the accession ERR486840 Download the 2 fastq files associated with the run. wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_1.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486840/ERR486840_2.fastq.gz The files that were deposited in ENA were already trimmed, so we do not have to trim ourselves! Question How many reads are in the files?","title":"Getting the data"},{"location":"tutorials/docs/assembly/#de-novo-assembly","text":"We will be using the MEGAHIT assembler to assemble our bacterium megahit -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz -o m_genitalium This will take a few minutes. The result of the assembly is in the directory m_genitalium under the name final.contigs.fa Let's make a copy of it cp m_genitalium/final.contigs.fa m_genitalium.fasta and look at it head m_genitalium.fasta","title":"De-novo assembly"},{"location":"tutorials/docs/assembly/#quality-of-the-assembly","text":"QUAST is a software evaluating the quality of genome assemblies by computing various metrics, including Run Quast on your assembly quast.py m_genitalium.fasta -o m_genitalium_report and take a look at the text report cat m_genitalium_report/report.txt You should see something like All statistics are based on contigs of size >= 500 bp, unless otherwise noted (e.g., \"# contigs (>= 0 bp)\" and \"Total length (>= 0 bp)\" include all contigs). Assembly m_genitalium # contigs (>= 0 bp) 17 # contigs (>= 1000 bp) 8 # contigs (>= 5000 bp) 7 # contigs (>= 10000 bp) 6 # contigs (>= 25000 bp) 5 # contigs (>= 50000 bp) 2 Total length (>= 0 bp) 584267 Total length (>= 1000 bp) 580160 Total length (>= 5000 bp) 577000 Total length (>= 10000 bp) 570240 Total length (>= 25000 bp) 554043 Total length (>= 50000 bp) 446481 # contigs 11 Largest contig 368542 Total length 582257 GC (%) 31.71 N50 368542 N75 77939 L50 1 L75 2 # N's per 100 kbp 0.00 which is a summary stats about our assembly. Additionally, the file m_genitalium_report/report.html You can either download it and open it in your own web browser, or we make it available for your convenience: m_genitalium_report/report.html Note N50: length for which the collection of all contigs of that length or longer covers at least 50% of assembly length Question How well does the assembly total consensus size and coverage correspond to your earlier estimation? Question How many contigs in total did the assembly produce? Question What is the N50 of the assembly? What does this mean?","title":"Quality of the Assembly"},{"location":"tutorials/docs/assembly/#fixing-misassemblies","text":"Pilon is a software tool which can be used to automatically improve draft assemblies. It attempts to make improvements to the input genome, including: Single base differences Small Indels Larger Indels or block substitution events Gap filling Identification of local misassemblies, including optional opening of new gaps Pilon then outputs a FASTA file containing an improved representation of the genome from the read data and an optional VCF file detailing variation seen between the read data and the input genome. Before running Pilon itself, we have to align our reads against the assembly bowtie2-build m_genitalium.fasta m_genitalium bowtie2 -x m_genitalium -1 ERR486840_1.fastq.gz -2 ERR486840_2.fastq.gz | \\ samtools view -bS -o m_genitalium.bam samtools sort m_genitalium.bam -o m_genitalium.sorted.bam samtools index m_genitalium.sorted.bam then we run Pilon pilon --genome m_genitalium.fasta --frags m_genitalium.sorted.bam --output m_genitalium_improved which will correct eventual mismatches in our assembly and write the new improved assembly to m_genitalium_improved.fasta","title":"Fixing misassemblies"},{"location":"tutorials/docs/assembly/#assembly-completeness","text":"Although quast output a range of metric to assess how contiguous our assembly is, having a long N50 does not guarantee a good assembly: it could be riddled by misassemblies! We will run busco to try to find marker genes in our assembly. Marker genes are conserved across a range of species and finding intact conserved genes in our assembly would be a good indication of its quality First we need to download and unpack the bacterial datasets used by busco wget http://busco.ezlab.org/datasets/bacteria_odb9.tar.gz tar xzf bacteria_odb9.tar.gz then we can run busco with BUSCO.py -i m_genitalium.fasta -l bacteria_odb9 -o busco_genitalium -m genome Question How many marker genes has busco found?","title":"Assembly Completeness"},{"location":"tutorials/docs/assembly/#course-literature","text":"Course litteraturer for today is: Next-Generation Sequence Assembly: Four Stages of Data Processing and Computational Challenges: https://doi.org/10.1371/journal.pcbi.1003345","title":"Course literature"},{"location":"tutorials/docs/command_line/","text":"The command-line Warning This lesson has been deprecated. Please refer to http://swcarpentry.github.io/shell-novice/ for a better, up-to-date lesson This tutorial is largely inspired of the Introduction to UNIX course from the Sanger Institute. The aim of this module is to introduce Unix and cover some of the basics that will allow you to be more comfortable with the command-line. Several of the programs that you are going to use during this course are useful for bioinformatics analyses. This module is only designed to provide a very brief introduction to some of the features and useful commands of Unix. During this module we will also obtain a genome sequence and examine the basic structure of an EMBL entry. Introduction Unix is the standard operating system on most large computer systems in scientific research, in the same way that Microsoft Windows is the dominant operating system on desktop PCs. Unix and MS Windows both perform the important job of managing the computer\u2019s hardware (screen, keyboard, mouse, hard disks, network connections, etc...) on your behalf. They also provide you with tools to manage your files and to run application software. They both offer a graphical user interface (desktop). The desktops look different, call things by different names but they mostly can do the same things. Unix is a powerful, secure, robust and stable operating system that allows dozens of people to run programs on the same computer at the same time. This is why it is the preferred operating system for large-scale scientific computing. It is run on all kind of machines, like mobile phones (Android), desktop PCs, kitchen appliances,... all the way up to supercomputers. Unix powers the majority of the Internet. Aims The aim of this course is to introduce Unix and cover the basics. The programs that you are going to use during the courses, plus many others that are useful for bioinformatics analyses, are run in Unix. This module is only designed to provide a very brief introduction to some of the features and useful commands of Unix. During this module we will also obtain a genome sequence and examine the basic structure of an EMBL entry. Why use Unix? Unix is a well established, very widespread operating system. You probably have a device running on Unix in your home without realising it (e.g. playstation, TV box, wireless router, android tablets/phones,... Command line driven, with a huge number of often terse, but powerful commands. In contrast to Windows, it is designed to allow many users to run their programs simultaneously on the same computer. Designed to work in computer networks - for example, most of the Internet is Unix based. It is used on many of the powerful computers at bioinformatics centres and also on many desktops and laptops (MacOS is largely UNIX compatible). The major difference between Unix and Windows is that it is free (as in freedom) and you can modify it to work however you want. This same principle of freedom is also used in most bioinformatics software. There are many distributions of Unix such as Ubuntu, RedHat, Fedora, Mint,...). These are all Unix, but they bundle up extra software in a different way or combinations. Some are known for being conservative and reliable; whilst others are know for being cutting-edge (and less reliable). The MacOSX operating system used by the eBioKit is also based on Unix. Getting started For this course, you will have to connect to the eBiokit using SSH. SSH stands for Secure Shell and is a network protocol used to securely connect to a server. To do so, you will need an SSH client: On Linux: it is included by default, named Terminal. On MacOS: it is included by default, also named Terminal. On Windows: you'll have to download and install MobaXterm , a terminal emulator. Once you've opened your terminal (or terminal emulator), type ssh username@ip_address replacing username and ip_address with your username and the ip address of the server you are connecting to. Type your password when prompted. As you type, nothing will show on screen. No stars, no dots. It is supposed to be that way. Just type the password and press enter! You can type commands directly into the terminal at the \u2018$' prompt. A list of useful commands can be found on the next page. Many of them are two- or three-letter abbreviations. The earliest Unix systems ( circa 1970) only had slow Teletype terminals, so it was faster to type 'rm' to remove a file than 'delete' or 'erase'. This terseness is a feature of Unix that still survives. The command line All Unix programs may be run by typing commands at the Unix prompt. The command line tells the computer what to do. You may subtly alter these commands by specifying certain options when typing in the command line. Command line Arguments Typing any Unix command for example ls , mv or cd at the Unix prompt with the appropriate variables such as files names or directories will result in the tasks being performed on pressing the enter key. The command is separated from the options and arguments by a space. Additional options and/or arguments can be added to the commands to affect the way the command works. Options usually have one dash and a letter (e.g. -h) or two dashes and a word (--help) with no space between the dash and the letter/word. Arguments are usually filenames or directories. For example: List the contents of a directory ls List the contents of a directoryList the contents of a directory with extra information about the files ls \u2013l List the contents of a directory with extra information about the files ls \u2013a List all contents including hidden files & directories ls -al List all contents including hidden files & directories, with extra information about the files ls \u2013l /usr/ List the contents of the directory /usr/, with extra information about the files Files and Directories Directories are the Unix equivalent of folders on a PC or Mac. They are organised in a hierarchy, so directories can have sub-directories. Directories are very useful for organising your work and keeping your account tidy - for example, if you have more than one project, you can organise the files for each project into different directories to keep them separate. You can think of directories as rooms in a house. You can only be in one room (directory) at a time. When you are in a room you can see everything in that room easily. To see things in other rooms, you have to go to the appropriate door and crane your head around. Unix works in a similar manner, moving from directory to directory to access files. The location or directory that you are in is referred to as the current working directory. Directory structure example Therefore if there is a file called genome.seq in the dna directory its location or full pathname can be expressed as /nfs/dna/genome.seq. General Points Unix is pretty straightforward, but there are some general points to remember that will make your life easier: most flavors of UNIX are case sensitive - typing ls is generally not the same as typing LS . You need to put a space between a command and its argument - for example, less my_file will show you the contents of the file called my_file; lessmyfile will just give you an error! Unix is not psychic: If you misspell the name of a command or the name of a file, it will not understand you. Many of the commands are only a few letters long; this can be confusing until you start to think logically about why those letters were chosen - ls for list, rm for remove and so on. Often when you have problems with Unix, it is due to a spelling mistake, or perhaps you have omitted a space. If you want to know more about Unix and its commands there are plenty of resources available that provide a more comprehensive guide (including a cheat sheet at the end of this chapter. http://unix.t-a-y-l-o-r.com/ In what follows, we shall use the following typographical conventions: Characters written in bold typewriter font are commands to be typed into the computer as they stand. Characters written in italic typewriter font indicate non-specific file or directory names. Words inserted within square brackets [Ctrl] indicate keys to be pressed. So, for example, $ **ls** *any_directory* [Enter] means \"at the Unix prompt $, type ls followed by the name of some directory, then press Enter\" Don't forget to press the [Enter] key: commands are not sent to the computer until this is done. Some useful Unix commands Command and What it does Command What it does ls Lists the contents of the current directory mkdir Creates a new directory mv Moves or renames a file cp Copies a file rm Removes a file cat Concatenates files less Displays the contents of a file one page at a time head Displays the first ten lines of a file tail Displays the last ten lines of a file cd Changes current working directory pwd Prints working directory find Finds files matching an expression grep Searches a file for patterns wc Counts the lines, words, characters, and bytes in a file kill Stops a process jobs Lists the processes that are running Firts steps The following exercise introduces a few useful Unix commands and provides examples of how they can be used. Many people panic when they are confronted with an Unix prompt! Don\u2019t! The exercise is designed to be step-by-step, so all the commands you need are provided in the text. If you get lost ask a demonstrator. If you are a person skilled at Unix, be patient it is only a short exercise. Finding where you are and what you\u2019ve got pwd Print the working directory As seen previously directories are arranged in a hierarchical structure. To determine where you are in the hierarchy you can use the pwd command to display the name of the current working directory. The current working directory may be thought of as the directory you are in, i.e. your current position in the file-system tree To find out where you are type pwd [enter] You will see that you are in your home directory. We need to move into the ngs_course_data directory. Remember, Unix is case sensitive PWD is not the same as pwd cd Change current working directory The cd command will change the current working directory to another, in other words allow you to move up or down in the directory hierarchy. First of all we are going to move into the \"ngs_course_data\" directory below. To do this type: cd ngs_course_data [enter] Now use the pwd command to check your location in the directory hierarchy. Change again the directory to Module_Unix ls List the contents of a directory To find out what are the contents of the current directory type ls [enter] The ls command lists the contents of your current directory, this includes files and directories You should see that there are several other directories. Now use the cd command again to change to the Module_Unix directory. Changing and moving what you\u2019ve got cp Copy a file. cp file1 file2 is the command which makes a copy of file1 in the current working directory and calls it file2! What you are going to do is make a copy of AL513382.embl. This file contains the genome of Salmonella typhi strain CT18 in EMBL format (we'll learn more about file formats later during the course). The new file will be called S_typhi.embl. cp AL513382.embl S_typhi.embl [enter] If you use the ls command to check the contents of the current directory you will see that there is an extra file called S_typhi.embl. rm Delete a file. This command removes a file permanently, so be careful! You are now going to remove the old version of the S. typhi genome file, AL513382.embl rm AL513382.embl [enter] The file will be removed. Use the ls command to check the contents of the current directory to see that AL513382.embl has been removed. Unix, as a general rule does exactly what you ask, and does not ask for confirmation. Unfortunately there is no \"recycle bin\" on the command line to recover the file from, so you have to be careful. cd Change current working directory. As before the cd command will change the current working directory to another, in other words allow you to move up or down in the directory hierarchy. First of all we are going to move into the directory above, type: cd .. [enter] Now use the pwd command to check your location in the directory hierarchy. Next, we are going to move into the Module_Artemis directory. To change to the Module_Artemis directory type: cd Module_Artemis [enter] use the ls command to check the contents of the directory. Tips There are some short cuts for referring to directories: . Current directory (one full stop) .. Directory above (two full stops) ~ Home directory (tilde) / Root of the file system (like C:\\ in Windows) Pressing the tab key twice will try and autocomplete what you\u2019ve started typing or give you a list of all possible completions. This saves a lot of typing and typos. Pressing the up/down arrows will let you scroll through the previous commands. If you highlight some text, middle clicking will paste it on the command line. mv Move a file. To move a file from one place to another use the mv command. This moves the file rather than copies it, therefore you end up with only one file rather than two. When using the command the path or pathname is used to tell Unix where to find the file. You refer to files in other directories by using the list of hierarchical names separated by slashes. For example, the file bases in the directory genome has the path genome/bases If no path is specified Unix assumes that the file is in the current working directory. What you are going to do is move the file S_typhi.embl from the Module_Unix directory, to the current working directory. mv ../Module_Unix/S_typhi.embl . [enter] Use the ls command to check the contents of the current directory to see that S_typhi.embl has been moved. ../Module_Unix/S_typhi.embl specifies that S_typhi.embl is in the Module_Unix directory. If the file was in the directory above, the path would change to: ../ S_typhi.embl The command can also be used to rename a file in the current working directory. Previously we used the cp command, but mv provides an alternative without the need to delete the original file. Therefore we could have used: mv AL513382.embl S_typhi.embl [enter] instead of: cp AL513382.embl S_typhi.embl [enter] rm AL513382.embl [enter] Viewing what you\u2019ve got less Display file contents. This command displays the contents of a specified file one screen at a time. You are now going to look at the contents of S_typhi.embl. less S_typhi.embl [enter] The contents of S_typhi.embl will be displayed one screen at a time, to view the next screen press the space bar. less can also scroll backwards if you hit the b key. Another useful feature is the slash key, /, to search for a word in the file. You type the word you are looking for and press enter. The screen will jump to the next occurrence and highlight it. As S_typhi.embl is a large file this will take a while, therefore you may want to escape or exit from this command. To exit press the letter \u2018q\u2019. If you really need to exit from a program and it isn\u2019t responding press \u2018control\u2019 and the letter \u2018c\u2019 at the same time. head Display the first ten lines of a file tail Display the last ten lines of a file Sometimes you may just want to view the text at the beginning or the end of a file, without having to display all of the file. The head and tail commands can be used to do this. You are now going to look at the beginning of S_typhi.embl. head S_typhi.embl [enter] To look at the end of S_typhi.embl type: tail S_typhi.embl [enter] The number of lines that are displayed can be increased by adding extra arguments. To increase the number of lines viewed from 10 to 100 add the \u2013100 argument to the command. For example to view the last 100 lines of S_typhi.embl type: tail -100 S_typhi.embl [enter] Do this for both head and tail commands. What type of information is at the beginning and end of the EMBL format file? cat Join files together. Having looked at the beginning and end of the S_typhi.embl file you should notice that in EMBL format files the annotation comes first, then the DNA sequence at the end. If you had two separate files containing the annotation and the DNA sequence, both in EMBL format, it is possible to concatenate or join the two together to make a single file like the S_typhi.embl file you have just looked at. The Unix command cat can be used to join two or more files into a single file. The order in which the files are joined is determined by the order in which they appear in the command line. For example, we have two separate files, MAL13P1.dna and MAL13P1.tab, that contain the DNA and annotation, respectively, from the P. falciparum genome. Return to the Module_Unix directory using the cd command: cd ../Module_Unix [enter] and type cat MAL13P1.tab MAL13P1.dna > MAL13P1.embl [enter] MAL13P1.tab and MAL13P1.dna will be joined together and written to a file called MAL13P1.embl The > symbol in the command line directs the output of the cat program to the designated file MAL13P1.embl wc Counts the lines, words or characters of files. By typing the command line: ls | wc -l [enter] The above command uses wc to count the number of files that are listed by ls. The \u2018-l\u2019 option tells wc to return a count of the number of lines. The | symbol (known as the \u2018pipe\u2019 character) in the command line connects the two commands into a single operation for simplicity. You can connect as many commands as you want: ls | grep \".embl\" | wc -l This command will list out all of the files in the current directory, then send the results to the grep command which searches for all filenames containing the \u2018embl\u2019, then sends the results to wc which counts the number of lines (which corresponds to the number of files). grep Searches a file for patterns. grep is a powerful tool to search for patterns in a file. In the examples below, we are going to use the file called Malaria.fasta that contains the set of P. falciparum chromosomes in FASTA format. A FASTA file has the following format: Sequence Header CTAAACCTAAACCTAAACCCTGAACCCTAA... Therefore if we want to get the sequence headers, we can extract the lines that match the \u2018>\u2019 symbol: grep \u2018>\u2019 Malaria.fasta [enter] By typing the command line: grep -B 1 -A 1 'aagtagggttca' Malaria.fasta [enter] This command will search for a nucliotide sequence and print 1 line before and after any match. It won\u2019t find the pattern if it spans more than 1 line. find Finds files matching an expression. The find command is similar to ls but in many ways it is more powerful. It can be used to recursively search the directory tree for a specified path name, seeking files that match a given Boolean expression (a test which returns true or false) find . -name \u201c*.embl\u201d This command will return the files which name has the .embl suffix. mkdir test_directory find . -type d This command will return all the subdirectories contained in the current directory. These are just two basic examples but it is possible to search in many other ways: -mtime search files by modifying date -atime search files by last access date -size search files by file size -user search files by user they belong to. Tips You need to be careful with quoting when using wildcards! The wildcard * symbol represents a string of any character and of any length. For more information on Unix command see EMBNet UNIX Quick Guide. End of the module # Introduction to Unix (continued) In this part of the Unix tutorial, you will learn to download files, compress and decompress them, and combine commands. ## Download files `wget` can be used to download files from the internet and store them. `wget https://raw.githubusercontent.com/HadrienG/tutorials/master/LICENSE` will download the file that is located at the above URL on the internet, and put it **in the current directory**. This is the license under which this course is released. Open it and read it if you like! The `-O` option can be used to change the output file name. `wget -O GNU_FDL.txt https://raw.githubusercontent.com/HadrienG/tutorials/master/LICENSE` You can also use wget to download a file list using -i option and giving a text file containing file URLs. The following \u0002wzxhzdk:2\u0003 `wget -i download-file-list.txt` ## Compressing and decompressing files ### Compressing files with gzip gzip is a utility for compressing and decompressing individual files. To compress files, use: `gzip filename` The filename will be deleted and replaced by a compressed file called filename.gz. To reverse the compression process, use: `gzip -d filename.gz` Try it on the License you just downloaded! ### Tar archives Quite often, you don't want to compress just one file, but rather a bunch of them, or a directory. tar backs up entire directories and files as an archive. An archive is a file that contains other files plus information about them, such as their filename, owner, timestamps, and access permissions. tar does not perform any compression by default. To create a gzipped disk file tar archive, use `tar -czvf archivename filenames` where archivename will usually have a .tar.gz extension The c option means create, the v option means verbose (output filenames as they are archived), option f means file, and z means that the tar archive should be gzip compressed. To list the contents of a gzipped tar archive, use `tar -tzvf archivename` To unpack files from a tar archive, use `tar -xzvf archivename` Try to archive the folder `Module_Unix` from the previous exercise! You will notice a file called tutorials.tar.bz2 in your home directory. This is also a compressed archive, but compressed in the bzip format. Read the tar manual and find a way to decompress it. Hint: you can read the manual for any command using `man` `man tar` ### Redirection Some commands give you an output to your screen, but you would have preferred it to go into another program or into a file. For those cases you have some redirection characters. #### Output redirection The output from a command normally intended for standard output (that is, your screen) can be easily diverted to a file instead. This capability is known as output redirection: If the notation `> file` is appended to any command that normally writes its output to standard output, the output of that command will be written to file instead of your terminal. I.e, the following who command: `who > users.txt` No output appears at the terminal. This is because the output has been redirected into the specified file. `less users.txt` Be careful, if a command has its output redirected to a file and the file already contains some data, that data will be lost. Consider this example: `echo Hello > users.txt` `less users.txt` You can use the `>>` operator to append the output in an existing file as follows: \u0002wzxhzdk:3\u0003 `less users.txt` #### Piping You can connect two commands together so that the output from one program becomes the input of the next program. Two or more commands connected in this way form a pipe. To make a pipe, put a vertical bar `|` on the command line between two commands. Remember the command `grep`? We can pipe other commands to it, to refine searches per example: `ls -l ngs_course_data | grep \"Jan\"` will only give you the files and directories created in January. Tip: There are various options you can use with the grep command, look at the manual! Pipes are extremely useful to connect various bioinformatics software together. We'll use them extensively later. # Introduction to Unix (continued) In this part of the tutorial, we'll learn how to install programs in a Unix system ## Using a package manager This is the most straight-forward way, and the way used by most of the people using unix at home, or administrating their own machine. This course is aimed at giving you a working knowledge of linux for bioinformatics, and in that setting, you will rarely, if ever, be the administrator of your own machine. The methods below are here as an information ### On Ubuntu and Debian: Apt To install a software: `apt-get install name_of_the_software` to uninstall: `apt-get remove name_of_the_software` to update all installed softwares: \u0002wzxhzdk:4\u0003 ### On Fedora, CentOS and RedHat: yum To install a software: `yum install name_of_the_software` to uninstall: `yum remove name_of_the_software` to update: `yum update` ### MacOS: brew Although there are no official package managers on MacOS, two popular, community-driven alternatives exist: macports and brew. Brew is particularly pupular within the bioinformatics community, and allows easy installation of many bioinformatics softwares on MacOS To install brew on your mac: `/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"` To install a software: `brew install name_of_the_software` To uninstall: `brew uninstall name_of_the_software` To update all brew-installed softwares: \u0002wzxhzdk:5\u0003 More info on [brew.sh](http://brew.sh) and [brew.sh/homebrew-science/](http://brew.sh/homebrew-science/) ## Downloading binaries In a university setting, you will rarely by administrator of your own machine. This is a very good thing for one reason: it's harder for you to break something! The downside is that it makes installing softwares more complicated. We'll start wit simply downloading the software and executing it, then we'll learn how to obtain packages from source code. for example, we'll install the blast binaries: First, download the archive: `wget ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.6.0+-x64-linux.tar.gz` then unpack it and go to the newly created directory \u0002wzxhzdk:6\u0003 you should have a `bin` directory, go inside and look at the files. You have a bunch of executable files. ### Execute a file Most of the lunix commands that you execute on a regular basis (ls, cp, mkdir) are located in `/usr/bin`, but you don't have to invoke them with their full path: i.e. you dont type `/usr/bin/ls` but just `ls`. This is because `/usr/bin/ls` is in your $PATH. to execute a file that you just downloaded, and is therefore not in your path, you have to type the absolute or relative path to that file. Meaning, for the blast program suite that we just downloaded: `bin/blastn -help` or \u0002wzxhzdk:7\u0003 and that's it! But it is not very convenient. You want to be able to execute blast without having to remember where it is. If you have administrator rights (sudo), you can move the software in `/usr/bin`. If you don't you can modify your $PATH in a configuration file called `.bash_profile` that is located in your home. More information on how to correctly modify your PATH [here](http://unix.stackexchange.com/a/26059) ## Compiling from source Sometimes pre-compiled binaries are not available. You then have to compile from source: transforming the human-readable code (written in one or another programming language) into machine-readable code (binary) The most common way to do so, if a software package has its source coud available online is \u0002wzxhzdk:8\u0003 If you don't have the administrator rights, you'll often have to pass an extra argument to ./configure: \u0002wzxhzdk:9\u0003 Most of the softwares come with instructions on how to install them. Always read the file called README or INSTALL in the package directory before installing! ### Exercice The most popular unix distributions come with a version of python (a programming language) that is not the most recent one. Install from source the most recent version of python in a folder called `bin` in your home directory. You can download the python source code at https://www.python.org/ftp/python/3.6.0/Python-3.6.0.tgz ## Install python packages Python is a really popular programming language in the world of bioinformatics. Python has a package manager called `pip` that you can use to install softwares written in python. Please us the python executable you installed in the above exercise! Firstly, get pip: `wget https://bootstrap.pypa.io/get-pip.py` then execute the script `python get-pip.py` Thenm you can use pip to install package, either globally (if you're an administrator): `pip install youtube_dl` or just for you: `pip install --user youtube_dl` ## Final exercise One of the oldest and most famous bioinformatics package is called EMBOSS. Install EMBOSS in the bin directory of your home. Good luck! # Introduction to UNIX (continued) In the 4th and last module of your unix course, we'll how to write small programs, or scripts. Shell scripts allow us to program commands in chains and have the system execute them as a scripted chain of events. They also allow for far more useful functions, such as command substitution. You can invoke a command, like date, and use it\u2019s output as part of a file-naming scheme. You can automate backups and each copied file can have the current date appended to the end of its name. You can automate a bioinformatics analysis pipeline. Before we begin our scripting tutorial, let\u2019s cover some basic information. We\u2019ll be using the bash shell, which most Linux distributions use natively. Bash is available for Mac OS users and Cygwin on Windows (which you are using with MobaXterm). Since it\u2019s so universal, you should be able to script regardless of your platform. At their core, scripts are just plain text files. You can use nano (or any other text editor) to write them. ## Permissions Scripts are executed like programs. For this to happen, you need to have the proper permissions. You can make the script executable for you by running the following command: `chmod u+x my_script.sh` by convention, bash script are saved with the .sh extension. Linux doesn't really care about file extension, but it is easier for the user to use the \"proper\" extensions! ## executing a script You have to cd in the proper directory, then run the script like this: `./my_script.sh` To make things more convenient, you can place scripts in a \u201cbin\u201d folder in your home directory and add it to your path `mkdir -p ~/bin` More information on how to correctly modify your PATH [here](http://unix.stackexchange.com/a/26059) ## Getting started As previously said, every script is a text file. Still, there are rules and conventions to follow in order of you file being recognized as a script If you juste write a few command and try to execute it as is, with `./my_script`, it will not work. You can invoke `sh my_script`, but it is not very convenient. `./` tries to find out which interpreter to use (e.g. which programming language and how to execute your script). It does so by looking at the first line: The first line of your bash scripts should be: `#!/bin/bash` or `#!/usr/bin/env bash` The second version being better and more portable. Ask your teacher why! This line will have the same syntax for every interpreted language. If you are programming in python: `#!/usr/bin/env python` ### New line = new command After the firstline, every line of your script will be a new command. Your first scripts will essentially be a succession of terminal commands. We'll learn about flow control (if, for, while, ...) later on. ### Comments It is good practise to comment your scripts, i.e give some explanation of what is does, and explain a particularly arcane method that you wrote. Comments start with a `#` and are snippets of texts that are ignored by the interpreter. ### Your first script Let's start with a simple script, that copy files and append today's date to the end of the file name. We'll call it `datecp.sh` In your `~/bin` folder: \u0002wzxhzdk:10\u0003 and let's start writing our script `nano datecp.sh` \u0002wzxhzdk:11\u0003 Next, we need to declare a variable. A variable allows us to store and reuse information (characters, the date or the command `date`). Variables have a name, but can **expend** to their content when referenced if they contain a command. Variables can hold strings and characers, like this: `my_variable=\"hippopotamus\"` or a command. In bash, the correct way to store a command in a variable is within the syntax `$()`: `variable=$(command \u2013options arguments)` Store the date and time in a variable. Test the date command first in your terminal, then when you got the right format, store it in a variable in your script. It is generally bad practice to put spaces in file names in unix, so we'll want the following date format: `date +%m_%d_%y-%H.%M.%S` and for putting it into a variable: date_formatted=$(date +%m_%d_%y-%H.%M.%S) Your script now can print thedate without too much more coding: \u0002wzxhzdk:12\u0003 Now we need to add the copying part: `cp \u2013iv $1 $2.$date_formatted` This will invoke the copy command, with two options: -i for asking for permission before overwriting a file, and -v for verbose. You can also notice two variables: $1 and $2. When scripting in bash, a dollar sign ($) followed by a number will denote an argument of the script. For example in the following command: `cp \u2013iv a_file a_file_copy` the first argument ($1) is `a_file` and the second argument ($2) is `a_file_copy` What our script will do is a simple copy of a file, but with adding the date to the end of the file name. Save it and try it out! ### Exercise Write a script that backs itself up, that is, copies itself to a file named backup.sh. Hint: Use the cat command and the appropriate positional parameter.","title":"The command-line"},{"location":"tutorials/docs/command_line/#the-command-line","text":"Warning This lesson has been deprecated. Please refer to http://swcarpentry.github.io/shell-novice/ for a better, up-to-date lesson This tutorial is largely inspired of the Introduction to UNIX course from the Sanger Institute. The aim of this module is to introduce Unix and cover some of the basics that will allow you to be more comfortable with the command-line. Several of the programs that you are going to use during this course are useful for bioinformatics analyses. This module is only designed to provide a very brief introduction to some of the features and useful commands of Unix. During this module we will also obtain a genome sequence and examine the basic structure of an EMBL entry.","title":"The command-line"},{"location":"tutorials/docs/command_line/#introduction","text":"Unix is the standard operating system on most large computer systems in scientific research, in the same way that Microsoft Windows is the dominant operating system on desktop PCs. Unix and MS Windows both perform the important job of managing the computer\u2019s hardware (screen, keyboard, mouse, hard disks, network connections, etc...) on your behalf. They also provide you with tools to manage your files and to run application software. They both offer a graphical user interface (desktop). The desktops look different, call things by different names but they mostly can do the same things. Unix is a powerful, secure, robust and stable operating system that allows dozens of people to run programs on the same computer at the same time. This is why it is the preferred operating system for large-scale scientific computing. It is run on all kind of machines, like mobile phones (Android), desktop PCs, kitchen appliances,... all the way up to supercomputers. Unix powers the majority of the Internet.","title":"Introduction"},{"location":"tutorials/docs/command_line/#aims","text":"The aim of this course is to introduce Unix and cover the basics. The programs that you are going to use during the courses, plus many others that are useful for bioinformatics analyses, are run in Unix. This module is only designed to provide a very brief introduction to some of the features and useful commands of Unix. During this module we will also obtain a genome sequence and examine the basic structure of an EMBL entry.","title":"Aims"},{"location":"tutorials/docs/command_line/#why-use-unix","text":"Unix is a well established, very widespread operating system. You probably have a device running on Unix in your home without realising it (e.g. playstation, TV box, wireless router, android tablets/phones,... Command line driven, with a huge number of often terse, but powerful commands. In contrast to Windows, it is designed to allow many users to run their programs simultaneously on the same computer. Designed to work in computer networks - for example, most of the Internet is Unix based. It is used on many of the powerful computers at bioinformatics centres and also on many desktops and laptops (MacOS is largely UNIX compatible). The major difference between Unix and Windows is that it is free (as in freedom) and you can modify it to work however you want. This same principle of freedom is also used in most bioinformatics software. There are many distributions of Unix such as Ubuntu, RedHat, Fedora, Mint,...). These are all Unix, but they bundle up extra software in a different way or combinations. Some are known for being conservative and reliable; whilst others are know for being cutting-edge (and less reliable). The MacOSX operating system used by the eBioKit is also based on Unix.","title":"Why use Unix?"},{"location":"tutorials/docs/command_line/#getting-started","text":"For this course, you will have to connect to the eBiokit using SSH. SSH stands for Secure Shell and is a network protocol used to securely connect to a server. To do so, you will need an SSH client: On Linux: it is included by default, named Terminal. On MacOS: it is included by default, also named Terminal. On Windows: you'll have to download and install MobaXterm , a terminal emulator. Once you've opened your terminal (or terminal emulator), type ssh username@ip_address replacing username and ip_address with your username and the ip address of the server you are connecting to. Type your password when prompted. As you type, nothing will show on screen. No stars, no dots. It is supposed to be that way. Just type the password and press enter! You can type commands directly into the terminal at the \u2018$' prompt. A list of useful commands can be found on the next page. Many of them are two- or three-letter abbreviations. The earliest Unix systems ( circa 1970) only had slow Teletype terminals, so it was faster to type 'rm' to remove a file than 'delete' or 'erase'. This terseness is a feature of Unix that still survives.","title":"Getting started"},{"location":"tutorials/docs/command_line/#the-command-line_1","text":"All Unix programs may be run by typing commands at the Unix prompt. The command line tells the computer what to do. You may subtly alter these commands by specifying certain options when typing in the command line.","title":"The command line"},{"location":"tutorials/docs/command_line/#command-line-arguments","text":"Typing any Unix command for example ls , mv or cd at the Unix prompt with the appropriate variables such as files names or directories will result in the tasks being performed on pressing the enter key. The command is separated from the options and arguments by a space. Additional options and/or arguments can be added to the commands to affect the way the command works. Options usually have one dash and a letter (e.g. -h) or two dashes and a word (--help) with no space between the dash and the letter/word. Arguments are usually filenames or directories. For example: List the contents of a directory ls List the contents of a directoryList the contents of a directory with extra information about the files ls \u2013l List the contents of a directory with extra information about the files ls \u2013a List all contents including hidden files & directories ls -al List all contents including hidden files & directories, with extra information about the files ls \u2013l /usr/ List the contents of the directory /usr/, with extra information about the files","title":"Command line Arguments"},{"location":"tutorials/docs/command_line/#files-and-directories","text":"Directories are the Unix equivalent of folders on a PC or Mac. They are organised in a hierarchy, so directories can have sub-directories. Directories are very useful for organising your work and keeping your account tidy - for example, if you have more than one project, you can organise the files for each project into different directories to keep them separate. You can think of directories as rooms in a house. You can only be in one room (directory) at a time. When you are in a room you can see everything in that room easily. To see things in other rooms, you have to go to the appropriate door and crane your head around. Unix works in a similar manner, moving from directory to directory to access files. The location or directory that you are in is referred to as the current working directory. Directory structure example Therefore if there is a file called genome.seq in the dna directory its location or full pathname can be expressed as /nfs/dna/genome.seq.","title":"Files and Directories"},{"location":"tutorials/docs/command_line/#general-points","text":"Unix is pretty straightforward, but there are some general points to remember that will make your life easier: most flavors of UNIX are case sensitive - typing ls is generally not the same as typing LS . You need to put a space between a command and its argument - for example, less my_file will show you the contents of the file called my_file; lessmyfile will just give you an error! Unix is not psychic: If you misspell the name of a command or the name of a file, it will not understand you. Many of the commands are only a few letters long; this can be confusing until you start to think logically about why those letters were chosen - ls for list, rm for remove and so on. Often when you have problems with Unix, it is due to a spelling mistake, or perhaps you have omitted a space. If you want to know more about Unix and its commands there are plenty of resources available that provide a more comprehensive guide (including a cheat sheet at the end of this chapter. http://unix.t-a-y-l-o-r.com/ In what follows, we shall use the following typographical conventions: Characters written in bold typewriter font are commands to be typed into the computer as they stand. Characters written in italic typewriter font indicate non-specific file or directory names. Words inserted within square brackets [Ctrl] indicate keys to be pressed. So, for example, $ **ls** *any_directory* [Enter] means \"at the Unix prompt $, type ls followed by the name of some directory, then press Enter\" Don't forget to press the [Enter] key: commands are not sent to the computer until this is done.","title":"General Points"},{"location":"tutorials/docs/command_line/#some-useful-unix-commands-command-and-what-it-does","text":"Command What it does ls Lists the contents of the current directory mkdir Creates a new directory mv Moves or renames a file cp Copies a file rm Removes a file cat Concatenates files less Displays the contents of a file one page at a time head Displays the first ten lines of a file tail Displays the last ten lines of a file cd Changes current working directory pwd Prints working directory find Finds files matching an expression grep Searches a file for patterns wc Counts the lines, words, characters, and bytes in a file kill Stops a process jobs Lists the processes that are running","title":"Some useful Unix commands Command\u00a0and What it does"},{"location":"tutorials/docs/command_line/#firts-steps","text":"The following exercise introduces a few useful Unix commands and provides examples of how they can be used. Many people panic when they are confronted with an Unix prompt! Don\u2019t! The exercise is designed to be step-by-step, so all the commands you need are provided in the text. If you get lost ask a demonstrator. If you are a person skilled at Unix, be patient it is only a short exercise. Finding where you are and what you\u2019ve got pwd Print the working directory As seen previously directories are arranged in a hierarchical structure. To determine where you are in the hierarchy you can use the pwd command to display the name of the current working directory. The current working directory may be thought of as the directory you are in, i.e. your current position in the file-system tree To find out where you are type pwd [enter] You will see that you are in your home directory. We need to move into the ngs_course_data directory. Remember, Unix is case sensitive PWD is not the same as pwd cd Change current working directory The cd command will change the current working directory to another, in other words allow you to move up or down in the directory hierarchy. First of all we are going to move into the \"ngs_course_data\" directory below. To do this type: cd ngs_course_data [enter] Now use the pwd command to check your location in the directory hierarchy. Change again the directory to Module_Unix ls List the contents of a directory To find out what are the contents of the current directory type ls [enter] The ls command lists the contents of your current directory, this includes files and directories You should see that there are several other directories. Now use the cd command again to change to the Module_Unix directory.","title":"Firts steps"},{"location":"tutorials/docs/command_line/#changing-and-moving-what-youve-got","text":"cp Copy a file. cp file1 file2 is the command which makes a copy of file1 in the current working directory and calls it file2! What you are going to do is make a copy of AL513382.embl. This file contains the genome of Salmonella typhi strain CT18 in EMBL format (we'll learn more about file formats later during the course). The new file will be called S_typhi.embl. cp AL513382.embl S_typhi.embl [enter] If you use the ls command to check the contents of the current directory you will see that there is an extra file called S_typhi.embl. rm Delete a file. This command removes a file permanently, so be careful! You are now going to remove the old version of the S. typhi genome file, AL513382.embl rm AL513382.embl [enter] The file will be removed. Use the ls command to check the contents of the current directory to see that AL513382.embl has been removed. Unix, as a general rule does exactly what you ask, and does not ask for confirmation. Unfortunately there is no \"recycle bin\" on the command line to recover the file from, so you have to be careful. cd Change current working directory. As before the cd command will change the current working directory to another, in other words allow you to move up or down in the directory hierarchy. First of all we are going to move into the directory above, type: cd .. [enter] Now use the pwd command to check your location in the directory hierarchy. Next, we are going to move into the Module_Artemis directory. To change to the Module_Artemis directory type: cd Module_Artemis [enter] use the ls command to check the contents of the directory.","title":"Changing and moving what you\u2019ve got"},{"location":"tutorials/docs/command_line/#tips","text":"There are some short cuts for referring to directories: . Current directory (one full stop) .. Directory above (two full stops) ~ Home directory (tilde) / Root of the file system (like C:\\ in Windows) Pressing the tab key twice will try and autocomplete what you\u2019ve started typing or give you a list of all possible completions. This saves a lot of typing and typos. Pressing the up/down arrows will let you scroll through the previous commands. If you highlight some text, middle clicking will paste it on the command line. mv Move a file. To move a file from one place to another use the mv command. This moves the file rather than copies it, therefore you end up with only one file rather than two. When using the command the path or pathname is used to tell Unix where to find the file. You refer to files in other directories by using the list of hierarchical names separated by slashes. For example, the file bases in the directory genome has the path genome/bases If no path is specified Unix assumes that the file is in the current working directory. What you are going to do is move the file S_typhi.embl from the Module_Unix directory, to the current working directory. mv ../Module_Unix/S_typhi.embl . [enter] Use the ls command to check the contents of the current directory to see that S_typhi.embl has been moved. ../Module_Unix/S_typhi.embl specifies that S_typhi.embl is in the Module_Unix directory. If the file was in the directory above, the path would change to: ../ S_typhi.embl The command can also be used to rename a file in the current working directory. Previously we used the cp command, but mv provides an alternative without the need to delete the original file. Therefore we could have used: mv AL513382.embl S_typhi.embl [enter] instead of: cp AL513382.embl S_typhi.embl [enter] rm AL513382.embl [enter]","title":"Tips"},{"location":"tutorials/docs/command_line/#viewing-what-youve-got","text":"less Display file contents. This command displays the contents of a specified file one screen at a time. You are now going to look at the contents of S_typhi.embl. less S_typhi.embl [enter] The contents of S_typhi.embl will be displayed one screen at a time, to view the next screen press the space bar. less can also scroll backwards if you hit the b key. Another useful feature is the slash key, /, to search for a word in the file. You type the word you are looking for and press enter. The screen will jump to the next occurrence and highlight it. As S_typhi.embl is a large file this will take a while, therefore you may want to escape or exit from this command. To exit press the letter \u2018q\u2019. If you really need to exit from a program and it isn\u2019t responding press \u2018control\u2019 and the letter \u2018c\u2019 at the same time. head Display the first ten lines of a file tail Display the last ten lines of a file Sometimes you may just want to view the text at the beginning or the end of a file, without having to display all of the file. The head and tail commands can be used to do this. You are now going to look at the beginning of S_typhi.embl. head S_typhi.embl [enter] To look at the end of S_typhi.embl type: tail S_typhi.embl [enter] The number of lines that are displayed can be increased by adding extra arguments. To increase the number of lines viewed from 10 to 100 add the \u2013100 argument to the command. For example to view the last 100 lines of S_typhi.embl type: tail -100 S_typhi.embl [enter] Do this for both head and tail commands. What type of information is at the beginning and end of the EMBL format file? cat Join files together. Having looked at the beginning and end of the S_typhi.embl file you should notice that in EMBL format files the annotation comes first, then the DNA sequence at the end. If you had two separate files containing the annotation and the DNA sequence, both in EMBL format, it is possible to concatenate or join the two together to make a single file like the S_typhi.embl file you have just looked at. The Unix command cat can be used to join two or more files into a single file. The order in which the files are joined is determined by the order in which they appear in the command line. For example, we have two separate files, MAL13P1.dna and MAL13P1.tab, that contain the DNA and annotation, respectively, from the P. falciparum genome. Return to the Module_Unix directory using the cd command: cd ../Module_Unix [enter] and type cat MAL13P1.tab MAL13P1.dna > MAL13P1.embl [enter] MAL13P1.tab and MAL13P1.dna will be joined together and written to a file called MAL13P1.embl The > symbol in the command line directs the output of the cat program to the designated file MAL13P1.embl wc Counts the lines, words or characters of files. By typing the command line: ls | wc -l [enter] The above command uses wc to count the number of files that are listed by ls. The \u2018-l\u2019 option tells wc to return a count of the number of lines. The | symbol (known as the \u2018pipe\u2019 character) in the command line connects the two commands into a single operation for simplicity. You can connect as many commands as you want: ls | grep \".embl\" | wc -l This command will list out all of the files in the current directory, then send the results to the grep command which searches for all filenames containing the \u2018embl\u2019, then sends the results to wc which counts the number of lines (which corresponds to the number of files). grep Searches a file for patterns. grep is a powerful tool to search for patterns in a file. In the examples below, we are going to use the file called Malaria.fasta that contains the set of P. falciparum chromosomes in FASTA format. A FASTA file has the following format: Sequence Header CTAAACCTAAACCTAAACCCTGAACCCTAA... Therefore if we want to get the sequence headers, we can extract the lines that match the \u2018>\u2019 symbol: grep \u2018>\u2019 Malaria.fasta [enter] By typing the command line: grep -B 1 -A 1 'aagtagggttca' Malaria.fasta [enter] This command will search for a nucliotide sequence and print 1 line before and after any match. It won\u2019t find the pattern if it spans more than 1 line. find Finds files matching an expression. The find command is similar to ls but in many ways it is more powerful. It can be used to recursively search the directory tree for a specified path name, seeking files that match a given Boolean expression (a test which returns true or false) find . -name \u201c*.embl\u201d This command will return the files which name has the .embl suffix. mkdir test_directory find . -type d This command will return all the subdirectories contained in the current directory. These are just two basic examples but it is possible to search in many other ways: -mtime search files by modifying date -atime search files by last access date -size search files by file size -user search files by user they belong to.","title":"Viewing what you\u2019ve got"},{"location":"tutorials/docs/command_line/#tips_1","text":"You need to be careful with quoting when using wildcards! The wildcard * symbol represents a string of any character and of any length. For more information on Unix command see EMBNet UNIX Quick Guide.","title":"Tips"},{"location":"tutorials/docs/file_formats/","text":"File Formats This lecture is aimed at making you discover the most popular file formats used in bioinformatics. You're expected to have basic working knowledge of Linux to be able to follow the lesson. Table of Contents The fasta format The fastq format The sam/bam format The vcf format The gff format The fasta format The fasta format was invented in 1988 and designed to represent nucleotide or peptide sequences. It originates from the FASTA software package, but is now a standard in the world of bioinformatics. The first line in a FASTA file starts with a \">\" (greater-than) symbol followed by the description or identifier of the sequence. Following the initial line (used for a unique description of the sequence) is the actual sequence itself in standard one-letter code. A few sample sequences: >KX580312.1 Homo sapiens truncated breast cancer 1 (BRCA1) gene, exon 15 and partial cds GTCATCCCCTTCTAAATGCCCATCATTAGATGATAGGTGGTACATGCACAGTTGCTCTGGGAGTCTTCAG AATAGAAACTACCCATCTCAAGAGGAGCTCATTAAGGTTGTTGATGTGGAGGAGTAACAGCTGGAAGAGT CTGGGCCACACGATTTGACGGAAACATCTTACTTGCCAAGGCAAGATCTAG >KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457] MSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY HDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN GNHIEIE A fasta file can contain multiple sequence. Each sequence will be separated by their \"header\" line, starting by \">\". Example: >KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457] MSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY HDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN GNHIEIE >3HHU_A Chain A, Human Heat-Shock Protein 90 (Hsp90) MPEETQTQDQPMEEEEVETFAFQAEIAQLMSLIINTFYSNKEIFLRELISNSSDALDKIRYESLTDPSKL DSGKELHINLIPNKQDRTLTIVDTGIGMTKADLINNLGTIAKSGTKAFMEALQAGADISMIGQFGVGFYS AYLVAEKVTVITKHNDDEQYAWESSAGGSFTVRTDTGEPMGRGTKVILHLKEDQTEYLEERRIKEIVKKH SQFIGYPITLFVEK The fastq format The fastq format is also a text based format to represent nucleotide sequences, but also contains the corresponding quality of each nucleotide. It is the standard for storing the output of high-throughput sequencing instruments such as the Illumina machines. A fastq file uses four lines per sequence: Line 1 begins with a '@' character and is followed by a sequence identifier and an optional description (like a FASTA title line). Line 2 is the raw sequence letters. Line 3 begins with a '+' character and is optionally followed by the same sequence identifier (and any description) again. Line 4 encodes the quality values for the sequence in Line 2, and must contain the same number of symbols as letters in the sequence. An example sequence in fastq format: @SEQ_ID GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT + !''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65 Quality The quality, also called phred score, is the probability that the corresponding basecall is incorrect. Phred scores use a logarithmic scale, and are represented by ASCII characters, mapping to a quality usually going from 0 to 40. Phred Quality Score Probability of incorrect base call Base call accuracy 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9% 40 1 in 10,000 99.99% 50 1 in 100,000 99.999% 60 1 in 1,000,000 99.9999% the sam/bam format From Wikipedia : SAM (file format) is a text-based format for storing biological sequences aligned to a reference sequence developed by Heng Li. The acronym SAM stands for Sequence Alignment/Map. It is widely used for storing data, such as nucleotide sequences, generated by Next generation sequencing technologies and usually mapped to a reference. The SAM format consists of a header and an alignment section. The binary representation of a SAM file is a BAM file, which is a compressed SAM file.[1] SAM files can be analysed and edited with the software SAMtools. The SAM format has a really extensive and complex specification that you can find here . In brief it consists of a header section and reads (with other information) in tab delimited format. Example header section @HD VN:1.0 SO:unsorted @SQ SN:O_volvulusOVOC_OM1a LN:2816604 @SQ SN:O_volvulusOVOC_OM1b LN:28345163 @SQ SN:O_volvulusOVOC_OM2 LN:25485961 Example read M01137:130:00-A:17009:1352/14 * 0 0 * * 0 0 AGCAAAATACAACGATCTGGATGGTAGCATTAGCGATGCGACACTGCTTGAACCGTCAAAG FGGFGCFGFFGC8,,@D?E6EFCF,=AEFFGGDGGGADFGG@>FFEGGG:+<7D>AFCFGG YT:Z:UU the vcf format The vcf format is also a text-based file format. VCF stands for Variant Call Format and is used to store gene sequence variations (SNVs, indels). The format has been developped for genotyping projects, and is the standard to represent variations in the genome of a species. A vcf is a tab-delimited file, described here . VCF Example ##fileformat=VCFv4.0 ##fileDate=20110705 ##reference=1000GenomesPilot-NCBI37 ##phasing=partial ##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\"> ##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\"> ##INFO=<ID=AF,Number=.,Type=Float,Description=\"Allele Frequency\"> ##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\"> ##INFO=<ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\"> ##INFO=<ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\"> ##FILTER=<ID=q10,Description=\"Quality below 10\"> ##FILTER=<ID=s50,Description=\"Less than 50% of samples have data\"> ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\"> ##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\"> ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\"> ##FORMAT=<ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\"> #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample1 Sample2 Sample3 2 4370 rs6057 G A 29 . NS=2;DP=13;AF=0.5;DB;H2 GT:GQ:DP:HQ 0|0:48:1:52,51 1|0:48:8:51,51 1/1:43:5:.,. 2 7330 . T A 3 q10 NS=5;DP=12;AF=0.017 GT:GQ:DP:HQ 0|0:46:3:58,50 0|1:3:5:65,3 0/0:41:3 2 110696 rs6055 A G,T 67 PASS NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2 2/2:35:4 2 130237 . T . 47 . NS=2;DP=16;AA=T GT:GQ:DP:HQ 0|0:54:7:56,60 0|0:48:4:56,51 0/0:61:2 2 134567 microsat1 GTCT G,GTACT 50 PASS NS=2;DP=9;AA=G GT:GQ:DP 0/1:35:4 0/2:17:2 1/1:40:3 chr1 45796269 . G C chr1 45797505 . C G chr1 45798555 . T C chr1 45798901 . C T chr1 45805566 . G C chr2 47703379 . C T chr2 48010488 . G A chr2 48030838 . A T chr2 48032875 . CTAT - chr2 48032937 . T C chr2 48033273 . TTTTTGTTTTAATTCCT - chr2 48033551 . C G chr2 48033910 . A T chr2 215632048 . G T chr2 215632125 . TT - chr2 215632155 . T C chr2 215632192 . G A chr2 215632255 . CA TG chr2 215634055 . C T the gff format The general feature format (gff) is another text file format, used for describing genes and other features of DNA, RNA and protein sequences. It is the standard for annotation of genomes. A gff file should contain 9 columns, described here Example gff ##description: evidence-based annotation of the human genome (GRCh38), version 25 (Ensembl 85) ##provider: GENCODE ##contact: gencode-help@sanger.ac.uk ##format: gtf ##date: 2016-07-15 chr1 HAVANA gene 11869 14409 . + . gene_id \"ENSG00000223972.5\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; level 2; havana_gene \"OTTHUMG00000000961.2\"; chr1 HAVANA transcript 11869 14409 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 11869 12227 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 1; exon_id \"ENSE00002234944.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 12613 12721 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 2; exon_id \"ENSE00003582793.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 13221 14409 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 3; exon_id \"ENSE00002312635.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";","title":"File Formats"},{"location":"tutorials/docs/file_formats/#file-formats","text":"This lecture is aimed at making you discover the most popular file formats used in bioinformatics. You're expected to have basic working knowledge of Linux to be able to follow the lesson.","title":"File Formats"},{"location":"tutorials/docs/file_formats/#table-of-contents","text":"The fasta format The fastq format The sam/bam format The vcf format The gff format","title":"Table of Contents"},{"location":"tutorials/docs/file_formats/#the-fasta-format","text":"The fasta format was invented in 1988 and designed to represent nucleotide or peptide sequences. It originates from the FASTA software package, but is now a standard in the world of bioinformatics. The first line in a FASTA file starts with a \">\" (greater-than) symbol followed by the description or identifier of the sequence. Following the initial line (used for a unique description of the sequence) is the actual sequence itself in standard one-letter code. A few sample sequences: >KX580312.1 Homo sapiens truncated breast cancer 1 (BRCA1) gene, exon 15 and partial cds GTCATCCCCTTCTAAATGCCCATCATTAGATGATAGGTGGTACATGCACAGTTGCTCTGGGAGTCTTCAG AATAGAAACTACCCATCTCAAGAGGAGCTCATTAAGGTTGTTGATGTGGAGGAGTAACAGCTGGAAGAGT CTGGGCCACACGATTTGACGGAAACATCTTACTTGCCAAGGCAAGATCTAG >KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457] MSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY HDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN GNHIEIE A fasta file can contain multiple sequence. Each sequence will be separated by their \"header\" line, starting by \">\". Example: >KRN06561.1 heat shock [Lactobacillus sucicola DSM 21376 = JCM 15457] MSLVMANELTNRFNNWMKQDDFFGNLGRSFFDLDNSVNRALKTDVKETDKAYEVRIDVPGIDKKDITVDY HDGVLSVNAKRDSFNDESDSEGNVIASERSYGRFARQYSLPNVDESGIKAKCEDGVLKLTLPKLAEEKIN GNHIEIE >3HHU_A Chain A, Human Heat-Shock Protein 90 (Hsp90) MPEETQTQDQPMEEEEVETFAFQAEIAQLMSLIINTFYSNKEIFLRELISNSSDALDKIRYESLTDPSKL DSGKELHINLIPNKQDRTLTIVDTGIGMTKADLINNLGTIAKSGTKAFMEALQAGADISMIGQFGVGFYS AYLVAEKVTVITKHNDDEQYAWESSAGGSFTVRTDTGEPMGRGTKVILHLKEDQTEYLEERRIKEIVKKH SQFIGYPITLFVEK","title":"The fasta format"},{"location":"tutorials/docs/file_formats/#the-fastq-format","text":"The fastq format is also a text based format to represent nucleotide sequences, but also contains the corresponding quality of each nucleotide. It is the standard for storing the output of high-throughput sequencing instruments such as the Illumina machines. A fastq file uses four lines per sequence: Line 1 begins with a '@' character and is followed by a sequence identifier and an optional description (like a FASTA title line). Line 2 is the raw sequence letters. Line 3 begins with a '+' character and is optionally followed by the same sequence identifier (and any description) again. Line 4 encodes the quality values for the sequence in Line 2, and must contain the same number of symbols as letters in the sequence. An example sequence in fastq format: @SEQ_ID GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT + !''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65","title":"The fastq format"},{"location":"tutorials/docs/file_formats/#quality","text":"The quality, also called phred score, is the probability that the corresponding basecall is incorrect. Phred scores use a logarithmic scale, and are represented by ASCII characters, mapping to a quality usually going from 0 to 40. Phred Quality Score Probability of incorrect base call Base call accuracy 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9% 40 1 in 10,000 99.99% 50 1 in 100,000 99.999% 60 1 in 1,000,000 99.9999%","title":"Quality"},{"location":"tutorials/docs/file_formats/#the-sambam-format","text":"From Wikipedia : SAM (file format) is a text-based format for storing biological sequences aligned to a reference sequence developed by Heng Li. The acronym SAM stands for Sequence Alignment/Map. It is widely used for storing data, such as nucleotide sequences, generated by Next generation sequencing technologies and usually mapped to a reference. The SAM format consists of a header and an alignment section. The binary representation of a SAM file is a BAM file, which is a compressed SAM file.[1] SAM files can be analysed and edited with the software SAMtools. The SAM format has a really extensive and complex specification that you can find here . In brief it consists of a header section and reads (with other information) in tab delimited format.","title":"the sam/bam format"},{"location":"tutorials/docs/file_formats/#example-header-section","text":"@HD VN:1.0 SO:unsorted @SQ SN:O_volvulusOVOC_OM1a LN:2816604 @SQ SN:O_volvulusOVOC_OM1b LN:28345163 @SQ SN:O_volvulusOVOC_OM2 LN:25485961","title":"Example header section"},{"location":"tutorials/docs/file_formats/#example-read","text":"M01137:130:00-A:17009:1352/14 * 0 0 * * 0 0 AGCAAAATACAACGATCTGGATGGTAGCATTAGCGATGCGACACTGCTTGAACCGTCAAAG FGGFGCFGFFGC8,,@D?E6EFCF,=AEFFGGDGGGADFGG@>FFEGGG:+<7D>AFCFGG YT:Z:UU","title":"Example read"},{"location":"tutorials/docs/file_formats/#the-vcf-format","text":"The vcf format is also a text-based file format. VCF stands for Variant Call Format and is used to store gene sequence variations (SNVs, indels). The format has been developped for genotyping projects, and is the standard to represent variations in the genome of a species. A vcf is a tab-delimited file, described here .","title":"the vcf format"},{"location":"tutorials/docs/file_formats/#vcf-example","text":"##fileformat=VCFv4.0 ##fileDate=20110705 ##reference=1000GenomesPilot-NCBI37 ##phasing=partial ##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\"> ##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\"> ##INFO=<ID=AF,Number=.,Type=Float,Description=\"Allele Frequency\"> ##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\"> ##INFO=<ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\"> ##INFO=<ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\"> ##FILTER=<ID=q10,Description=\"Quality below 10\"> ##FILTER=<ID=s50,Description=\"Less than 50% of samples have data\"> ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\"> ##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\"> ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\"> ##FORMAT=<ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\"> #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample1 Sample2 Sample3 2 4370 rs6057 G A 29 . NS=2;DP=13;AF=0.5;DB;H2 GT:GQ:DP:HQ 0|0:48:1:52,51 1|0:48:8:51,51 1/1:43:5:.,. 2 7330 . T A 3 q10 NS=5;DP=12;AF=0.017 GT:GQ:DP:HQ 0|0:46:3:58,50 0|1:3:5:65,3 0/0:41:3 2 110696 rs6055 A G,T 67 PASS NS=2;DP=10;AF=0.333,0.667;AA=T;DB GT:GQ:DP:HQ 1|2:21:6:23,27 2|1:2:0:18,2 2/2:35:4 2 130237 . T . 47 . NS=2;DP=16;AA=T GT:GQ:DP:HQ 0|0:54:7:56,60 0|0:48:4:56,51 0/0:61:2 2 134567 microsat1 GTCT G,GTACT 50 PASS NS=2;DP=9;AA=G GT:GQ:DP 0/1:35:4 0/2:17:2 1/1:40:3 chr1 45796269 . G C chr1 45797505 . C G chr1 45798555 . T C chr1 45798901 . C T chr1 45805566 . G C chr2 47703379 . C T chr2 48010488 . G A chr2 48030838 . A T chr2 48032875 . CTAT - chr2 48032937 . T C chr2 48033273 . TTTTTGTTTTAATTCCT - chr2 48033551 . C G chr2 48033910 . A T chr2 215632048 . G T chr2 215632125 . TT - chr2 215632155 . T C chr2 215632192 . G A chr2 215632255 . CA TG chr2 215634055 . C T","title":"VCF Example"},{"location":"tutorials/docs/file_formats/#the-gff-format","text":"The general feature format (gff) is another text file format, used for describing genes and other features of DNA, RNA and protein sequences. It is the standard for annotation of genomes. A gff file should contain 9 columns, described here","title":"the gff format"},{"location":"tutorials/docs/file_formats/#example-gff","text":"##description: evidence-based annotation of the human genome (GRCh38), version 25 (Ensembl 85) ##provider: GENCODE ##contact: gencode-help@sanger.ac.uk ##format: gtf ##date: 2016-07-15 chr1 HAVANA gene 11869 14409 . + . gene_id \"ENSG00000223972.5\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; level 2; havana_gene \"OTTHUMG00000000961.2\"; chr1 HAVANA transcript 11869 14409 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 11869 12227 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 1; exon_id \"ENSE00002234944.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 12613 12721 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 2; exon_id \"ENSE00003582793.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\"; chr1 HAVANA exon 13221 14409 . + . gene_id \"ENSG00000223972.5\"; transcript_id \"ENST00000456328.2\"; gene_type \"transcribed_unprocessed_pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"processed_transcript\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1-002\"; exon_number 3; exon_id \"ENSE00002312635.1\"; level 2; transcript_support_level \"1\"; tag \"basic\"; havana_gene \"OTTHUMG00000000961.2\"; havana_transcript \"OTTHUMT00000362751.1\";","title":"Example gff"},{"location":"tutorials/docs/mapping/","text":"Mapping and Variant Calling In this practical you will learn to map NGS reads to a reference sequence, check the output using a viewer software and investigate some aspects of the results. You will be using the read data from the Quality Control practical. EHEC O157 strains generally carry a large virulence plasmid, pO157. Plasmids are circular genetic elements that many bacteria carry in addition to their chromosomes. This particular plasmid encodes a number of proteins which are known or suspected to be involved in the ability to cause severe disease in infected humans. Your task in this practical is to map your prepared read set to a reference sequence of the virulence plasmid, to determine if the pO157 plasmid is present in the St. Louis outbreak strain. Illustration of plasmid integration into a host bacteria Downloading a Reference You will need a reference sequence to map your reads to. cd ~/work curl -O -J -L https://osf.io/rnzbe/download This file contains the sequence of the pO157 plasmid from the Sakai outbreak strain of E. coli O157. In contrast to the strain we are working on, this strain is available as a finished genome, i.e. the whole sequence of both the single chromosome and the large virulence plasmid are known. Indexing the reference Before aligning the reads against a reference, it is necessary to build an index of that reference bowtie2-build pO157_Sakai.fasta.gz pO157_Sakai Note Indexing the reference is a necessary pre-processing step that makes searching for patterns much much faster. Many popular aligners such as Bowtie and BWA use an algorithm called the Burrows\u2013Wheeler transform to build the index. Aligning reads Now we are ready to map our reads bowtie2 -x pO157_Sakai -1 SRR957824_trimmed_R1.fastq.gz \\ -2 SRR957824_trimmed_R2.fastq.gz -S SRR957824.sam The output of the mapping will be in the SAM format. You can find a brief explanation of the SAM format here Note In this tutorial as well as many other places, you'll often see the terms mapping and alignment being used interchangeably. If you want to read more about the difference between the two, I invite you to read this excellent Biostars discussion Visualising with tview head SRR957824.sam But it is not very informative. We'll use samtools to visualise our data Before downloading the data in tablet, we have to convert our SAM file into BAM, a compressed version of SAM that can be indexed. samtools view -hSbo SRR957824.bam SRR957824.sam Sort the bam file per position in the genome and index it samtools sort SRR957824.bam SRR2584857.sorted.bam samtools index SRR2584857.sorted.bam Finally we can visualise with samtools tview samtools tview SRR2584857.sorted.bam pO157_Sakai.fasta.gz Tip navigate in tview: - left and right arrows scroll - q to quit - CTRL-h and CTRL-l scrolls more - g gi|10955266|ref|NC_002128.1|:8000 will take you to a specific location. Variant Calling A frequent application for mapping reads is variant calling, i.e. finding positions where the reads are systematically different from the reference genome. Single nucleotide polymorphism (SNP)-based typing is particularly popular and used for a broad range of applications. For an EHEC O157 outbreak you could use it to identify the source, for instance. We can call the variants using samtools mpileup samtools mpileup -uD -f pO157_Sakai.fasta.gz SRR2584857.sorted.bam | \\ bcftools view - > variants.vcf You can read about the structure of vcf files here . The documentation is quite painful to read and take a look at the file Look at the non-commented lines grep -v ^## variants.vcf The first five columns are CHROM POS ID REF ALT . Use grep -v ^## variants.vcf | less -S for a better view. Tip Use your left and right arrows to scroll horizontally, and q to quit. Question How many SNPs did the variant caller find? Did you find any indels? Examine one of the variants with tview samtools tview SRR2584857.sorted.bam pO157_Sakai.fasta.gz \\ -p 'gi|10955266|ref|NC_002128.1|:43071' That seems very real! Question Where do reference genomes come from?","title":"Mapping and Variant Calling"},{"location":"tutorials/docs/mapping/#mapping-and-variant-calling","text":"In this practical you will learn to map NGS reads to a reference sequence, check the output using a viewer software and investigate some aspects of the results. You will be using the read data from the Quality Control practical. EHEC O157 strains generally carry a large virulence plasmid, pO157. Plasmids are circular genetic elements that many bacteria carry in addition to their chromosomes. This particular plasmid encodes a number of proteins which are known or suspected to be involved in the ability to cause severe disease in infected humans. Your task in this practical is to map your prepared read set to a reference sequence of the virulence plasmid, to determine if the pO157 plasmid is present in the St. Louis outbreak strain. Illustration of plasmid integration into a host bacteria","title":"Mapping and Variant Calling"},{"location":"tutorials/docs/mapping/#downloading-a-reference","text":"You will need a reference sequence to map your reads to. cd ~/work curl -O -J -L https://osf.io/rnzbe/download This file contains the sequence of the pO157 plasmid from the Sakai outbreak strain of E. coli O157. In contrast to the strain we are working on, this strain is available as a finished genome, i.e. the whole sequence of both the single chromosome and the large virulence plasmid are known.","title":"Downloading a Reference"},{"location":"tutorials/docs/mapping/#indexing-the-reference","text":"Before aligning the reads against a reference, it is necessary to build an index of that reference bowtie2-build pO157_Sakai.fasta.gz pO157_Sakai Note Indexing the reference is a necessary pre-processing step that makes searching for patterns much much faster. Many popular aligners such as Bowtie and BWA use an algorithm called the Burrows\u2013Wheeler transform to build the index.","title":"Indexing the reference"},{"location":"tutorials/docs/mapping/#aligning-reads","text":"Now we are ready to map our reads bowtie2 -x pO157_Sakai -1 SRR957824_trimmed_R1.fastq.gz \\ -2 SRR957824_trimmed_R2.fastq.gz -S SRR957824.sam The output of the mapping will be in the SAM format. You can find a brief explanation of the SAM format here Note In this tutorial as well as many other places, you'll often see the terms mapping and alignment being used interchangeably. If you want to read more about the difference between the two, I invite you to read this excellent Biostars discussion","title":"Aligning reads"},{"location":"tutorials/docs/mapping/#visualising-with-tview","text":"head SRR957824.sam But it is not very informative. We'll use samtools to visualise our data Before downloading the data in tablet, we have to convert our SAM file into BAM, a compressed version of SAM that can be indexed. samtools view -hSbo SRR957824.bam SRR957824.sam Sort the bam file per position in the genome and index it samtools sort SRR957824.bam SRR2584857.sorted.bam samtools index SRR2584857.sorted.bam Finally we can visualise with samtools tview samtools tview SRR2584857.sorted.bam pO157_Sakai.fasta.gz Tip navigate in tview: - left and right arrows scroll - q to quit - CTRL-h and CTRL-l scrolls more - g gi|10955266|ref|NC_002128.1|:8000 will take you to a specific location.","title":"Visualising with tview"},{"location":"tutorials/docs/mapping/#variant-calling","text":"A frequent application for mapping reads is variant calling, i.e. finding positions where the reads are systematically different from the reference genome. Single nucleotide polymorphism (SNP)-based typing is particularly popular and used for a broad range of applications. For an EHEC O157 outbreak you could use it to identify the source, for instance. We can call the variants using samtools mpileup samtools mpileup -uD -f pO157_Sakai.fasta.gz SRR2584857.sorted.bam | \\ bcftools view - > variants.vcf You can read about the structure of vcf files here . The documentation is quite painful to read and take a look at the file Look at the non-commented lines grep -v ^## variants.vcf The first five columns are CHROM POS ID REF ALT . Use grep -v ^## variants.vcf | less -S for a better view. Tip Use your left and right arrows to scroll horizontally, and q to quit. Question How many SNPs did the variant caller find? Did you find any indels? Examine one of the variants with tview samtools tview SRR2584857.sorted.bam pO157_Sakai.fasta.gz \\ -p 'gi|10955266|ref|NC_002128.1|:43071' That seems very real! Question Where do reference genomes come from?","title":"Variant Calling"},{"location":"tutorials/docs/meta_assembly/","text":"Metagenome assembly and binning In this tutorial you'll learn how to inspect assemble metagenomic data and retrieve draft genomes from assembled metagenomes We'll use a mock community of 20 bacteria sequenced using the Illumina HiSeq. In reality the data were simulated using InSilicoSeq . The 20 bacteria in the dataset were selected from the Tara Ocean study that recovered 957 distinct Metagenome-assembled-genomes (or MAGs) that were previsouly unknown! (full list on figshare ) Getting the Data mkdir -p ~/data cd ~/data curl -O -J -L https://osf.io/th9z6/download curl -O -J -L https://osf.io/k6vme/download chmod -w tara_reads_R* Quality Control we'll use FastQC to check the quality of our data, as well as sickle for trimming the bad quality part of the reads. If you need a refresher on how and why to check the quality of sequence data, please check the Quality Control and Trimming tutorial mkdir -p ~/results cd ~/results ln -s ~/data/tara_reads_* . fastqc tara_reads_*.fastq.gz Question What is the average read length? The average quality? Question Compared to single genome sequencing, what graphs differ? Now we'll trim the reads using sickle sickle pe -f tara_reads_R1.fastq.gz -r tara_reads_R2.fastq.gz -t sanger \\ -o tara_trimmed_R1.fastq -p tara_trimmed_R2.fastq -s /dev/null Question How many reads were trimmed? Assembly Megahit will be used for the assembly. megahit -1 tara_trimmed_R1.fastq -2 tara_trimmed_R2.fastq -o tara_assembly the resulting assenmbly can be found under tara_assembly/final.contigs.fa . Question How many contigs does this assembly contain? Binning First we need to map the reads back against the assembly to get coverage information ln -s tara_assembly/final.contigs.fa . bowtie2-build final.contigs.fa final.contigs bowtie2 -x final.contigs -1 tara_reads_R1.fastq.gz -2 tara_reads_R2.fastq.gz | \\ samtools view -bS -o tara_to_sort.bam samtools sort tara_to_sort.bam -o tara.bam samtools index tara.bam then we run metabat runMetaBat.sh -m 1500 final.contigs.fa tara.bam mv final.contigs.fa.metabat-bins1500 metabat Question How many bins did we obtain? Checking the quality of the bins The first time you run checkm you have to create the database sudo checkm data setRoot ~/.local/data/checkm checkm lineage_wf -x fa metabat checkm/ checkm bin_qa_plot -x fa checkm metabat plots Question Which bins should we keep for downstream analysis? Note checkm can plot a lot of metrics. If you have time, check the manual and try to produce different plots Warning if checkm fails at the phylogeny step, it is likely that your vm doesn't have enough RAM. pplacer requires about 35G of RAM to place the bins in the tree of life. In that case, execute the following cd ~/results curl -O -J -L https://osf.io/xuzhn/download tar xzf checkm.tar.gz checkm qa checkm/lineage.ms checkm then plot the completeness checkm bin_qa_plot -x fa checkm metabat plots and take a look at plots/bin_qa_plot.png Further reading Recovery of nearly 8,000 metagenome-assembled genomes substantially expands the tree of life The reconstruction of 2,631 draft metagenome-assembled genomes from the global oceans","title":"Metagenome assembly"},{"location":"tutorials/docs/meta_assembly/#metagenome-assembly-and-binning","text":"In this tutorial you'll learn how to inspect assemble metagenomic data and retrieve draft genomes from assembled metagenomes We'll use a mock community of 20 bacteria sequenced using the Illumina HiSeq. In reality the data were simulated using InSilicoSeq . The 20 bacteria in the dataset were selected from the Tara Ocean study that recovered 957 distinct Metagenome-assembled-genomes (or MAGs) that were previsouly unknown! (full list on figshare )","title":"Metagenome assembly and binning"},{"location":"tutorials/docs/meta_assembly/#getting-the-data","text":"mkdir -p ~/data cd ~/data curl -O -J -L https://osf.io/th9z6/download curl -O -J -L https://osf.io/k6vme/download chmod -w tara_reads_R*","title":"Getting the Data"},{"location":"tutorials/docs/meta_assembly/#quality-control","text":"we'll use FastQC to check the quality of our data, as well as sickle for trimming the bad quality part of the reads. If you need a refresher on how and why to check the quality of sequence data, please check the Quality Control and Trimming tutorial mkdir -p ~/results cd ~/results ln -s ~/data/tara_reads_* . fastqc tara_reads_*.fastq.gz Question What is the average read length? The average quality? Question Compared to single genome sequencing, what graphs differ? Now we'll trim the reads using sickle sickle pe -f tara_reads_R1.fastq.gz -r tara_reads_R2.fastq.gz -t sanger \\ -o tara_trimmed_R1.fastq -p tara_trimmed_R2.fastq -s /dev/null Question How many reads were trimmed?","title":"Quality Control"},{"location":"tutorials/docs/meta_assembly/#assembly","text":"Megahit will be used for the assembly. megahit -1 tara_trimmed_R1.fastq -2 tara_trimmed_R2.fastq -o tara_assembly the resulting assenmbly can be found under tara_assembly/final.contigs.fa . Question How many contigs does this assembly contain?","title":"Assembly"},{"location":"tutorials/docs/meta_assembly/#binning","text":"First we need to map the reads back against the assembly to get coverage information ln -s tara_assembly/final.contigs.fa . bowtie2-build final.contigs.fa final.contigs bowtie2 -x final.contigs -1 tara_reads_R1.fastq.gz -2 tara_reads_R2.fastq.gz | \\ samtools view -bS -o tara_to_sort.bam samtools sort tara_to_sort.bam -o tara.bam samtools index tara.bam then we run metabat runMetaBat.sh -m 1500 final.contigs.fa tara.bam mv final.contigs.fa.metabat-bins1500 metabat Question How many bins did we obtain?","title":"Binning"},{"location":"tutorials/docs/meta_assembly/#checking-the-quality-of-the-bins","text":"The first time you run checkm you have to create the database sudo checkm data setRoot ~/.local/data/checkm checkm lineage_wf -x fa metabat checkm/ checkm bin_qa_plot -x fa checkm metabat plots Question Which bins should we keep for downstream analysis? Note checkm can plot a lot of metrics. If you have time, check the manual and try to produce different plots Warning if checkm fails at the phylogeny step, it is likely that your vm doesn't have enough RAM. pplacer requires about 35G of RAM to place the bins in the tree of life. In that case, execute the following cd ~/results curl -O -J -L https://osf.io/xuzhn/download tar xzf checkm.tar.gz checkm qa checkm/lineage.ms checkm then plot the completeness checkm bin_qa_plot -x fa checkm metabat plots and take a look at plots/bin_qa_plot.png","title":"Checking the quality of the bins"},{"location":"tutorials/docs/meta_assembly/#further-reading","text":"Recovery of nearly 8,000 metagenome-assembled genomes substantially expands the tree of life The reconstruction of 2,631 draft metagenome-assembled genomes from the global oceans","title":"Further reading"},{"location":"tutorials/docs/nanopore/","text":"Introduction to Nanopore Sequencing In this tutorial we will assemble the E. coli genome using a mix of long, error-prone reads from the MinION (Oxford Nanopore) and short reads from a HiSeq instrument (Illumina). The MinION data used in this tutorial come a test run by the Loman lab . The Illumina data were simulated using InSilicoSeq Get the Data First download the nanopore data wget http://s3.climb.ac.uk/nanopore/ecoli_allreads.fasta You will not need the HiSeq data right away, but you can start the download in another window curl -O -J -L https://osf.io/pxk7f/download curl -O -J -L https://osf.io/zax3c/download look at basic stats of the nanopore reads assembly-stats ecoli_allreads.fasta Question How many nanopore reads do we have? Question How long is the longest read? Question What is the average read length? Adapter trimming The guppy basecaller, i.e. the program that transform raw electrical signal in fastq files, already demultiplex and trim for us. Assembly We assemble the reads using wtdbg2 (version > 2.3) head -n 20000 ecoli_allreads.fasta > subset.fasta wtdbg2 -x rs -i subset.fasta -fo assembly wtpoa-cns -i assembly.ctg.lay -fo assembly.ctg.fa Polishing Since the miniasm assembly likely contains a lot if errors, we correct it with Illumina reads. First we map the short reads against the assembly bowtie2-build assembly.ctg.fa assembly bowtie2 -x assembly -1 ecoli_hiseq_R1.fastq.gz -2 ecoli_hiseq_R2.fastq.gz | \\ samtools view -bS -o assembly_short_reads.bam samtools sort assembly_short.bam -o assembly_short_sorted.bam samtools index assembly_short_sorted.bam then we run the consensus step samtools view assembly_short_sorted.bam | ./wtpoa-cns -t 16 -x sam-sr \\ -d assembly.ctg.fa -i - -fo assembly_polished.fasta which will correct eventual misamatches in our assembly and write the new improved assembly to assembly_polished.fasta For better results we should perform more than one round of polishing. Compare with the existing assembly and an illumina only assembly an existing assembly Go to https://www.ncbi.nlm.nih.gov and search for NC_000913. Download the associated genome in fasta format and rename it to ecoli_ref.fasta nucmer --maxmatch -c 100 -p ecoli ERR1147227_trimmed.fastq ecoli_ref.fasta mummerplot --fat --filter --png --large -p ecoli ecoli.delta then take a look at ecoli.png compare metrics Note First you need to assembly the illumina data Then run busco and quast on the 3 assemblies Question which assembly would you say is the best? Annotation If you havw time, train your annotation skills by running prokka on your genome! awk '/^>/{print \">ctg\" ++i; next}{print}' < assembly_polished.fasta \\ > assembly_polished_formatted.fasta prokka --outdir annotation --kingdom Bacteria assembly_polished_formatted.fasta You can open the output to see how it went cat annotation/*.txt Question Does it fit your expectations? How many genes were you expecting?","title":"Nanopore Sequencing"},{"location":"tutorials/docs/nanopore/#introduction-to-nanopore-sequencing","text":"In this tutorial we will assemble the E. coli genome using a mix of long, error-prone reads from the MinION (Oxford Nanopore) and short reads from a HiSeq instrument (Illumina). The MinION data used in this tutorial come a test run by the Loman lab . The Illumina data were simulated using InSilicoSeq","title":"Introduction to Nanopore Sequencing"},{"location":"tutorials/docs/nanopore/#get-the-data","text":"First download the nanopore data wget http://s3.climb.ac.uk/nanopore/ecoli_allreads.fasta You will not need the HiSeq data right away, but you can start the download in another window curl -O -J -L https://osf.io/pxk7f/download curl -O -J -L https://osf.io/zax3c/download look at basic stats of the nanopore reads assembly-stats ecoli_allreads.fasta Question How many nanopore reads do we have? Question How long is the longest read? Question What is the average read length?","title":"Get the Data"},{"location":"tutorials/docs/nanopore/#adapter-trimming","text":"The guppy basecaller, i.e. the program that transform raw electrical signal in fastq files, already demultiplex and trim for us.","title":"Adapter trimming"},{"location":"tutorials/docs/nanopore/#assembly","text":"We assemble the reads using wtdbg2 (version > 2.3) head -n 20000 ecoli_allreads.fasta > subset.fasta wtdbg2 -x rs -i subset.fasta -fo assembly wtpoa-cns -i assembly.ctg.lay -fo assembly.ctg.fa","title":"Assembly"},{"location":"tutorials/docs/nanopore/#polishing","text":"Since the miniasm assembly likely contains a lot if errors, we correct it with Illumina reads. First we map the short reads against the assembly bowtie2-build assembly.ctg.fa assembly bowtie2 -x assembly -1 ecoli_hiseq_R1.fastq.gz -2 ecoli_hiseq_R2.fastq.gz | \\ samtools view -bS -o assembly_short_reads.bam samtools sort assembly_short.bam -o assembly_short_sorted.bam samtools index assembly_short_sorted.bam then we run the consensus step samtools view assembly_short_sorted.bam | ./wtpoa-cns -t 16 -x sam-sr \\ -d assembly.ctg.fa -i - -fo assembly_polished.fasta which will correct eventual misamatches in our assembly and write the new improved assembly to assembly_polished.fasta For better results we should perform more than one round of polishing.","title":"Polishing"},{"location":"tutorials/docs/nanopore/#compare-with-the-existing-assembly-and-an-illumina-only-assembly","text":"","title":"Compare with the existing assembly and an illumina only assembly"},{"location":"tutorials/docs/nanopore/#an-existing-assembly","text":"Go to https://www.ncbi.nlm.nih.gov and search for NC_000913. Download the associated genome in fasta format and rename it to ecoli_ref.fasta nucmer --maxmatch -c 100 -p ecoli ERR1147227_trimmed.fastq ecoli_ref.fasta mummerplot --fat --filter --png --large -p ecoli ecoli.delta then take a look at ecoli.png","title":"an existing assembly"},{"location":"tutorials/docs/nanopore/#compare-metrics","text":"Note First you need to assembly the illumina data Then run busco and quast on the 3 assemblies Question which assembly would you say is the best?","title":"compare metrics"},{"location":"tutorials/docs/nanopore/#annotation","text":"If you havw time, train your annotation skills by running prokka on your genome! awk '/^>/{print \">ctg\" ++i; next}{print}' < assembly_polished.fasta \\ > assembly_polished_formatted.fasta prokka --outdir annotation --kingdom Bacteria assembly_polished_formatted.fasta You can open the output to see how it went cat annotation/*.txt Question Does it fit your expectations? How many genes were you expecting?","title":"Annotation"},{"location":"tutorials/docs/pan_genome/","text":"Pan-Genome Analysis In this tutorial we will learn how to determine a pan-genome from a collection of isolate genomes. This tutorial is inspired from Genome annotation and Pangenome Analysis from the CBIB in Santiago, Chile Getting the data We'll data from this article and analyse the core and accessory genomes of E. coli strains Firstly, download the supplementary csv file containing information on all the strains using in the study. Link Then, open it in Excel (or any software that opens .csv files) and select 32 strains. Download these 32 strains from the ENA website! Note Alternatively you can use ena browser tools to download the files. It is available as a bioconda recipe. # for getting 10 random strains at the command-line cut -d',' -f 1 journal.pcbi.1006258.s010.csv | tail -n +2 | shuf | head -10 > strains.txt cat strains.txt | parallel enaGroupGet -f fastq {} if you wish to use the same strains as your instructor: curl -O -J -L https://osf.io/s43mv/download cat strains.txt | parallel enaGroupGet -f fastq {} and then put all the reads in the same directory mkdir reads mv ERS*/*/*.fastq.gz reads/ rm -r ERS* Assemble and Annotate the strains You'll assemble your strains with megahit. mkdir assemblies for r1 in reads/*_1.fastq.gz do prefix=$(basename $r1 _1.fastq.gz) r2=reads/${prefix}_2.fastq.gz megahit -1 $r1 -2 $r2 -o ${prefix} --out-prefix ${prefix} mv ${prefix}/${prefix}.contigs.fa assemblies/ rm -r ${prefix} done and use prokka to annotate mkdir annotation for assembly in assemblies/*.fa do prefix=$(basename $assembly .contigs.fa) prokka --usegenus --genus Escherichia --species coli --strain ${prefix} \\ --outdir ${prefix} --prefix ${prefix} ${assembly} mv ${prefix}/${prefix}.gff annotation/ rm -r ${prefix} done Pan-genome analysis put all the .gff files in the same folder (e.g., ./gff ) and run Roary roary -f roary -e -n -v annotation/*.gff Roary will get all the coding sequences, convert them into protein, and create pre-clusters. Then, using BLASTP and MCL, Roary will create clusters, and check for paralogs. Finally, Roary will take every isolate and order them by presence/absence of orthologs. The summary output is present in the summary_statistics.txt file. Additionally, Roary produces a gene_presence_absence.csv file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not. Plotting the result Roary comes with a python script that allows you to generate a few plots to graphically assess your analysis output. First, we need to generate a tree file from the alignment generated by Roary: cd roary FastTree \u2013nt \u2013gtr core_gene_alignment.aln > my_tree.newick Then we can plot the Roary results with roary_plots.py , a community contriubuted python script to visualise roary results: wget https://raw.githubusercontent.com/sanger-pathogens/Roary/master/contrib/roary_plots/roary_plots.py python roary_plots.py roary_plots.py my_tree.newick gene_presence_absence.csv then look at the 3 /png files that have been generated","title":"Pan-genome Analysis"},{"location":"tutorials/docs/pan_genome/#pan-genome-analysis","text":"In this tutorial we will learn how to determine a pan-genome from a collection of isolate genomes. This tutorial is inspired from Genome annotation and Pangenome Analysis from the CBIB in Santiago, Chile","title":"Pan-Genome Analysis"},{"location":"tutorials/docs/pan_genome/#getting-the-data","text":"We'll data from this article and analyse the core and accessory genomes of E. coli strains Firstly, download the supplementary csv file containing information on all the strains using in the study. Link Then, open it in Excel (or any software that opens .csv files) and select 32 strains. Download these 32 strains from the ENA website! Note Alternatively you can use ena browser tools to download the files. It is available as a bioconda recipe. # for getting 10 random strains at the command-line cut -d',' -f 1 journal.pcbi.1006258.s010.csv | tail -n +2 | shuf | head -10 > strains.txt cat strains.txt | parallel enaGroupGet -f fastq {} if you wish to use the same strains as your instructor: curl -O -J -L https://osf.io/s43mv/download cat strains.txt | parallel enaGroupGet -f fastq {} and then put all the reads in the same directory mkdir reads mv ERS*/*/*.fastq.gz reads/ rm -r ERS*","title":"Getting the data"},{"location":"tutorials/docs/pan_genome/#assemble-and-annotate-the-strains","text":"You'll assemble your strains with megahit. mkdir assemblies for r1 in reads/*_1.fastq.gz do prefix=$(basename $r1 _1.fastq.gz) r2=reads/${prefix}_2.fastq.gz megahit -1 $r1 -2 $r2 -o ${prefix} --out-prefix ${prefix} mv ${prefix}/${prefix}.contigs.fa assemblies/ rm -r ${prefix} done and use prokka to annotate mkdir annotation for assembly in assemblies/*.fa do prefix=$(basename $assembly .contigs.fa) prokka --usegenus --genus Escherichia --species coli --strain ${prefix} \\ --outdir ${prefix} --prefix ${prefix} ${assembly} mv ${prefix}/${prefix}.gff annotation/ rm -r ${prefix} done","title":"Assemble and Annotate the strains"},{"location":"tutorials/docs/pan_genome/#pan-genome-analysis_1","text":"put all the .gff files in the same folder (e.g., ./gff ) and run Roary roary -f roary -e -n -v annotation/*.gff Roary will get all the coding sequences, convert them into protein, and create pre-clusters. Then, using BLASTP and MCL, Roary will create clusters, and check for paralogs. Finally, Roary will take every isolate and order them by presence/absence of orthologs. The summary output is present in the summary_statistics.txt file. Additionally, Roary produces a gene_presence_absence.csv file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not.","title":"Pan-genome analysis"},{"location":"tutorials/docs/pan_genome/#plotting-the-result","text":"Roary comes with a python script that allows you to generate a few plots to graphically assess your analysis output. First, we need to generate a tree file from the alignment generated by Roary: cd roary FastTree \u2013nt \u2013gtr core_gene_alignment.aln > my_tree.newick Then we can plot the Roary results with roary_plots.py , a community contriubuted python script to visualise roary results: wget https://raw.githubusercontent.com/sanger-pathogens/Roary/master/contrib/roary_plots/roary_plots.py python roary_plots.py roary_plots.py my_tree.newick gene_presence_absence.csv then look at the 3 /png files that have been generated","title":"Plotting the result"},{"location":"tutorials/docs/qc/","text":"Quality Control and Trimming Lecture Practical In this practical you will learn to import, view and check the quality of raw high thoughput sequencing sequencing data. The first dataset you will be working with is from an Illumina MiSeq dataset. The sequenced organism is an enterohaemorrhagic E. coli (EHEC) of the serotype O157, a potentially fatal gastrointestinal pathogen. The sequenced bacterium was part of an outbreak investigation in the St. Louis area, USA in 2011. The sequencing was done as paired-end 2x150bp. Downloading the data The raw data were deposited at the European Nucleotide Archive, under the accession number SRR957824. You could go to the ENA website and search for the run with the accession SRR957824. However these files contain about 3 million reads and are therefore quite big. We are only gonna use a subset of the original dataset for this tutorial. First create a data/ directory in your home folder mkdir ~/data now let's download the subset cd ~/data curl -O -J -L https://osf.io/shqpv/download curl -O -J -L https://osf.io/9m3ch/download Let\u2019s make sure we downloaded all of our data using md5sum. md5sum SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz you should see this 1e8cf249e3217a5a0bcc0d8a654585fb SRR957824_500K_R1.fastq.gz 70c726a31f05f856fe942d727613adb7 SRR957824_500K_R2.fastq.gz and now look at the file names and their size ls -l total 97M -rw-r--r-- 1 hadrien 48M Nov 19 18:44 SRR957824_500K_R1.fastq.gz -rw-r--r-- 1 hadrien 50M Nov 19 18:53 SRR957824_500K_R2.fastq.gz There are 500 000 paired-end reads taken randomly from the original data One last thing before we get to the quality control: those files are writeable. By default, UNIX makes things writeable by the file owner. This poses an issue with creating typos or errors in raw data. We fix that before going further chmod u-w * Working Directory First we make a work directory: a directory where we can play around with a copy of the data without messing with the original mkdir ~/work cd ~/work Now we make a link of the data in our working directory ln -s ~/data/* . The files that we've downloaded are FASTQ files. Take a look at one of them with zless SRR957824_500K_R1.fastq.gz Tip Use the spacebar to scroll down, and type \u2018q\u2019 to exit \u2018less\u2019 You can read more on the FASTQ format in the File Formats lesson. Question Where does the filename come from? Question Why are there 1 and 2 in the file names? FastQC To check the quality of the sequence data we will use a tool called FastQC. FastQC has a graphical interface and can be downloaded and run on a Windows or Linux computer without installation. It is available here . However, FastQC is also available as a command line utility on the training server you are using. To run FastQC on our two files fastqc SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz and look what FastQC has produced ls *fastqc* For each file, FastQC has produced both a .zip archive containing all the plots, and a html report. Download and open the html files with your favourite web browser. Alternatively you can look a these copies of them: SRR957824_500K_R1_fastqc.html SRR957824_500K_R2_fastqc.html Question What should you pay attention to in the FastQC report? Question Which file is of better quality? Pay special attention to the per base sequence quality and sequence length distribution. Explanations for the various quality modules can be found here . Also, have a look at examples of a good and a bad illumina read set for comparison. You will note that the reads in your uploaded dataset have fairly poor quality (<20) towards the end. There are also outlier reads that have very poor quality for most of the second half of the reads. Scythe Now we'll do some trimming! Scythe uses a Naive Bayesian approach to classify contaminant substrings in sequence reads. It considers quality information, which can make it robust in picking out 3'-end adapters, which often include poor quality bases. The first thing we need is the adapters to trim off curl -O -J -L https://osf.io/v24pt/download Now we run scythe on both our read files scythe -a adapters.fasta -o SRR957824_adapt_R1.fastq SRR957824_500K_R1.fastq.gz scythe -a adapters.fasta -o SRR957824_adapt_R2.fastq SRR957824_500K_R2.fastq.gz Question What adapters do you use? Sickle Most modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well. Incorrectly called bases in both regions negatively impact assembles, mapping, and downstream bioinformatics analyses. We will trim each read individually down to the good quality part to keep the bad part from interfering with downstream applications. To do so, we will use sickle. Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It will also discard reads based upon a length threshold. To run sickle sickle pe -f SRR957824_adapt_R1.fastq -r SRR957824_adapt_R2.fastq \\ -t sanger -o SRR957824_trimmed_R1.fastq -p SRR957824_trimmed_R2.fastq \\ -s /dev/null -q 25 which should output something like PE forward file: SRR957824_trimmed_R1.fastq PE reverse file: SRR957824_trimmed_R2.fastq Total input FastQ records: 1000000 (500000 pairs) FastQ paired records kept: 834570 (417285 pairs) FastQ single records kept: 13263 (from PE1: 11094, from PE2: 2169) FastQ paired records discarded: 138904 (69452 pairs) FastQ single records discarded: 13263 (from PE1: 2169, from PE2: 11094) FastQC again Run fastqc again on the filtered reads fastqc SRR957824_trimmed_R1.fastq SRR957824_trimmed_R2.fastq and look at the reports SRR957824_trimmed_R1_fastqc.html SRR957824_trimmed_R2_fastqc.html MultiQC MultiQC is a tool that aggreagtes results from several popular QC bioinformatics software into one html report. Let's run MultiQC in our current directory multiqc . You can download the report or view it by clickinh on the link below multiqc_report.html Question What did the trimming do to the per-base sequence quality, the per sequence quality scores and the sequence length distribution?","title":"Quality Control"},{"location":"tutorials/docs/qc/#quality-control-and-trimming","text":"","title":"Quality Control and Trimming"},{"location":"tutorials/docs/qc/#lecture","text":"","title":"Lecture"},{"location":"tutorials/docs/qc/#practical","text":"In this practical you will learn to import, view and check the quality of raw high thoughput sequencing sequencing data. The first dataset you will be working with is from an Illumina MiSeq dataset. The sequenced organism is an enterohaemorrhagic E. coli (EHEC) of the serotype O157, a potentially fatal gastrointestinal pathogen. The sequenced bacterium was part of an outbreak investigation in the St. Louis area, USA in 2011. The sequencing was done as paired-end 2x150bp.","title":"Practical"},{"location":"tutorials/docs/qc/#downloading-the-data","text":"The raw data were deposited at the European Nucleotide Archive, under the accession number SRR957824. You could go to the ENA website and search for the run with the accession SRR957824. However these files contain about 3 million reads and are therefore quite big. We are only gonna use a subset of the original dataset for this tutorial. First create a data/ directory in your home folder mkdir ~/data now let's download the subset cd ~/data curl -O -J -L https://osf.io/shqpv/download curl -O -J -L https://osf.io/9m3ch/download Let\u2019s make sure we downloaded all of our data using md5sum. md5sum SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz you should see this 1e8cf249e3217a5a0bcc0d8a654585fb SRR957824_500K_R1.fastq.gz 70c726a31f05f856fe942d727613adb7 SRR957824_500K_R2.fastq.gz and now look at the file names and their size ls -l total 97M -rw-r--r-- 1 hadrien 48M Nov 19 18:44 SRR957824_500K_R1.fastq.gz -rw-r--r-- 1 hadrien 50M Nov 19 18:53 SRR957824_500K_R2.fastq.gz There are 500 000 paired-end reads taken randomly from the original data One last thing before we get to the quality control: those files are writeable. By default, UNIX makes things writeable by the file owner. This poses an issue with creating typos or errors in raw data. We fix that before going further chmod u-w *","title":"Downloading the data"},{"location":"tutorials/docs/qc/#working-directory","text":"First we make a work directory: a directory where we can play around with a copy of the data without messing with the original mkdir ~/work cd ~/work Now we make a link of the data in our working directory ln -s ~/data/* . The files that we've downloaded are FASTQ files. Take a look at one of them with zless SRR957824_500K_R1.fastq.gz Tip Use the spacebar to scroll down, and type \u2018q\u2019 to exit \u2018less\u2019 You can read more on the FASTQ format in the File Formats lesson. Question Where does the filename come from? Question Why are there 1 and 2 in the file names?","title":"Working Directory"},{"location":"tutorials/docs/qc/#fastqc","text":"To check the quality of the sequence data we will use a tool called FastQC. FastQC has a graphical interface and can be downloaded and run on a Windows or Linux computer without installation. It is available here . However, FastQC is also available as a command line utility on the training server you are using. To run FastQC on our two files fastqc SRR957824_500K_R1.fastq.gz SRR957824_500K_R2.fastq.gz and look what FastQC has produced ls *fastqc* For each file, FastQC has produced both a .zip archive containing all the plots, and a html report. Download and open the html files with your favourite web browser. Alternatively you can look a these copies of them: SRR957824_500K_R1_fastqc.html SRR957824_500K_R2_fastqc.html Question What should you pay attention to in the FastQC report? Question Which file is of better quality? Pay special attention to the per base sequence quality and sequence length distribution. Explanations for the various quality modules can be found here . Also, have a look at examples of a good and a bad illumina read set for comparison. You will note that the reads in your uploaded dataset have fairly poor quality (<20) towards the end. There are also outlier reads that have very poor quality for most of the second half of the reads.","title":"FastQC"},{"location":"tutorials/docs/qc/#scythe","text":"Now we'll do some trimming! Scythe uses a Naive Bayesian approach to classify contaminant substrings in sequence reads. It considers quality information, which can make it robust in picking out 3'-end adapters, which often include poor quality bases. The first thing we need is the adapters to trim off curl -O -J -L https://osf.io/v24pt/download Now we run scythe on both our read files scythe -a adapters.fasta -o SRR957824_adapt_R1.fastq SRR957824_500K_R1.fastq.gz scythe -a adapters.fasta -o SRR957824_adapt_R2.fastq SRR957824_500K_R2.fastq.gz Question What adapters do you use?","title":"Scythe"},{"location":"tutorials/docs/qc/#sickle","text":"Most modern sequencing technologies produce reads that have deteriorating quality towards the 3'-end and some towards the 5'-end as well. Incorrectly called bases in both regions negatively impact assembles, mapping, and downstream bioinformatics analyses. We will trim each read individually down to the good quality part to keep the bad part from interfering with downstream applications. To do so, we will use sickle. Sickle is a tool that uses sliding windows along with quality and length thresholds to determine when quality is sufficiently low to trim the 3'-end of reads and also determines when the quality is sufficiently high enough to trim the 5'-end of reads. It will also discard reads based upon a length threshold. To run sickle sickle pe -f SRR957824_adapt_R1.fastq -r SRR957824_adapt_R2.fastq \\ -t sanger -o SRR957824_trimmed_R1.fastq -p SRR957824_trimmed_R2.fastq \\ -s /dev/null -q 25 which should output something like PE forward file: SRR957824_trimmed_R1.fastq PE reverse file: SRR957824_trimmed_R2.fastq Total input FastQ records: 1000000 (500000 pairs) FastQ paired records kept: 834570 (417285 pairs) FastQ single records kept: 13263 (from PE1: 11094, from PE2: 2169) FastQ paired records discarded: 138904 (69452 pairs) FastQ single records discarded: 13263 (from PE1: 2169, from PE2: 11094)","title":"Sickle"},{"location":"tutorials/docs/qc/#fastqc-again","text":"Run fastqc again on the filtered reads fastqc SRR957824_trimmed_R1.fastq SRR957824_trimmed_R2.fastq and look at the reports SRR957824_trimmed_R1_fastqc.html SRR957824_trimmed_R2_fastqc.html","title":"FastQC again"},{"location":"tutorials/docs/qc/#multiqc","text":"MultiQC is a tool that aggreagtes results from several popular QC bioinformatics software into one html report. Let's run MultiQC in our current directory multiqc . You can download the report or view it by clickinh on the link below multiqc_report.html Question What did the trimming do to the per-base sequence quality, the per sequence quality scores and the sequence length distribution?","title":"MultiQC"},{"location":"tutorials/docs/rna/","text":"RNA-Seq Downloading the data For this tutorial we will use the test data from this paper: Malachi Griffith , Jason R. Walker, Nicholas C. Spies, Benjamin J. Ainscough, Obi L. Griffith . 2015. Informatics for RNA-seq: A web resource for analysis on the cloud. PLoS Comp Biol. 11(8):e1004393. The test data consists of two commercially available RNA samples: Universal Human Reference (UHR) and Human Brain Reference (HBR). The UHR is total RNA isolated from a diverse set of 10 cancer cell lines. The HBR is total RNA isolated from the brains of 23 Caucasians, male and female, of varying age but mostly 60-80 years old. In addition, a spike-in control was used. Specifically we added an aliquot of the ERCC ExFold RNA Spike-In Control Mixes to each sample. The spike-in consists of 92 transcripts that are present in known concentrations across a wide abundance range (from very few copies to many copies). This range allows us to test the degree to which the RNA-seq assay (including all laboratory and analysis steps) accurately reflects the relative abundance of transcript species within a sample. There are two 'mixes' of these transcripts to allow an assessment of differential expression output between samples if you put one mix in each of your two comparisons. In our case, Mix1 was added to the UHR sample, and Mix2 was added to the HBR sample. We also have 3 complete experimental replicates for each sample. This allows us to assess the technical variability of our overall process of producing RNA-seq data in the lab. For all libraries we prepared low-throughput (Set A) TruSeq Stranded Total RNA Sample Prep Kit libraries with Ribo-Zero Gold to remove both cytoplasmic and mitochondrial rRNA. Triplicate, indexed libraries were made starting with 100ng Agilent/Strategene Universal Human Reference total RNA and 100ng Ambion Human Brain Reference total RNA. The Universal Human Reference replicates received 2 ul of 1:1000 ERCC Mix 1. The Human Brain Reference replicates received 1:1000 ERCC Mix 2. The libraries were quantified with KAPA Library Quantification qPCR and adjusted to the appropriate concentration for sequencing. The triplicate, indexed libraries were then pooled prior to sequencing. Each pool of three replicate libraries were sequenced across 2 lanes of a HiSeq 2000 using paired-end sequence chemistry with 100bp read lengths. So to summarize we have: UHR + ERCC Spike-In Mix1, Replicate 1 UHR + ERCC Spike-In Mix1, Replicate 2 UHR + ERCC Spike-In Mix1, Replicate 3 HBR + ERCC Spike-In Mix2, Replicate 1 HBR + ERCC Spike-In Mix2, Replicate 2 HBR + ERCC Spike-In Mix2, Replicate 3 You can download the data from here . Download and unpack the data curl -O -J -L https://osf.io/7zepj/download tar xzf toy_rna.tar.gz cd toy_rna Indexing transcriptome salmon index -t chr22_transcripts.fa -i chr22_index Quantify reads using salmon for i in *_R1.fastq.gz do prefix=$(basename $i _R1.fastq.gz) salmon quant -i chr22_index --libType A \\ -1 ${prefix}_R1.fastq.gz -2 ${prefix}_R2.fastq.gz -o quant/${prefix}; done This loop simply goes through each sample and invokes salmon using fairly basic options: The -i argument tells salmon where to find the index --libType A tells salmon that it should automatically determine the library type of the sequencing reads (e.g. stranded vs. unstranded etc.) The -1 and -2 arguments tell salmon where to find the left and right reads for this sample (notice, salmon will accept gzipped FASTQ files directly). the -o argument specifies the directory where salmon\u2019s quantification results sould be written. Salmon exposes many different options to the user that enable extra features or modify default behavior. However, the purpose and behavior of all of those options is beyond the scope of this introductory tutorial. You can read about salmon\u2019s many options in the documentation . After the salmon commands finish running, you should have a directory named quant , which will have a sub-directory for each sample. These sub-directories contain the quantification results of salmon, as well as a lot of other information salmon records about the sample and the run. The main output file (called quant.sf) is rather self-explanatory. For example, take a peek at the quantification file for sample HBR_Rep1 in quant/HBR_Rep1/quant.sf and you\u2019ll see a simple TSV format file listing the name (Name) of each transcript, its length (Length), effective length (EffectiveLength) (more details on this in the documentation), and its abundance in terms of Transcripts Per Million (TPM) and estimated number of reads (NumReads) originating from this transcript. Import read counts using tximport Using the tximport R package, you can import salmon\u2019s transcript-level quantifications and optionally aggregate them to the gene level for gene-level differential expression analysis. First, go in Rstudio server by typing the address to your server in your browser: http://MY_IP_ADDRESS:8787/ where you replace MY_IP_ADDRESS by the IP address of your Virtual Machine. Note To access Rstudio server on the virtual machine, you'll need a password Ask your instructor for the password! Note If you wish, you may work on Rstudio on your own laptop if it is powerful enough. You will need an up-to-date version of R, and can install the necessary packages using this script You will also need to download the toy_rna directory Once in Rstudio, set your working directory setwd('~/toy_rna') Then load the modules: library(tximport) library(GenomicFeatures) library(readr) Salmon did the quantifiation of the transcript level. We want to see which genes are differentially expressed, so we need to link the transcript names to the gene names. We can use our .gtf annotation for that, and the GenomicFeatures package: txdb <- makeTxDbFromGFF(\"chr22_genes.gtf\") k <- keys(txdb, keytype = \"GENEID\") tx2gene <- select(txdb, keys = k, keytype = \"GENEID\", columns = \"TXNAME\") head(tx2gene) Now we can import the salmon quantification. samples <- read.table(\"samples.txt\", header = TRUE) files <- file.path(\"quant\", samples$sample, \"quant.sf\") names(files) <- paste0(samples$sample) txi.salmon <- tximport(files, type = \"salmon\", tx2gene = tx2gene) Take a look at the data: head(txi.salmon$counts) Differential expression using DESeq2 load DESeq2: library(DESeq2) Instantiate the DESeqDataSet and generate result table. See ?DESeqDataSetFromTximport and ?DESeq for more information about the steps performed by the program. dds <- DESeqDataSetFromTximport(txi.salmon, samples, ~condition) dds <- DESeq(dds) res <- results(dds) Run the summary command to get an idea of how many genes are up- and downregulated between the two conditions: summary(res) DESeq uses a negative binomial distribution. Such distributions have two parameters: mean and dispersion. The dispersion is a parameter describing how much the variance deviates from the mean. You can read more about the methods used by DESeq2 in the paper or the vignette Plot dispersions: plotDispEsts(dds, main=\"Dispersion plot\") For clustering and heatmaps, we need to log transform our data: rld <- rlogTransformation(dds) head(assay(rld)) Then, we create a sample distance heatmap: library(RColorBrewer) library(gplots) (mycols <- brewer.pal(8, \"Dark2\")[1:length(unique(samples$condition))]) sampleDists <- as.matrix(dist(t(assay(rld)))) heatmap.2(as.matrix(sampleDists), key=F, trace=\"none\", col=colorpanel(100, \"black\", \"white\"), ColSideColors=mycols[samples$condition], RowSideColors=mycols[samples$condition], margin=c(10, 10), main=\"Sample Distance Matrix\") We can also plot a PCA: DESeq2::plotPCA(rld, intgroup=\"condition\") It is time to look at some p-values: table(res$padj<0.05) res <- res[order(res$padj), ] resdata <- merge(as.data.frame(res), as.data.frame(counts(dds, normalized=TRUE)), by=\"row.names\", sort=FALSE) names(resdata)[1] <- \"Gene\" head(resdata) Examine plot of p-values, the MA plot and the Volcano Plot: hist(res$pvalue, breaks=50, col=\"grey\") DESeq2::plotMA(dds, ylim=c(-1,1), cex=1) # Volcano plot with(res, plot(log2FoldChange, -log10(pvalue), pch=20, main=\"Volcano plot\", xlim=c(-2.5,2))) with(subset(res, padj<.05 ), points(log2FoldChange, -log10(pvalue), pch=20, col=\"red\")) KEGG pathway analysis As always, load the necessary packages: library(AnnotationDbi) library(org.Hs.eg.db) library(pathview) library(gage) library(gageData) Let\u2019s use the mapIds function to add more columns to the results. The row.names of our results table has the Ensembl gene ID (our key), so we need to specify keytype=ENSEMBL . The column argument tells the mapIds function which information we want, and the multiVals argument tells the function what to do if there are multiple possible values for a single input value. Here we ask to just give us back the first one that occurs in the database. Let\u2019s get the Entrez IDs, gene symbols, and full gene names. res$symbol <- mapIds(org.Hs.eg.db, keys=row.names(res), column=\"SYMBOL\", keytype=\"ENSEMBL\", multiVals=\"first\") res$entrez <- mapIds(org.Hs.eg.db, keys=row.names(res), column=\"ENTREZID\", keytype=\"ENSEMBL\", multiVals=\"first\") res$name <- mapIds(org.Hs.eg.db, keys=row.names(res), column=\"GENENAME\", keytype=\"ENSEMBL\", multiVals=\"first\") head(res) We\u2019re going to use the gage package for pathway analysis, and the pathview package to draw a pathway diagram. The gageData package has pre-compiled databases mapping genes to KEGG pathways and GO terms for common organisms: data(kegg.sets.hs) data(sigmet.idx.hs) kegg.sets.hs <- kegg.sets.hs[sigmet.idx.hs] head(kegg.sets.hs, 3) Run the pathway analysis. See help on the gage function with ?gage . Specifically, you might want to try changing the value of same.dir. foldchanges <- res$log2FoldChange names(foldchanges) <- res$entrez keggres <- gage(foldchanges, gsets=kegg.sets.hs, same.dir=TRUE) lapply(keggres, head) Pull out the top 5 upregulated pathways, then further process that just to get the IDs. We\u2019ll use these KEGG pathway IDs downstream for plotting. The dplyr package is required to use the pipe ( %>% ) construct. library(dplyr) # Get the pathways keggrespathways <- data.frame(id=rownames(keggres$greater), keggres$greater) %>% tbl_df() %>% filter(row_number()<=5) %>% .$id %>% as.character() keggrespathways # Get the IDs. keggresids <- substr(keggrespathways, start=1, stop=8) keggresids Finally, the pathview() function in the pathview package makes the plots. Let\u2019s write a function so we can loop through and draw plots for the top 5 pathways we created above. # Define plotting function for applying later plot_pathway <- function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=\"hsa\", new.signature=FALSE) # Unload dplyr since it conflicts with the next line detach(\"package:dplyr\", unload=T) # plot multiple pathways (plots saved to disk and returns a throwaway list object) tmp <- sapply(keggresids, function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=\"hsa\")) Thanks This material was inspired by Stephen Turner's blog post: Tutorial: RNA-seq differential expression & pathway analysis with Sailfish, DESeq2, GAGE, and Pathview: http://www.gettinggeneticsdone.com/2015/12/tutorial-rna-seq-differential.html","title":"RNA Sequencing"},{"location":"tutorials/docs/rna/#rna-seq","text":"","title":"RNA-Seq"},{"location":"tutorials/docs/rna/#downloading-the-data","text":"For this tutorial we will use the test data from this paper: Malachi Griffith , Jason R. Walker, Nicholas C. Spies, Benjamin J. Ainscough, Obi L. Griffith . 2015. Informatics for RNA-seq: A web resource for analysis on the cloud. PLoS Comp Biol. 11(8):e1004393. The test data consists of two commercially available RNA samples: Universal Human Reference (UHR) and Human Brain Reference (HBR). The UHR is total RNA isolated from a diverse set of 10 cancer cell lines. The HBR is total RNA isolated from the brains of 23 Caucasians, male and female, of varying age but mostly 60-80 years old. In addition, a spike-in control was used. Specifically we added an aliquot of the ERCC ExFold RNA Spike-In Control Mixes to each sample. The spike-in consists of 92 transcripts that are present in known concentrations across a wide abundance range (from very few copies to many copies). This range allows us to test the degree to which the RNA-seq assay (including all laboratory and analysis steps) accurately reflects the relative abundance of transcript species within a sample. There are two 'mixes' of these transcripts to allow an assessment of differential expression output between samples if you put one mix in each of your two comparisons. In our case, Mix1 was added to the UHR sample, and Mix2 was added to the HBR sample. We also have 3 complete experimental replicates for each sample. This allows us to assess the technical variability of our overall process of producing RNA-seq data in the lab. For all libraries we prepared low-throughput (Set A) TruSeq Stranded Total RNA Sample Prep Kit libraries with Ribo-Zero Gold to remove both cytoplasmic and mitochondrial rRNA. Triplicate, indexed libraries were made starting with 100ng Agilent/Strategene Universal Human Reference total RNA and 100ng Ambion Human Brain Reference total RNA. The Universal Human Reference replicates received 2 ul of 1:1000 ERCC Mix 1. The Human Brain Reference replicates received 1:1000 ERCC Mix 2. The libraries were quantified with KAPA Library Quantification qPCR and adjusted to the appropriate concentration for sequencing. The triplicate, indexed libraries were then pooled prior to sequencing. Each pool of three replicate libraries were sequenced across 2 lanes of a HiSeq 2000 using paired-end sequence chemistry with 100bp read lengths. So to summarize we have: UHR + ERCC Spike-In Mix1, Replicate 1 UHR + ERCC Spike-In Mix1, Replicate 2 UHR + ERCC Spike-In Mix1, Replicate 3 HBR + ERCC Spike-In Mix2, Replicate 1 HBR + ERCC Spike-In Mix2, Replicate 2 HBR + ERCC Spike-In Mix2, Replicate 3 You can download the data from here . Download and unpack the data curl -O -J -L https://osf.io/7zepj/download tar xzf toy_rna.tar.gz cd toy_rna","title":"Downloading the data"},{"location":"tutorials/docs/rna/#indexing-transcriptome","text":"salmon index -t chr22_transcripts.fa -i chr22_index","title":"Indexing transcriptome"},{"location":"tutorials/docs/rna/#quantify-reads-using-salmon","text":"for i in *_R1.fastq.gz do prefix=$(basename $i _R1.fastq.gz) salmon quant -i chr22_index --libType A \\ -1 ${prefix}_R1.fastq.gz -2 ${prefix}_R2.fastq.gz -o quant/${prefix}; done This loop simply goes through each sample and invokes salmon using fairly basic options: The -i argument tells salmon where to find the index --libType A tells salmon that it should automatically determine the library type of the sequencing reads (e.g. stranded vs. unstranded etc.) The -1 and -2 arguments tell salmon where to find the left and right reads for this sample (notice, salmon will accept gzipped FASTQ files directly). the -o argument specifies the directory where salmon\u2019s quantification results sould be written. Salmon exposes many different options to the user that enable extra features or modify default behavior. However, the purpose and behavior of all of those options is beyond the scope of this introductory tutorial. You can read about salmon\u2019s many options in the documentation . After the salmon commands finish running, you should have a directory named quant , which will have a sub-directory for each sample. These sub-directories contain the quantification results of salmon, as well as a lot of other information salmon records about the sample and the run. The main output file (called quant.sf) is rather self-explanatory. For example, take a peek at the quantification file for sample HBR_Rep1 in quant/HBR_Rep1/quant.sf and you\u2019ll see a simple TSV format file listing the name (Name) of each transcript, its length (Length), effective length (EffectiveLength) (more details on this in the documentation), and its abundance in terms of Transcripts Per Million (TPM) and estimated number of reads (NumReads) originating from this transcript.","title":"Quantify reads using salmon"},{"location":"tutorials/docs/rna/#import-read-counts-using-tximport","text":"Using the tximport R package, you can import salmon\u2019s transcript-level quantifications and optionally aggregate them to the gene level for gene-level differential expression analysis. First, go in Rstudio server by typing the address to your server in your browser: http://MY_IP_ADDRESS:8787/ where you replace MY_IP_ADDRESS by the IP address of your Virtual Machine. Note To access Rstudio server on the virtual machine, you'll need a password Ask your instructor for the password! Note If you wish, you may work on Rstudio on your own laptop if it is powerful enough. You will need an up-to-date version of R, and can install the necessary packages using this script You will also need to download the toy_rna directory Once in Rstudio, set your working directory setwd('~/toy_rna') Then load the modules: library(tximport) library(GenomicFeatures) library(readr) Salmon did the quantifiation of the transcript level. We want to see which genes are differentially expressed, so we need to link the transcript names to the gene names. We can use our .gtf annotation for that, and the GenomicFeatures package: txdb <- makeTxDbFromGFF(\"chr22_genes.gtf\") k <- keys(txdb, keytype = \"GENEID\") tx2gene <- select(txdb, keys = k, keytype = \"GENEID\", columns = \"TXNAME\") head(tx2gene) Now we can import the salmon quantification. samples <- read.table(\"samples.txt\", header = TRUE) files <- file.path(\"quant\", samples$sample, \"quant.sf\") names(files) <- paste0(samples$sample) txi.salmon <- tximport(files, type = \"salmon\", tx2gene = tx2gene) Take a look at the data: head(txi.salmon$counts)","title":"Import read counts using tximport"},{"location":"tutorials/docs/rna/#differential-expression-using-deseq2","text":"load DESeq2: library(DESeq2) Instantiate the DESeqDataSet and generate result table. See ?DESeqDataSetFromTximport and ?DESeq for more information about the steps performed by the program. dds <- DESeqDataSetFromTximport(txi.salmon, samples, ~condition) dds <- DESeq(dds) res <- results(dds) Run the summary command to get an idea of how many genes are up- and downregulated between the two conditions: summary(res) DESeq uses a negative binomial distribution. Such distributions have two parameters: mean and dispersion. The dispersion is a parameter describing how much the variance deviates from the mean. You can read more about the methods used by DESeq2 in the paper or the vignette Plot dispersions: plotDispEsts(dds, main=\"Dispersion plot\") For clustering and heatmaps, we need to log transform our data: rld <- rlogTransformation(dds) head(assay(rld)) Then, we create a sample distance heatmap: library(RColorBrewer) library(gplots) (mycols <- brewer.pal(8, \"Dark2\")[1:length(unique(samples$condition))]) sampleDists <- as.matrix(dist(t(assay(rld)))) heatmap.2(as.matrix(sampleDists), key=F, trace=\"none\", col=colorpanel(100, \"black\", \"white\"), ColSideColors=mycols[samples$condition], RowSideColors=mycols[samples$condition], margin=c(10, 10), main=\"Sample Distance Matrix\") We can also plot a PCA: DESeq2::plotPCA(rld, intgroup=\"condition\") It is time to look at some p-values: table(res$padj<0.05) res <- res[order(res$padj), ] resdata <- merge(as.data.frame(res), as.data.frame(counts(dds, normalized=TRUE)), by=\"row.names\", sort=FALSE) names(resdata)[1] <- \"Gene\" head(resdata) Examine plot of p-values, the MA plot and the Volcano Plot: hist(res$pvalue, breaks=50, col=\"grey\") DESeq2::plotMA(dds, ylim=c(-1,1), cex=1) # Volcano plot with(res, plot(log2FoldChange, -log10(pvalue), pch=20, main=\"Volcano plot\", xlim=c(-2.5,2))) with(subset(res, padj<.05 ), points(log2FoldChange, -log10(pvalue), pch=20, col=\"red\"))","title":"Differential expression using DESeq2"},{"location":"tutorials/docs/rna/#kegg-pathway-analysis","text":"As always, load the necessary packages: library(AnnotationDbi) library(org.Hs.eg.db) library(pathview) library(gage) library(gageData) Let\u2019s use the mapIds function to add more columns to the results. The row.names of our results table has the Ensembl gene ID (our key), so we need to specify keytype=ENSEMBL . The column argument tells the mapIds function which information we want, and the multiVals argument tells the function what to do if there are multiple possible values for a single input value. Here we ask to just give us back the first one that occurs in the database. Let\u2019s get the Entrez IDs, gene symbols, and full gene names. res$symbol <- mapIds(org.Hs.eg.db, keys=row.names(res), column=\"SYMBOL\", keytype=\"ENSEMBL\", multiVals=\"first\") res$entrez <- mapIds(org.Hs.eg.db, keys=row.names(res), column=\"ENTREZID\", keytype=\"ENSEMBL\", multiVals=\"first\") res$name <- mapIds(org.Hs.eg.db, keys=row.names(res), column=\"GENENAME\", keytype=\"ENSEMBL\", multiVals=\"first\") head(res) We\u2019re going to use the gage package for pathway analysis, and the pathview package to draw a pathway diagram. The gageData package has pre-compiled databases mapping genes to KEGG pathways and GO terms for common organisms: data(kegg.sets.hs) data(sigmet.idx.hs) kegg.sets.hs <- kegg.sets.hs[sigmet.idx.hs] head(kegg.sets.hs, 3) Run the pathway analysis. See help on the gage function with ?gage . Specifically, you might want to try changing the value of same.dir. foldchanges <- res$log2FoldChange names(foldchanges) <- res$entrez keggres <- gage(foldchanges, gsets=kegg.sets.hs, same.dir=TRUE) lapply(keggres, head) Pull out the top 5 upregulated pathways, then further process that just to get the IDs. We\u2019ll use these KEGG pathway IDs downstream for plotting. The dplyr package is required to use the pipe ( %>% ) construct. library(dplyr) # Get the pathways keggrespathways <- data.frame(id=rownames(keggres$greater), keggres$greater) %>% tbl_df() %>% filter(row_number()<=5) %>% .$id %>% as.character() keggrespathways # Get the IDs. keggresids <- substr(keggrespathways, start=1, stop=8) keggresids Finally, the pathview() function in the pathview package makes the plots. Let\u2019s write a function so we can loop through and draw plots for the top 5 pathways we created above. # Define plotting function for applying later plot_pathway <- function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=\"hsa\", new.signature=FALSE) # Unload dplyr since it conflicts with the next line detach(\"package:dplyr\", unload=T) # plot multiple pathways (plots saved to disk and returns a throwaway list object) tmp <- sapply(keggresids, function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=\"hsa\"))","title":"KEGG pathway analysis"},{"location":"tutorials/docs/rna/#thanks","text":"This material was inspired by Stephen Turner's blog post: Tutorial: RNA-seq differential expression & pathway analysis with Sailfish, DESeq2, GAGE, and Pathview: http://www.gettinggeneticsdone.com/2015/12/tutorial-rna-seq-differential.html","title":"Thanks"},{"location":"tutorials/docs/wms/","text":"Whole Metagenome Sequencin Table of Contents Introduction The Pig Microbiome Whole Metagenome Sequencing Softwares Required for this Tutorial Getting the Data and Checking their Quality Taxonomic Classification Visualization Introduction Microbiome used In this tutorial we will compare samples from the Pig Gut Microbiome to samples from the Human Gut Microbiome. Below you'll find a brief description of the two projects: The Pig Microbiome: Pig is a main species for livestock and biomedicine. The pig genome sequence was recently reported. To boost research, we established a catalogue of the genes of the gut microbiome based on faecal samples of 287 pigs from France, Denmark and China. More than 7.6 million non-redundant genes representing 719 metagenomic species were identified by deep metagenome sequencing, highlighting more similarities with the human than with the mouse catalogue. The pig and human catalogues share only 12.6 and 9.3 % of their genes, respectively, but 70 and 95% of their functional pathways. The pig gut microbiota is influenced by gender, age and breed. Analysis of the prevalence of antibiotics resistance genes (ARGs) reflected antibiotics supplementation in each farm system, and revealed that non-antibiotics-fed animals still harbour ARGs. The pig catalogue creates a resource for whole metagenomics-based studies, highly valuable for research in biomedicine and for sustainable knowledge-based pig farming The Human Microbiome: We are facing a global metabolic health crisis provoked by an obesity epidemic. Here we report the human gut microbial composition in a population sample of 123 non-obese and 169 obese Danish individuals. We find two groups of individuals that differ by the number of gut microbial genes and thus gut bacterial richness. They harbour known and previously unknown bacterial species at different proportions; individuals with a low bacterial richness (23% of the population) are characterized by more marked overall adiposity, insulin resistance and dyslipidaemia and a more pronounced inflammatory phenotype when compared with high bacterial richness individuals. The obese individuals among the former also gain more weight over time. Only a few bacterial species are sufficient to distinguish between individuals with high and low bacterial richness, and even between lean and obese. Our classifications based on variation in the gut microbiome identify subsets of individuals in the general white adult population who may be at increased risk of progressing to adiposity-associated co-morbidities Whole Metagenome Sequencing Whole Metagenome sequencing (WMS), or shotgun metagenome sequencing, is a relatively new and powerful sequencing approach that provides insight into community biodiversity and function. On the contrary of Metabarcoding, where only a specific region of the bacterial community (the 16s rRNA) is sequenced, WMS aims at sequencing all the genomic material present in the environment. The choice of shotgun or 16S approaches is usually dictated by the nature of the studies being conducted. For instance, 16S is well suited for analysis of large number of samples, i.e., multiple patients, longitudinal studies, etc. but offers limited taxonomical and functional resolution. WMS is generally more expensive but offers increased resolution, and allows the discovery of viruses as well as other mobile genetic elements. Softwares Required for this Tutorial FastQC Kraken R Pavian Prepare and organise your working directory You will first login to your virtual machine using the IP provided by the teachers. All the exercise will be performed on your VM in the cloud. Note When you login with the ssh command, please add the option -X at the end of it to be able to use graphical interface mkdir ~/wms cd ~/wms mkdir data mkdir results mkdir scripts Getting the Data and Checking their Quality As the data were very big, we have prepared performed a downsampling on all 6 datasets (3 pigs and 3 humans). We will first download and unpack the data. cd ~/wms/data curl -O -J -L https://osf.io/h9x6e/download tar xvf subset_wms.tar.gz cd sub_100000 We'll use FastQC to check the quality of our data. FastQC should be already installed on your VM, so you need to type fastqc When FastQC has started you can select the fastq file you just downloaded with file -> open What do you think about the quality of the reads? Do they need trimming? Are there still adapters present? Overrepresented sequences? Note FastQC can be downloaded and run on a Windows or Linux computer without installation. It is available here Alternatively, run fastqc on the command-line: fastqc *.fastq If the quality appears to be good, it's because it was probably the cleaned reads that were deposited into SRA. We can directly move to the classification step. Taxonomic Classification Kraken is a system for assigning taxonomic labels to short DNA sequences (i.e. reads) Kraken aims to achieve high sensitivity and high speed by utilizing exact alignments of k-mers and a novel classification algorithm (sic). In short, kraken uses a new approach with exact k-mer matching to assign taxonomy to short reads. It is extremely fast compared to traditional approaches (i.e. BLAST). By default, the authors of kraken built their database based on RefSeq Bacteria, Archaea and Viruses. We'll use it for the purpose of this tutorial. We will download a shrunk database (minikraken) provided by Kraken developers that is only 4GB. # First we create a databases directory in our home cd /mnt sudo mkdir databases cd databases # Then we download the minikraken database sudo wget https://ccb.jhu.edu/software/kraken/dl/minikraken_20171019_4GB.tgz sudo tar xzf minikraken_20171019_4GB.tgz KRAKEN_DB=/mnt/databases/minikraken_20171013_4GB cd Now run kraken on the reads # In the data/ directory cd ~/wms/data/sub_100000 for i in *_1.fastq do prefix=$(basename $i _1.fastq) # print which sample is being processed echo $prefix kraken --db $KRAKEN_DB --threads 2 --fastq-input \\ ${prefix}_1.fastq ${prefix}_2.fastq > /home/student/wms/results/${prefix}.tab kraken-report --db $KRAKEN_DB \\ /home/student/wms/results/${prefix}.tab > /home/student/wms/results/${prefix}_tax.txt done which produces a tab-delimited file with an assigned TaxID for each read. Kraken includes a script called kraken-report to transform this file into a \"tree\" view with the percentage of reads assigned to each taxa. We've run this script at each step in the loop. Take a look at the _tax.txt files! Visualization with Pavian Pavian is a web application for exploring metagenomics classification results. First, go in Rstudio server by typing the address to your server in your browser: http://MY_IP_ADDRESS:8787/ where you replace MY_IP_ADDRESS by the IP address of your Virtual Machine. Note To access Rstudio server on the virtual machine, you'll need a password. Ask your instructor for the password! Note If you wish, you may work on Rstudio on your own laptop if it is powerful enough. You will need an up-to-date version of R, and can install the necessary packages using this script Install and run Pavian: options(repos = c(CRAN = \"http://cran.rstudio.com\")) if (!require(remotes)) { install.packages(\"remotes\") } remotes::install_github(\"fbreitwieser/pavian\") pavian::runApp(port=5000) Then you will explore and compare the results produced by Kraken.","title":"Comparative metagenomics"},{"location":"tutorials/docs/wms/#whole-metagenome-sequencin","text":"","title":"Whole Metagenome Sequencin"},{"location":"tutorials/docs/wms/#table-of-contents","text":"Introduction The Pig Microbiome Whole Metagenome Sequencing Softwares Required for this Tutorial Getting the Data and Checking their Quality Taxonomic Classification Visualization","title":"Table of Contents"},{"location":"tutorials/docs/wms/#introduction","text":"","title":"Introduction"},{"location":"tutorials/docs/wms/#microbiome-used","text":"In this tutorial we will compare samples from the Pig Gut Microbiome to samples from the Human Gut Microbiome. Below you'll find a brief description of the two projects: The Pig Microbiome: Pig is a main species for livestock and biomedicine. The pig genome sequence was recently reported. To boost research, we established a catalogue of the genes of the gut microbiome based on faecal samples of 287 pigs from France, Denmark and China. More than 7.6 million non-redundant genes representing 719 metagenomic species were identified by deep metagenome sequencing, highlighting more similarities with the human than with the mouse catalogue. The pig and human catalogues share only 12.6 and 9.3 % of their genes, respectively, but 70 and 95% of their functional pathways. The pig gut microbiota is influenced by gender, age and breed. Analysis of the prevalence of antibiotics resistance genes (ARGs) reflected antibiotics supplementation in each farm system, and revealed that non-antibiotics-fed animals still harbour ARGs. The pig catalogue creates a resource for whole metagenomics-based studies, highly valuable for research in biomedicine and for sustainable knowledge-based pig farming The Human Microbiome: We are facing a global metabolic health crisis provoked by an obesity epidemic. Here we report the human gut microbial composition in a population sample of 123 non-obese and 169 obese Danish individuals. We find two groups of individuals that differ by the number of gut microbial genes and thus gut bacterial richness. They harbour known and previously unknown bacterial species at different proportions; individuals with a low bacterial richness (23% of the population) are characterized by more marked overall adiposity, insulin resistance and dyslipidaemia and a more pronounced inflammatory phenotype when compared with high bacterial richness individuals. The obese individuals among the former also gain more weight over time. Only a few bacterial species are sufficient to distinguish between individuals with high and low bacterial richness, and even between lean and obese. Our classifications based on variation in the gut microbiome identify subsets of individuals in the general white adult population who may be at increased risk of progressing to adiposity-associated co-morbidities","title":"Microbiome used"},{"location":"tutorials/docs/wms/#whole-metagenome-sequencing","text":"Whole Metagenome sequencing (WMS), or shotgun metagenome sequencing, is a relatively new and powerful sequencing approach that provides insight into community biodiversity and function. On the contrary of Metabarcoding, where only a specific region of the bacterial community (the 16s rRNA) is sequenced, WMS aims at sequencing all the genomic material present in the environment. The choice of shotgun or 16S approaches is usually dictated by the nature of the studies being conducted. For instance, 16S is well suited for analysis of large number of samples, i.e., multiple patients, longitudinal studies, etc. but offers limited taxonomical and functional resolution. WMS is generally more expensive but offers increased resolution, and allows the discovery of viruses as well as other mobile genetic elements.","title":"Whole Metagenome Sequencing"},{"location":"tutorials/docs/wms/#softwares-required-for-this-tutorial","text":"FastQC Kraken R Pavian","title":"Softwares Required for this Tutorial"},{"location":"tutorials/docs/wms/#prepare-and-organise-your-working-directory","text":"You will first login to your virtual machine using the IP provided by the teachers. All the exercise will be performed on your VM in the cloud. Note When you login with the ssh command, please add the option -X at the end of it to be able to use graphical interface mkdir ~/wms cd ~/wms mkdir data mkdir results mkdir scripts","title":"Prepare and organise your working directory"},{"location":"tutorials/docs/wms/#getting-the-data-and-checking-their-quality","text":"As the data were very big, we have prepared performed a downsampling on all 6 datasets (3 pigs and 3 humans). We will first download and unpack the data. cd ~/wms/data curl -O -J -L https://osf.io/h9x6e/download tar xvf subset_wms.tar.gz cd sub_100000 We'll use FastQC to check the quality of our data. FastQC should be already installed on your VM, so you need to type fastqc When FastQC has started you can select the fastq file you just downloaded with file -> open What do you think about the quality of the reads? Do they need trimming? Are there still adapters present? Overrepresented sequences? Note FastQC can be downloaded and run on a Windows or Linux computer without installation. It is available here Alternatively, run fastqc on the command-line: fastqc *.fastq If the quality appears to be good, it's because it was probably the cleaned reads that were deposited into SRA. We can directly move to the classification step.","title":"Getting the Data and Checking their Quality"},{"location":"tutorials/docs/wms/#taxonomic-classification","text":"Kraken is a system for assigning taxonomic labels to short DNA sequences (i.e. reads) Kraken aims to achieve high sensitivity and high speed by utilizing exact alignments of k-mers and a novel classification algorithm (sic). In short, kraken uses a new approach with exact k-mer matching to assign taxonomy to short reads. It is extremely fast compared to traditional approaches (i.e. BLAST). By default, the authors of kraken built their database based on RefSeq Bacteria, Archaea and Viruses. We'll use it for the purpose of this tutorial. We will download a shrunk database (minikraken) provided by Kraken developers that is only 4GB. # First we create a databases directory in our home cd /mnt sudo mkdir databases cd databases # Then we download the minikraken database sudo wget https://ccb.jhu.edu/software/kraken/dl/minikraken_20171019_4GB.tgz sudo tar xzf minikraken_20171019_4GB.tgz KRAKEN_DB=/mnt/databases/minikraken_20171013_4GB cd Now run kraken on the reads # In the data/ directory cd ~/wms/data/sub_100000 for i in *_1.fastq do prefix=$(basename $i _1.fastq) # print which sample is being processed echo $prefix kraken --db $KRAKEN_DB --threads 2 --fastq-input \\ ${prefix}_1.fastq ${prefix}_2.fastq > /home/student/wms/results/${prefix}.tab kraken-report --db $KRAKEN_DB \\ /home/student/wms/results/${prefix}.tab > /home/student/wms/results/${prefix}_tax.txt done which produces a tab-delimited file with an assigned TaxID for each read. Kraken includes a script called kraken-report to transform this file into a \"tree\" view with the percentage of reads assigned to each taxa. We've run this script at each step in the loop. Take a look at the _tax.txt files!","title":"Taxonomic Classification"},{"location":"tutorials/docs/wms/#visualization-with-pavian","text":"Pavian is a web application for exploring metagenomics classification results. First, go in Rstudio server by typing the address to your server in your browser: http://MY_IP_ADDRESS:8787/ where you replace MY_IP_ADDRESS by the IP address of your Virtual Machine. Note To access Rstudio server on the virtual machine, you'll need a password. Ask your instructor for the password! Note If you wish, you may work on Rstudio on your own laptop if it is powerful enough. You will need an up-to-date version of R, and can install the necessary packages using this script Install and run Pavian: options(repos = c(CRAN = \"http://cran.rstudio.com\")) if (!require(remotes)) { install.packages(\"remotes\") } remotes::install_github(\"fbreitwieser/pavian\") pavian::runApp(port=5000) Then you will explore and compare the results produced by Kraken.","title":"Visualization with Pavian"}]}